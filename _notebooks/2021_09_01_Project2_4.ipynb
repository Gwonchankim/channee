{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('base': conda)"
    },
    "interpreter": {
      "hash": "6941d73da0e9d2d11fdb9d364780673ce2a2282fb884d44d0d48b2aa5600bd91"
    },
    "colab": {
      "name": "2021-09-01-Project2-4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y9xXMgLUCQbU",
        "wct4muLiCQbU",
        "wcxj5I73CQbV",
        "WJQkPspjCQbW",
        "n_MLJMwaCQbX",
        "IgA1UgqRCQbX",
        "tLo1j9BpCQbY",
        "A-h4ZPuiCQbY",
        "IAyVupjWCQbZ",
        "GVsuHPeYCQbZ",
        "VcI5Yyz9CQba",
        "noVXlByfCQba",
        "YY4IUKcICQbb",
        "NkW0pQWKCQbb",
        "KpNUen8PCQbb",
        "4mPueGTcCQbb",
        "wXzouBp3CQbc",
        "aAcB2LIGCQbc",
        "cC1pwA4XCQbc",
        "KvOXQbTUCQbc",
        "7yjg1nwTCQbc",
        "6J4butFNCQbd",
        "HFfmssqdCQbd",
        "d8x3jJqDCQbd",
        "Zjt-ScemCQbe",
        "SkqmTl78CQbe",
        "2OM_npdSCQbf",
        "Yc7AqsJyCQbf",
        "xNIee9NTCQbi",
        "JZc9Ych0CQbi",
        "EHB3J1LQCQbj",
        "vX_DvwT4CQbj",
        "DgJ7xeQhCQbk"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZUrFtwACRuP"
      },
      "source": [
        "# \"Project2-4\"\n",
        "> \"ML을 이용한 대출 채무이행 판별\"\n",
        "\n",
        "- toc:true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: channee\n",
        "- categories: [jupyter, ML, MachineLearning]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHB3J1LQCQbj"
      },
      "source": [
        "#### 가공된 최종 데이터 세트 생성\n",
        "##### 지금까지 과정을 함수화\n",
        "##### 이전에 application 데이터 세트의 feature engineering 수행 후 새롭게 previous 데이터 세트로 가공된 데이터를 조인. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMhvWgpVCQbj"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyPqZ0C-CQbj"
      },
      "source": [
        "def get_dataset():\n",
        "    app_train = pd.read_csv('application_train.csv')\n",
        "    app_test = pd.read_csv('application_test.csv')\n",
        "    apps = pd.concat([app_train, app_test])\n",
        "    prev = pd.read_csv('previous_application.csv')\n",
        "\n",
        "    return apps, prev\n",
        "\n",
        "apps, prev = get_dataset()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX_DvwT4CQbj"
      },
      "source": [
        "#### 이전 application 데이터의 feature engineering 함수 복사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9GsxUErCQbj"
      },
      "source": [
        "def get_apps_processed(apps):\n",
        "    \n",
        "    # EXT_SOURCE_X FEATURE 가공\n",
        "    apps['APPS_EXT_SOURCE_MEAN'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "    apps['APPS_EXT_SOURCE_STD'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
        "    apps['APPS_EXT_SOURCE_STD'] = apps['APPS_EXT_SOURCE_STD'].fillna(apps['APPS_EXT_SOURCE_STD'].mean())\n",
        "    \n",
        "    # AMT_CREDIT 비율로 Feature 가공\n",
        "    apps['APPS_ANNUITY_CREDIT_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_CREDIT']\n",
        "    apps['APPS_GOODS_CREDIT_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_CREDIT']\n",
        "    \n",
        "    # AMT_INCOME_TOTAL 비율로 Feature 가공\n",
        "    apps['APPS_ANNUITY_INCOME_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_INCOME_TOTAL']\n",
        "    apps['APPS_CREDIT_INCOME_RATIO'] = apps['AMT_CREDIT']/apps['AMT_INCOME_TOTAL']\n",
        "    apps['APPS_GOODS_INCOME_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_INCOME_TOTAL']\n",
        "    apps['APPS_CNT_FAM_INCOME_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['CNT_FAM_MEMBERS']\n",
        "    \n",
        "    # DAYS_BIRTH, DAYS_EMPLOYED 비율로 Feature 가공\n",
        "    apps['APPS_EMPLOYED_BIRTH_RATIO'] = apps['DAYS_EMPLOYED']/apps['DAYS_BIRTH']\n",
        "    apps['APPS_INCOME_EMPLOYED_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_EMPLOYED']\n",
        "    apps['APPS_INCOME_BIRTH_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_BIRTH']\n",
        "    apps['APPS_CAR_BIRTH_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_BIRTH']\n",
        "    apps['APPS_CAR_EMPLOYED_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_EMPLOYED']\n",
        "    \n",
        "    return apps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgJ7xeQhCQbk"
      },
      "source": [
        "#### previous 데이터 가공후 인코딩 및 최종 데이터 집합 생성하는 함수 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uePMKCoRCQbk"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "def get_prev_processed(prev):\n",
        "    # 대출 신청 금액과 실제 대출액/대출 상품금액 차이 및 비율\n",
        "    prev['PREV_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
        "    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n",
        "    prev['PREV_CREDIT_APPL_RATIO'] = prev['AMT_CREDIT']/prev['AMT_APPLICATION']\n",
        "    # prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']\n",
        "    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE']/prev['AMT_APPLICATION']\n",
        "    \n",
        "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
        "    # 첫번째 만기일과 마지막 만기일까지의 기간\n",
        "    prev['PREV_DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
        "    # 매월 납부 금액과 납부 횟수 곱해서 전체 납부 금액 구함. \n",
        "    all_pay = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
        "    # 전체 납부 금액 대비 AMT_CREDIT 비율을 구하고 여기에 다시 납부횟수로 나누어서 이자율 계산. \n",
        "    prev['PREV_INTERESTS_RATE'] = (all_pay/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']\n",
        "        \n",
        "    return prev\n",
        "    \n",
        "    \n",
        "def get_prev_amt_agg(prev):\n",
        "    # 새롭게 생성된 대출 신청액 대비 다른 금액 차이 및 비율로 aggregation 수행. \n",
        "    agg_dict = {\n",
        "         # 기존 컬럼. \n",
        "        'SK_ID_CURR':['count'],\n",
        "        'AMT_CREDIT':['mean', 'max', 'sum'],\n",
        "        'AMT_ANNUITY':['mean', 'max', 'sum'], \n",
        "        'AMT_APPLICATION':['mean', 'max', 'sum'],\n",
        "        'AMT_DOWN_PAYMENT':['mean', 'max', 'sum'],\n",
        "        'AMT_GOODS_PRICE':['mean', 'max', 'sum'],\n",
        "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
        "        'CNT_PAYMENT': ['mean', 'sum'],\n",
        "        # 가공 컬럼\n",
        "        'PREV_CREDIT_DIFF':['mean', 'max', 'sum'], \n",
        "        'PREV_CREDIT_APPL_RATIO':['mean', 'max'],\n",
        "        'PREV_GOODS_DIFF':['mean', 'max', 'sum'],\n",
        "        'PREV_GOODS_APPL_RATIO':['mean', 'max'],\n",
        "        'PREV_DAYS_LAST_DUE_DIFF':['mean', 'max', 'sum'],\n",
        "        'PREV_INTERESTS_RATE':['mean', 'max']\n",
        "    }\n",
        "\n",
        "    prev_group = prev.groupby('SK_ID_CURR')\n",
        "    prev_amt_agg = prev_group.agg(agg_dict)\n",
        "\n",
        "    # multi index 컬럼을 '_'로 연결하여 컬럼명 변경\n",
        "    prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n",
        "    \n",
        "    return prev_amt_agg\n",
        "\n",
        "def get_prev_refused_appr_agg(prev):\n",
        "    # 원래 groupby 컬럼 + 세부 기준 컬럼으로 groupby 수행. 세분화된 레벨로 aggregation 수행 한 뒤에 unstack()으로 컬럼레벨로 변형. \n",
        "    prev_refused_appr_group = prev[prev['NAME_CONTRACT_STATUS'].isin(['Approved', 'Refused'])].groupby([ 'SK_ID_CURR', 'NAME_CONTRACT_STATUS'])\n",
        "    prev_refused_appr_agg = prev_refused_appr_group['SK_ID_CURR'].count().unstack()\n",
        "    # 컬럼명 변경. \n",
        "    prev_refused_appr_agg.columns = ['PREV_APPROVED_COUNT', 'PREV_REFUSED_COUNT' ]\n",
        "    # NaN값은 모두 0으로 변경. \n",
        "    prev_refused_appr_agg = prev_refused_appr_agg.fillna(0)\n",
        "    \n",
        "    return prev_refused_appr_agg\n",
        "\n",
        "    \n",
        "\n",
        "def get_prev_agg(prev):\n",
        "    prev = get_prev_processed(prev)\n",
        "    prev_amt_agg = get_prev_amt_agg(prev)\n",
        "    prev_refused_appr_agg = get_prev_refused_appr_agg(prev)\n",
        "    \n",
        "    # prev_amt_agg와 조인. \n",
        "    prev_agg = prev_amt_agg.merge(prev_refused_appr_agg, on='SK_ID_CURR', how='left')\n",
        "    # SK_ID_CURR별 과거 대출건수 대비 APPROVED_COUNT 및 REFUSED_COUNT 비율 생성. \n",
        "    prev_agg['PREV_REFUSED_RATIO'] = prev_agg['PREV_REFUSED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n",
        "    prev_agg['PREV_APPROVED_RATIO'] = prev_agg['PREV_APPROVED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n",
        "    # 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT' 컬럼 drop \n",
        "    prev_agg = prev_agg.drop(['PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'], axis=1)\n",
        "    \n",
        "    return prev_agg\n",
        "\n",
        "def get_apps_all_with_prev_agg(apps, prev):\n",
        "    apps_all =  get_apps_processed(apps)\n",
        "    prev_agg = get_prev_agg(prev)\n",
        "    print('prev_agg shape:', prev_agg.shape)\n",
        "    print('apps_all before merge shape:', apps_all.shape)\n",
        "    apps_all = apps_all.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
        "    print('apps_all after merge with prev_agg shape:', apps_all.shape)\n",
        "    \n",
        "    return apps_all\n",
        "\n",
        "def get_apps_all_encoded(apps_all):\n",
        "    object_columns = apps_all.dtypes[apps_all.dtypes == 'object'].index.tolist()\n",
        "    for column in object_columns:\n",
        "        apps_all[column] = pd.factorize(apps_all[column])[0]\n",
        "    \n",
        "    return apps_all\n",
        "\n",
        "def get_apps_all_train_test(apps_all):\n",
        "    apps_all_train = apps_all[~apps_all['TARGET'].isnull()]\n",
        "    apps_all_test = apps_all[apps_all['TARGET'].isnull()]\n",
        "\n",
        "    apps_all_test = apps_all_test.drop('TARGET', axis=1)\n",
        "    \n",
        "    return apps_all_train, apps_all_test\n",
        "    \n",
        "def train_apps_all(apps_all_train):\n",
        "    ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "    target_app = apps_all_train['TARGET']\n",
        "\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020)\n",
        "    print('train shape:', train_x.shape, 'valid shape:', valid_x.shape)\n",
        "    clf = LGBMClassifier(\n",
        "                nthread=4,\n",
        "                n_estimators=2000,\n",
        "                learning_rate=0.01,\n",
        "                num_leaves=32,\n",
        "                colsample_bytree=0.8,\n",
        "                subsample=0.8,\n",
        "                max_depth=8,\n",
        "                reg_alpha=0.04,\n",
        "                reg_lambda=0.07,\n",
        "                min_child_weight=40,\n",
        "                silent=-1,\n",
        "                verbose=-1,\n",
        "                )\n",
        "\n",
        "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 100, \n",
        "                early_stopping_rounds= 100)\n",
        "    \n",
        "    return clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7n8Qv-1CQbk"
      },
      "source": [
        "##### 최종 집합 생성 및 인코딩, 학습/테스트 데이터 분리, 학습/검증 피처와 타겟 데이터 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TgQ2WazCQbk",
        "outputId": "29c49c4c-1e45-4935-bbae-31f5deb34393"
      },
      "source": [
        "apps_all = get_apps_all_with_prev_agg(apps, prev)\n",
        "apps_all = get_apps_all_encoded(apps_all)\n",
        "apps_all_train, apps_all_test = get_apps_all_train_test(apps_all)\n",
        "ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "target_app = apps_all_train['TARGET']\n",
        "X_train, X_test, y_train, y_test = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-cfb8ca96aa23>:53: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
            "  prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prev_agg shape: (338857, 41)\n",
            "apps_all before merge shape: (356255, 135)\n",
            "apps_all after merge with prev_agg shape: (356255, 176)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrBbLfJFCQbk"
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from lightgbm import LGBMClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNUzTo2vCQbk"
      },
      "source": [
        "* 필요한 부분을 최대한 처리하여 Pipeline은 필요 없을 듯 하다.\n",
        "* randomforest와 LightGBM, XGboost를 수행해보고자 한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFdSX-PnCQbk",
        "outputId": "93735c9b-b7ab-41d5-ac5b-ec7097f8b5e6"
      },
      "source": [
        "# bayesian optimization 패키지 설치\n",
        "!pip install bayesian-optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bayesian-optimization in c:\\users\\channee\\anaconda3\\lib\\site-packages (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\channee\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\channee\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.20.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\channee\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\channee\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\channee\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWJTof9sCQbk"
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from lightgbm import LGBMClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI5hwgNKCQbk"
      },
      "source": [
        "# parameter 별로 search할 범위를 설정. \n",
        "bayesian_params = {\n",
        "    'max_depth': (6, 16), \n",
        "    'num_leaves': (24, 64), \n",
        "    'min_child_samples': (10, 200), \n",
        "    'min_child_weight':(1, 50),\n",
        "    'subsample':(0.5, 1.0),\n",
        "    'colsample_bytree': (0.5, 1.0),\n",
        "    'max_bin':(10, 500),\n",
        "    'reg_lambda':(0.001, 10),\n",
        "    'reg_alpha': (0.01, 50) \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL59Elt6CQbk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpYIk7cOCQbl"
      },
      "source": [
        "def lgb_roc_eval(max_depth, num_leaves, min_child_samples, min_child_weight, subsample, \n",
        "                colsample_bytree,max_bin, reg_lambda, reg_alpha):\n",
        "    params = {\n",
        "        \"n_estimators\":500, \"learning_rate\":0.02,\n",
        "        'max_depth': int(round(max_depth)), #  호출 시 실수형 값이 들어오므로 정수형 하이퍼 파라미터는 정수형으로 변경 \n",
        "        'num_leaves': int(round(num_leaves)), \n",
        "        'min_child_samples': int(round(min_child_samples)),\n",
        "        'min_child_weight': int(round(min_child_weight)),\n",
        "        'subsample': max(min(subsample, 1), 0), \n",
        "        'colsample_bytree': max(min(colsample_bytree, 1), 0),\n",
        "        'max_bin':  max(int(round(max_bin)),10),\n",
        "        'reg_lambda': max(reg_lambda,0),\n",
        "        'reg_alpha': max(reg_alpha, 0)\n",
        "    }\n",
        "    lgb_model = LGBMClassifier(**params)\n",
        "    lgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric= 'auc', verbose= 100, \n",
        "                early_stopping_rounds= 100)\n",
        "    proba = lgb_model.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(valid_y, valid_proba)\n",
        "    \n",
        "    return roc_auc   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KotYbOFACQbl",
        "outputId": "ac51105a-0bf4-49f0-ccf2-542b4f38b54d"
      },
      "source": [
        "# BayesianOptimization객체를 수행할 함수와 search할 parameter 범위를 설정하여 생성. \n",
        "lgbBO = BayesianOptimization(lgb_roc_eval,bayesian_params , random_state=0)\n",
        "# 함수 반환값이 최대가 되는 입력값 유추를 위한 iteration 수행. \n",
        "lgbBO.maximize(init_points=5, n_iter=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | colsam... |  max_bin  | max_depth | min_ch... | min_ch... | num_le... | reg_alpha | reg_la... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.769662\ttraining's binary_logloss: 0.246032\tvalid_1's auc: 0.755483\tvalid_1's binary_logloss: 0.248917\n",
            "[200]\ttraining's auc: 0.787253\ttraining's binary_logloss: 0.238487\tvalid_1's auc: 0.766224\tvalid_1's binary_logloss: 0.244243\n",
            "[300]\ttraining's auc: 0.798898\ttraining's binary_logloss: 0.23398\tvalid_1's auc: 0.771467\tvalid_1's binary_logloss: 0.242344\n",
            "[400]\ttraining's auc: 0.807869\ttraining's binary_logloss: 0.23065\tvalid_1's auc: 0.773972\tvalid_1's binary_logloss: 0.241458\n",
            "[500]\ttraining's auc: 0.815952\ttraining's binary_logloss: 0.227669\tvalid_1's auc: 0.775806\tvalid_1's binary_logloss: 0.240831\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.815952\ttraining's binary_logloss: 0.227669\tvalid_1's auc: 0.775806\tvalid_1's binary_logloss: 0.240831\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7758  \u001b[0m | \u001b[0m 0.7744  \u001b[0m | \u001b[0m 360.4   \u001b[0m | \u001b[0m 12.03   \u001b[0m | \u001b[0m 113.5   \u001b[0m | \u001b[0m 21.76   \u001b[0m | \u001b[0m 49.84   \u001b[0m | \u001b[0m 21.88   \u001b[0m | \u001b[0m 8.918   \u001b[0m | \u001b[0m 0.9818  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.76256\ttraining's binary_logloss: 0.247457\tvalid_1's auc: 0.7537\tvalid_1's binary_logloss: 0.249099\n",
            "[200]\ttraining's auc: 0.780205\ttraining's binary_logloss: 0.24047\tvalid_1's auc: 0.765756\tvalid_1's binary_logloss: 0.244226\n",
            "[300]\ttraining's auc: 0.79096\ttraining's binary_logloss: 0.236494\tvalid_1's auc: 0.771277\tvalid_1's binary_logloss: 0.242257\n",
            "[400]\ttraining's auc: 0.799189\ttraining's binary_logloss: 0.233539\tvalid_1's auc: 0.774083\tvalid_1's binary_logloss: 0.241244\n",
            "[500]\ttraining's auc: 0.806102\ttraining's binary_logloss: 0.231049\tvalid_1's auc: 0.775791\tvalid_1's binary_logloss: 0.240642\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.806102\ttraining's binary_logloss: 0.231049\tvalid_1's auc: 0.775791\tvalid_1's binary_logloss: 0.240642\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7758  \u001b[0m | \u001b[0m 0.6917  \u001b[0m | \u001b[0m 397.9   \u001b[0m | \u001b[0m 11.29   \u001b[0m | \u001b[0m 117.9   \u001b[0m | \u001b[0m 46.35   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 4.366   \u001b[0m | \u001b[0m 0.2032  \u001b[0m | \u001b[0m 0.9163  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.775644\ttraining's binary_logloss: 0.243872\tvalid_1's auc: 0.757262\tvalid_1's binary_logloss: 0.247984\n",
            "[200]\ttraining's auc: 0.796903\ttraining's binary_logloss: 0.235046\tvalid_1's auc: 0.768757\tvalid_1's binary_logloss: 0.243207\n",
            "[300]\ttraining's auc: 0.812041\ttraining's binary_logloss: 0.229261\tvalid_1's auc: 0.773366\tvalid_1's binary_logloss: 0.24156\n",
            "[400]\ttraining's auc: 0.824984\ttraining's binary_logloss: 0.224423\tvalid_1's auc: 0.776251\tvalid_1's binary_logloss: 0.240565\n",
            "[500]\ttraining's auc: 0.835704\ttraining's binary_logloss: 0.220372\tvalid_1's auc: 0.777178\tvalid_1's binary_logloss: 0.24019\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.835704\ttraining's binary_logloss: 0.220372\tvalid_1's auc: 0.777178\tvalid_1's binary_logloss: 0.24019\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.7772  \u001b[0m | \u001b[95m 0.8891  \u001b[0m | \u001b[95m 436.3   \u001b[0m | \u001b[95m 15.79   \u001b[0m | \u001b[95m 161.8   \u001b[0m | \u001b[95m 23.61   \u001b[0m | \u001b[95m 55.22   \u001b[0m | \u001b[95m 5.923   \u001b[0m | \u001b[95m 6.4     \u001b[0m | \u001b[95m 0.5717  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.765842\ttraining's binary_logloss: 0.246884\tvalid_1's auc: 0.753808\tvalid_1's binary_logloss: 0.249258\n",
            "[200]\ttraining's auc: 0.783137\ttraining's binary_logloss: 0.239739\tvalid_1's auc: 0.765526\tvalid_1's binary_logloss: 0.244489\n",
            "[300]\ttraining's auc: 0.793648\ttraining's binary_logloss: 0.23577\tvalid_1's auc: 0.770579\tvalid_1's binary_logloss: 0.242643\n",
            "[400]\ttraining's auc: 0.801582\ttraining's binary_logloss: 0.232812\tvalid_1's auc: 0.773253\tvalid_1's binary_logloss: 0.241685\n",
            "[500]\ttraining's auc: 0.808241\ttraining's binary_logloss: 0.23036\tvalid_1's auc: 0.774833\tvalid_1's binary_logloss: 0.241128\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.808241\ttraining's binary_logloss: 0.23036\tvalid_1's auc: 0.774833\tvalid_1's binary_logloss: 0.241128\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7748  \u001b[0m | \u001b[0m 0.9723  \u001b[0m | \u001b[0m 265.7   \u001b[0m | \u001b[0m 10.15   \u001b[0m | \u001b[0m 60.27   \u001b[0m | \u001b[0m 38.94   \u001b[0m | \u001b[0m 42.25   \u001b[0m | \u001b[0m 28.43   \u001b[0m | \u001b[0m 0.1889  \u001b[0m | \u001b[0m 0.8088  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.7659\ttraining's binary_logloss: 0.247276\tvalid_1's auc: 0.753968\tvalid_1's binary_logloss: 0.24956\n",
            "[200]\ttraining's auc: 0.781741\ttraining's binary_logloss: 0.240327\tvalid_1's auc: 0.764668\tvalid_1's binary_logloss: 0.244831\n",
            "[300]\ttraining's auc: 0.791758\ttraining's binary_logloss: 0.236503\tvalid_1's auc: 0.769722\tvalid_1's binary_logloss: 0.242955\n",
            "[400]\ttraining's auc: 0.799313\ttraining's binary_logloss: 0.233722\tvalid_1's auc: 0.772473\tvalid_1's binary_logloss: 0.241976\n",
            "[500]\ttraining's auc: 0.805605\ttraining's binary_logloss: 0.231421\tvalid_1's auc: 0.774035\tvalid_1's binary_logloss: 0.241428\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.805605\ttraining's binary_logloss: 0.231421\tvalid_1's auc: 0.774035\tvalid_1's binary_logloss: 0.241428\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.774   \u001b[0m | \u001b[0m 0.806   \u001b[0m | \u001b[0m 312.3   \u001b[0m | \u001b[0m 15.44   \u001b[0m | \u001b[0m 139.5   \u001b[0m | \u001b[0m 18.62   \u001b[0m | \u001b[0m 41.48   \u001b[0m | \u001b[0m 34.88   \u001b[0m | \u001b[0m 0.6032  \u001b[0m | \u001b[0m 0.8334  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.778416\ttraining's binary_logloss: 0.243866\tvalid_1's auc: 0.758906\tvalid_1's binary_logloss: 0.248045\n",
            "[200]\ttraining's auc: 0.797547\ttraining's binary_logloss: 0.235078\tvalid_1's auc: 0.768769\tvalid_1's binary_logloss: 0.243239\n",
            "[300]\ttraining's auc: 0.812388\ttraining's binary_logloss: 0.229334\tvalid_1's auc: 0.773393\tvalid_1's binary_logloss: 0.241516\n",
            "[400]\ttraining's auc: 0.824856\ttraining's binary_logloss: 0.224602\tvalid_1's auc: 0.77606\tvalid_1's binary_logloss: 0.240581\n",
            "[500]\ttraining's auc: 0.835757\ttraining's binary_logloss: 0.220492\tvalid_1's auc: 0.777391\tvalid_1's binary_logloss: 0.240112\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.835757\ttraining's binary_logloss: 0.220492\tvalid_1's auc: 0.777391\tvalid_1's binary_logloss: 0.240112\n",
            "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.7774  \u001b[0m | \u001b[95m 0.6405  \u001b[0m | \u001b[95m 435.0   \u001b[0m | \u001b[95m 13.5    \u001b[0m | \u001b[95m 169.3   \u001b[0m | \u001b[95m 26.92   \u001b[0m | \u001b[95m 57.69   \u001b[0m | \u001b[95m 5.768   \u001b[0m | \u001b[95m 9.196   \u001b[0m | \u001b[95m 0.613   \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.772281\ttraining's binary_logloss: 0.246014\tvalid_1's auc: 0.756518\tvalid_1's binary_logloss: 0.249054\n",
            "[200]\ttraining's auc: 0.789124\ttraining's binary_logloss: 0.238224\tvalid_1's auc: 0.766602\tvalid_1's binary_logloss: 0.244218\n",
            "[300]\ttraining's auc: 0.800463\ttraining's binary_logloss: 0.233735\tvalid_1's auc: 0.771115\tvalid_1's binary_logloss: 0.242475\n",
            "[400]\ttraining's auc: 0.809924\ttraining's binary_logloss: 0.230144\tvalid_1's auc: 0.774036\tvalid_1's binary_logloss: 0.241453\n",
            "[500]\ttraining's auc: 0.81804\ttraining's binary_logloss: 0.227105\tvalid_1's auc: 0.775536\tvalid_1's binary_logloss: 0.240903\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.81804\ttraining's binary_logloss: 0.227105\tvalid_1's auc: 0.775536\tvalid_1's binary_logloss: 0.240903\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7755  \u001b[0m | \u001b[0m 0.6778  \u001b[0m | \u001b[0m 477.2   \u001b[0m | \u001b[0m 11.75   \u001b[0m | \u001b[0m 194.9   \u001b[0m | \u001b[0m 46.85   \u001b[0m | \u001b[0m 59.47   \u001b[0m | \u001b[0m 26.32   \u001b[0m | \u001b[0m 8.388   \u001b[0m | \u001b[0m 0.7753  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.777583\ttraining's binary_logloss: 0.243725\tvalid_1's auc: 0.758288\tvalid_1's binary_logloss: 0.247968\n",
            "[200]\ttraining's auc: 0.797949\ttraining's binary_logloss: 0.234844\tvalid_1's auc: 0.768779\tvalid_1's binary_logloss: 0.24323\n",
            "[300]\ttraining's auc: 0.812857\ttraining's binary_logloss: 0.229109\tvalid_1's auc: 0.773238\tvalid_1's binary_logloss: 0.241582\n",
            "[400]\ttraining's auc: 0.825851\ttraining's binary_logloss: 0.22425\tvalid_1's auc: 0.776055\tvalid_1's binary_logloss: 0.240619\n",
            "[500]\ttraining's auc: 0.836858\ttraining's binary_logloss: 0.220087\tvalid_1's auc: 0.777214\tvalid_1's binary_logloss: 0.240157\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.836858\ttraining's binary_logloss: 0.220087\tvalid_1's auc: 0.777214\tvalid_1's binary_logloss: 0.240157\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7772  \u001b[0m | \u001b[0m 0.7374  \u001b[0m | \u001b[0m 408.1   \u001b[0m | \u001b[0m 12.84   \u001b[0m | \u001b[0m 197.6   \u001b[0m | \u001b[0m 5.693   \u001b[0m | \u001b[0m 57.73   \u001b[0m | \u001b[0m 6.171   \u001b[0m | \u001b[0m 8.007   \u001b[0m | \u001b[0m 0.6427  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.776884\ttraining's binary_logloss: 0.244363\tvalid_1's auc: 0.757931\tvalid_1's binary_logloss: 0.248329\n",
            "[200]\ttraining's auc: 0.795476\ttraining's binary_logloss: 0.235896\tvalid_1's auc: 0.767942\tvalid_1's binary_logloss: 0.243577\n",
            "[300]\ttraining's auc: 0.809275\ttraining's binary_logloss: 0.230547\tvalid_1's auc: 0.77254\tvalid_1's binary_logloss: 0.241869\n",
            "[400]\ttraining's auc: 0.82066\ttraining's binary_logloss: 0.226218\tvalid_1's auc: 0.774854\tvalid_1's binary_logloss: 0.241044\n",
            "[500]\ttraining's auc: 0.830938\ttraining's binary_logloss: 0.222269\tvalid_1's auc: 0.776395\tvalid_1's binary_logloss: 0.240484\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.830938\ttraining's binary_logloss: 0.222269\tvalid_1's auc: 0.776395\tvalid_1's binary_logloss: 0.240484\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7764  \u001b[0m | \u001b[0m 0.7749  \u001b[0m | \u001b[0m 388.9   \u001b[0m | \u001b[0m 14.3    \u001b[0m | \u001b[0m 193.9   \u001b[0m | \u001b[0m 49.91   \u001b[0m | \u001b[0m 62.85   \u001b[0m | \u001b[0m 13.15   \u001b[0m | \u001b[0m 9.906   \u001b[0m | \u001b[0m 0.6812  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.764203\ttraining's binary_logloss: 0.247826\tvalid_1's auc: 0.752311\tvalid_1's binary_logloss: 0.249919\n",
            "[200]\ttraining's auc: 0.779974\ttraining's binary_logloss: 0.241103\tvalid_1's auc: 0.763794\tvalid_1's binary_logloss: 0.245126\n",
            "[300]\ttraining's auc: 0.789033\ttraining's binary_logloss: 0.237594\tvalid_1's auc: 0.768866\tvalid_1's binary_logloss: 0.243267\n",
            "[400]\ttraining's auc: 0.795627\ttraining's binary_logloss: 0.235115\tvalid_1's auc: 0.771779\tvalid_1's binary_logloss: 0.242221\n",
            "[500]\ttraining's auc: 0.800884\ttraining's binary_logloss: 0.233158\tvalid_1's auc: 0.773414\tvalid_1's binary_logloss: 0.241632\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.800884\ttraining's binary_logloss: 0.233158\tvalid_1's auc: 0.773414\tvalid_1's binary_logloss: 0.241632\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7734  \u001b[0m | \u001b[0m 0.9461  \u001b[0m | \u001b[0m 421.7   \u001b[0m | \u001b[0m 6.672   \u001b[0m | \u001b[0m 189.9   \u001b[0m | \u001b[0m 10.04   \u001b[0m | \u001b[0m 47.84   \u001b[0m | \u001b[0m 46.75   \u001b[0m | \u001b[0m 9.148   \u001b[0m | \u001b[0m 0.7801  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.764174\ttraining's binary_logloss: 0.248029\tvalid_1's auc: 0.755162\tvalid_1's binary_logloss: 0.249577\n",
            "[200]\ttraining's auc: 0.778767\ttraining's binary_logloss: 0.241173\tvalid_1's auc: 0.765754\tvalid_1's binary_logloss: 0.244443\n",
            "[300]\ttraining's auc: 0.788759\ttraining's binary_logloss: 0.237386\tvalid_1's auc: 0.771036\tvalid_1's binary_logloss: 0.242441\n",
            "[400]\ttraining's auc: 0.795987\ttraining's binary_logloss: 0.234694\tvalid_1's auc: 0.773675\tvalid_1's binary_logloss: 0.241458\n",
            "[500]\ttraining's auc: 0.80232\ttraining's binary_logloss: 0.232393\tvalid_1's auc: 0.775601\tvalid_1's binary_logloss: 0.240772\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.80232\ttraining's binary_logloss: 0.232393\tvalid_1's auc: 0.775601\tvalid_1's binary_logloss: 0.240772\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7756  \u001b[0m | \u001b[0m 0.5017  \u001b[0m | \u001b[0m 393.7   \u001b[0m | \u001b[0m 10.71   \u001b[0m | \u001b[0m 119.0   \u001b[0m | \u001b[0m 45.57   \u001b[0m | \u001b[0m 28.2    \u001b[0m | \u001b[0m 10.95   \u001b[0m | \u001b[0m 4.662   \u001b[0m | \u001b[0m 0.9552  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.778162\ttraining's binary_logloss: 0.243432\tvalid_1's auc: 0.759238\tvalid_1's binary_logloss: 0.247741\n",
            "[200]\ttraining's auc: 0.799162\ttraining's binary_logloss: 0.234309\tvalid_1's auc: 0.769473\tvalid_1's binary_logloss: 0.24293\n",
            "[300]\ttraining's auc: 0.814812\ttraining's binary_logloss: 0.228328\tvalid_1's auc: 0.773682\tvalid_1's binary_logloss: 0.241344\n",
            "[400]\ttraining's auc: 0.827839\ttraining's binary_logloss: 0.223397\tvalid_1's auc: 0.776065\tvalid_1's binary_logloss: 0.240505\n",
            "[500]\ttraining's auc: 0.838855\ttraining's binary_logloss: 0.219213\tvalid_1's auc: 0.777105\tvalid_1's binary_logloss: 0.240137\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.838855\ttraining's binary_logloss: 0.219213\tvalid_1's auc: 0.777105\tvalid_1's binary_logloss: 0.240137\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7771  \u001b[0m | \u001b[0m 0.6423  \u001b[0m | \u001b[0m 400.8   \u001b[0m | \u001b[0m 13.13   \u001b[0m | \u001b[0m 174.5   \u001b[0m | \u001b[0m 22.2    \u001b[0m | \u001b[0m 53.42   \u001b[0m | \u001b[0m 0.8135  \u001b[0m | \u001b[0m 3.857   \u001b[0m | \u001b[0m 0.603   \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.777272\ttraining's binary_logloss: 0.243367\tvalid_1's auc: 0.759082\tvalid_1's binary_logloss: 0.247643\n",
            "[200]\ttraining's auc: 0.7988\ttraining's binary_logloss: 0.234288\tvalid_1's auc: 0.769727\tvalid_1's binary_logloss: 0.24285\n",
            "[300]\ttraining's auc: 0.814442\ttraining's binary_logloss: 0.228368\tvalid_1's auc: 0.773843\tvalid_1's binary_logloss: 0.241303\n",
            "[400]\ttraining's auc: 0.827084\ttraining's binary_logloss: 0.223525\tvalid_1's auc: 0.776152\tvalid_1's binary_logloss: 0.240502\n",
            "[500]\ttraining's auc: 0.838169\ttraining's binary_logloss: 0.219393\tvalid_1's auc: 0.777237\tvalid_1's binary_logloss: 0.240146\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.838169\ttraining's binary_logloss: 0.219393\tvalid_1's auc: 0.777237\tvalid_1's binary_logloss: 0.240146\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7772  \u001b[0m | \u001b[0m 0.6969  \u001b[0m | \u001b[0m 388.8   \u001b[0m | \u001b[0m 13.69   \u001b[0m | \u001b[0m 178.8   \u001b[0m | \u001b[0m 26.23   \u001b[0m | \u001b[0m 50.31   \u001b[0m | \u001b[0m 0.1285  \u001b[0m | \u001b[0m 1.032   \u001b[0m | \u001b[0m 0.8563  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.777861\ttraining's binary_logloss: 0.242936\tvalid_1's auc: 0.757958\tvalid_1's binary_logloss: 0.247557\n",
            "[200]\ttraining's auc: 0.801461\ttraining's binary_logloss: 0.233442\tvalid_1's auc: 0.769359\tvalid_1's binary_logloss: 0.242945\n",
            "[300]\ttraining's auc: 0.818486\ttraining's binary_logloss: 0.226978\tvalid_1's auc: 0.77388\tvalid_1's binary_logloss: 0.241346\n",
            "[400]\ttraining's auc: 0.832854\ttraining's binary_logloss: 0.221651\tvalid_1's auc: 0.776378\tvalid_1's binary_logloss: 0.240465\n",
            "[500]\ttraining's auc: 0.844619\ttraining's binary_logloss: 0.217191\tvalid_1's auc: 0.777355\tvalid_1's binary_logloss: 0.240144\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.844619\ttraining's binary_logloss: 0.217191\tvalid_1's auc: 0.777355\tvalid_1's binary_logloss: 0.240144\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7774  \u001b[0m | \u001b[0m 0.9789  \u001b[0m | \u001b[0m 363.4   \u001b[0m | \u001b[0m 15.32   \u001b[0m | \u001b[0m 190.7   \u001b[0m | \u001b[0m 2.003   \u001b[0m | \u001b[0m 57.05   \u001b[0m | \u001b[0m 1.982   \u001b[0m | \u001b[0m 4.416   \u001b[0m | \u001b[0m 0.9291  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.779098\ttraining's binary_logloss: 0.242597\tvalid_1's auc: 0.758713\tvalid_1's binary_logloss: 0.247432\n",
            "[200]\ttraining's auc: 0.802\ttraining's binary_logloss: 0.233149\tvalid_1's auc: 0.769621\tvalid_1's binary_logloss: 0.242884\n",
            "[300]\ttraining's auc: 0.81865\ttraining's binary_logloss: 0.22679\tvalid_1's auc: 0.773862\tvalid_1's binary_logloss: 0.241339\n",
            "[400]\ttraining's auc: 0.832546\ttraining's binary_logloss: 0.221521\tvalid_1's auc: 0.776112\tvalid_1's binary_logloss: 0.24053\n",
            "[500]\ttraining's auc: 0.844492\ttraining's binary_logloss: 0.216971\tvalid_1's auc: 0.776749\tvalid_1's binary_logloss: 0.240308\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.844492\ttraining's binary_logloss: 0.216971\tvalid_1's auc: 0.776749\tvalid_1's binary_logloss: 0.240308\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7767  \u001b[0m | \u001b[0m 0.8747  \u001b[0m | \u001b[0m 398.4   \u001b[0m | \u001b[0m 15.32   \u001b[0m | \u001b[0m 177.1   \u001b[0m | \u001b[0m 23.05   \u001b[0m | \u001b[0m 56.52   \u001b[0m | \u001b[0m 2.533   \u001b[0m | \u001b[0m 1.005   \u001b[0m | \u001b[0m 0.9827  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.76187\ttraining's binary_logloss: 0.247879\tvalid_1's auc: 0.753913\tvalid_1's binary_logloss: 0.249302\n",
            "[200]\ttraining's auc: 0.778662\ttraining's binary_logloss: 0.241031\tvalid_1's auc: 0.765519\tvalid_1's binary_logloss: 0.2444\n",
            "[300]\ttraining's auc: 0.78922\ttraining's binary_logloss: 0.23712\tvalid_1's auc: 0.771074\tvalid_1's binary_logloss: 0.242375\n",
            "[400]\ttraining's auc: 0.797187\ttraining's binary_logloss: 0.234247\tvalid_1's auc: 0.773953\tvalid_1's binary_logloss: 0.241344\n",
            "[500]\ttraining's auc: 0.804056\ttraining's binary_logloss: 0.231805\tvalid_1's auc: 0.775929\tvalid_1's binary_logloss: 0.24065\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.804056\ttraining's binary_logloss: 0.231805\tvalid_1's auc: 0.775929\tvalid_1's binary_logloss: 0.24065\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.7759  \u001b[0m | \u001b[0m 0.6846  \u001b[0m | \u001b[0m 368.2   \u001b[0m | \u001b[0m 14.93   \u001b[0m | \u001b[0m 197.8   \u001b[0m | \u001b[0m 25.54   \u001b[0m | \u001b[0m 25.02   \u001b[0m | \u001b[0m 4.245   \u001b[0m | \u001b[0m 2.664   \u001b[0m | \u001b[0m 0.8288  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.76672\ttraining's binary_logloss: 0.247028\tvalid_1's auc: 0.756541\tvalid_1's binary_logloss: 0.248962\n",
            "[200]\ttraining's auc: 0.783669\ttraining's binary_logloss: 0.239569\tvalid_1's auc: 0.767434\tvalid_1's binary_logloss: 0.24386\n",
            "[300]\ttraining's auc: 0.79523\ttraining's binary_logloss: 0.235244\tvalid_1's auc: 0.772526\tvalid_1's binary_logloss: 0.241924\n",
            "[400]\ttraining's auc: 0.804427\ttraining's binary_logloss: 0.231925\tvalid_1's auc: 0.775267\tvalid_1's binary_logloss: 0.240915\n",
            "[500]\ttraining's auc: 0.811987\ttraining's binary_logloss: 0.229176\tvalid_1's auc: 0.776778\tvalid_1's binary_logloss: 0.240381\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.811987\ttraining's binary_logloss: 0.229176\tvalid_1's auc: 0.776778\tvalid_1's binary_logloss: 0.240381\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.7768  \u001b[0m | \u001b[0m 0.5385  \u001b[0m | \u001b[0m 375.6   \u001b[0m | \u001b[0m 15.71   \u001b[0m | \u001b[0m 162.2   \u001b[0m | \u001b[0m 1.157   \u001b[0m | \u001b[0m 31.29   \u001b[0m | \u001b[0m 0.3692  \u001b[0m | \u001b[0m 9.23    \u001b[0m | \u001b[0m 0.5688  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.763487\ttraining's binary_logloss: 0.247559\tvalid_1's auc: 0.753988\tvalid_1's binary_logloss: 0.249255\n",
            "[200]\ttraining's auc: 0.780223\ttraining's binary_logloss: 0.240626\tvalid_1's auc: 0.76534\tvalid_1's binary_logloss: 0.244429\n",
            "[300]\ttraining's auc: 0.790465\ttraining's binary_logloss: 0.236701\tvalid_1's auc: 0.770206\tvalid_1's binary_logloss: 0.242582\n",
            "[400]\ttraining's auc: 0.797595\ttraining's binary_logloss: 0.234049\tvalid_1's auc: 0.772583\tvalid_1's binary_logloss: 0.24171\n",
            "[500]\ttraining's auc: 0.80371\ttraining's binary_logloss: 0.231846\tvalid_1's auc: 0.774369\tvalid_1's binary_logloss: 0.241097\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.80371\ttraining's binary_logloss: 0.231846\tvalid_1's auc: 0.774369\tvalid_1's binary_logloss: 0.241097\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7744  \u001b[0m | \u001b[0m 0.7394  \u001b[0m | \u001b[0m 417.5   \u001b[0m | \u001b[0m 6.129   \u001b[0m | \u001b[0m 174.7   \u001b[0m | \u001b[0m 31.59   \u001b[0m | \u001b[0m 28.05   \u001b[0m | \u001b[0m 1.518   \u001b[0m | \u001b[0m 6.418   \u001b[0m | \u001b[0m 0.8383  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.776986\ttraining's binary_logloss: 0.24363\tvalid_1's auc: 0.757374\tvalid_1's binary_logloss: 0.248025\n",
            "[200]\ttraining's auc: 0.797886\ttraining's binary_logloss: 0.234776\tvalid_1's auc: 0.768483\tvalid_1's binary_logloss: 0.243304\n",
            "[300]\ttraining's auc: 0.813076\ttraining's binary_logloss: 0.228954\tvalid_1's auc: 0.77327\tvalid_1's binary_logloss: 0.241566\n",
            "[400]\ttraining's auc: 0.825935\ttraining's binary_logloss: 0.224135\tvalid_1's auc: 0.775806\tvalid_1's binary_logloss: 0.240681\n",
            "[500]\ttraining's auc: 0.836838\ttraining's binary_logloss: 0.219956\tvalid_1's auc: 0.776992\tvalid_1's binary_logloss: 0.240235\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.836838\ttraining's binary_logloss: 0.219956\tvalid_1's auc: 0.776992\tvalid_1's binary_logloss: 0.240235\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.777   \u001b[0m | \u001b[0m 0.8929  \u001b[0m | \u001b[0m 381.0   \u001b[0m | \u001b[0m 12.24   \u001b[0m | \u001b[0m 162.5   \u001b[0m | \u001b[0m 1.124   \u001b[0m | \u001b[0m 62.24   \u001b[0m | \u001b[0m 9.051   \u001b[0m | \u001b[0m 8.559   \u001b[0m | \u001b[0m 0.5757  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.774485\ttraining's binary_logloss: 0.244852\tvalid_1's auc: 0.757004\tvalid_1's binary_logloss: 0.248424\n",
            "[200]\ttraining's auc: 0.792537\ttraining's binary_logloss: 0.236941\tvalid_1's auc: 0.767318\tvalid_1's binary_logloss: 0.243762\n",
            "[300]\ttraining's auc: 0.804149\ttraining's binary_logloss: 0.232344\tvalid_1's auc: 0.771777\tvalid_1's binary_logloss: 0.242089\n",
            "[400]\ttraining's auc: 0.8131\ttraining's binary_logloss: 0.228878\tvalid_1's auc: 0.774161\tvalid_1's binary_logloss: 0.241232\n",
            "[500]\ttraining's auc: 0.821163\ttraining's binary_logloss: 0.225782\tvalid_1's auc: 0.775404\tvalid_1's binary_logloss: 0.240783\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.821163\ttraining's binary_logloss: 0.225782\tvalid_1's auc: 0.775404\tvalid_1's binary_logloss: 0.240783\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.7754  \u001b[0m | \u001b[0m 0.8395  \u001b[0m | \u001b[0m 369.5   \u001b[0m | \u001b[0m 7.312   \u001b[0m | \u001b[0m 179.4   \u001b[0m | \u001b[0m 17.39   \u001b[0m | \u001b[0m 58.63   \u001b[0m | \u001b[0m 13.71   \u001b[0m | \u001b[0m 4.133   \u001b[0m | \u001b[0m 0.8568  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.780021\ttraining's binary_logloss: 0.243308\tvalid_1's auc: 0.759823\tvalid_1's binary_logloss: 0.24777\n",
            "[200]\ttraining's auc: 0.800958\ttraining's binary_logloss: 0.233946\tvalid_1's auc: 0.769898\tvalid_1's binary_logloss: 0.24286\n",
            "[300]\ttraining's auc: 0.81698\ttraining's binary_logloss: 0.22778\tvalid_1's auc: 0.774314\tvalid_1's binary_logloss: 0.241218\n",
            "[400]\ttraining's auc: 0.830722\ttraining's binary_logloss: 0.222612\tvalid_1's auc: 0.776942\tvalid_1's binary_logloss: 0.240308\n",
            "[500]\ttraining's auc: 0.842042\ttraining's binary_logloss: 0.218291\tvalid_1's auc: 0.778057\tvalid_1's binary_logloss: 0.239921\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.842042\ttraining's binary_logloss: 0.218291\tvalid_1's auc: 0.778057\tvalid_1's binary_logloss: 0.239921\n",
            "| \u001b[95m 21      \u001b[0m | \u001b[95m 0.7781  \u001b[0m | \u001b[95m 0.5967  \u001b[0m | \u001b[95m 403.2   \u001b[0m | \u001b[95m 12.91   \u001b[0m | \u001b[95m 158.8   \u001b[0m | \u001b[95m 7.338   \u001b[0m | \u001b[95m 58.12   \u001b[0m | \u001b[95m 0.6682  \u001b[0m | \u001b[95m 9.355   \u001b[0m | \u001b[95m 0.801   \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.77898\ttraining's binary_logloss: 0.243342\tvalid_1's auc: 0.758555\tvalid_1's binary_logloss: 0.247992\n",
            "[200]\ttraining's auc: 0.798959\ttraining's binary_logloss: 0.234514\tvalid_1's auc: 0.76841\tvalid_1's binary_logloss: 0.243369\n",
            "[300]\ttraining's auc: 0.814164\ttraining's binary_logloss: 0.228666\tvalid_1's auc: 0.773321\tvalid_1's binary_logloss: 0.241591\n",
            "[400]\ttraining's auc: 0.827085\ttraining's binary_logloss: 0.223771\tvalid_1's auc: 0.776039\tvalid_1's binary_logloss: 0.240644\n",
            "[500]\ttraining's auc: 0.838285\ttraining's binary_logloss: 0.219513\tvalid_1's auc: 0.777508\tvalid_1's binary_logloss: 0.240124\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.838285\ttraining's binary_logloss: 0.219513\tvalid_1's auc: 0.777508\tvalid_1's binary_logloss: 0.240124\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.7775  \u001b[0m | \u001b[0m 0.6984  \u001b[0m | \u001b[0m 408.4   \u001b[0m | \u001b[0m 15.75   \u001b[0m | \u001b[0m 137.0   \u001b[0m | \u001b[0m 1.697   \u001b[0m | \u001b[0m 63.85   \u001b[0m | \u001b[0m 11.6    \u001b[0m | \u001b[0m 0.1763  \u001b[0m | \u001b[0m 0.864   \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.777057\ttraining's binary_logloss: 0.243549\tvalid_1's auc: 0.757727\tvalid_1's binary_logloss: 0.247866\n",
            "[200]\ttraining's auc: 0.798708\ttraining's binary_logloss: 0.234457\tvalid_1's auc: 0.769064\tvalid_1's binary_logloss: 0.243119\n",
            "[300]\ttraining's auc: 0.814158\ttraining's binary_logloss: 0.228529\tvalid_1's auc: 0.773437\tvalid_1's binary_logloss: 0.241564\n",
            "[400]\ttraining's auc: 0.827436\ttraining's binary_logloss: 0.223518\tvalid_1's auc: 0.776108\tvalid_1's binary_logloss: 0.240607\n",
            "[500]\ttraining's auc: 0.83867\ttraining's binary_logloss: 0.219253\tvalid_1's auc: 0.777169\tvalid_1's binary_logloss: 0.240208\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.83867\ttraining's binary_logloss: 0.219253\tvalid_1's auc: 0.777169\tvalid_1's binary_logloss: 0.240208\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.7772  \u001b[0m | \u001b[0m 0.9089  \u001b[0m | \u001b[0m 407.4   \u001b[0m | \u001b[0m 15.54   \u001b[0m | \u001b[0m 180.7   \u001b[0m | \u001b[0m 24.17   \u001b[0m | \u001b[0m 58.56   \u001b[0m | \u001b[0m 5.618   \u001b[0m | \u001b[0m 9.113   \u001b[0m | \u001b[0m 0.788   \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.7743\ttraining's binary_logloss: 0.244663\tvalid_1's auc: 0.757346\tvalid_1's binary_logloss: 0.248295\n",
            "[200]\ttraining's auc: 0.7934\ttraining's binary_logloss: 0.236415\tvalid_1's auc: 0.768102\tvalid_1's binary_logloss: 0.243544\n",
            "[300]\ttraining's auc: 0.806554\ttraining's binary_logloss: 0.231308\tvalid_1's auc: 0.772786\tvalid_1's binary_logloss: 0.241797\n",
            "[400]\ttraining's auc: 0.817607\ttraining's binary_logloss: 0.22716\tvalid_1's auc: 0.775328\tvalid_1's binary_logloss: 0.240878\n",
            "[500]\ttraining's auc: 0.827256\ttraining's binary_logloss: 0.223529\tvalid_1's auc: 0.776919\tvalid_1's binary_logloss: 0.240307\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.827256\ttraining's binary_logloss: 0.223529\tvalid_1's auc: 0.776919\tvalid_1's binary_logloss: 0.240307\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.7769  \u001b[0m | \u001b[0m 0.8291  \u001b[0m | \u001b[0m 408.9   \u001b[0m | \u001b[0m 10.8    \u001b[0m | \u001b[0m 198.5   \u001b[0m | \u001b[0m 10.39   \u001b[0m | \u001b[0m 54.8    \u001b[0m | \u001b[0m 12.48   \u001b[0m | \u001b[0m 6.565   \u001b[0m | \u001b[0m 0.5806  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.780987\ttraining's binary_logloss: 0.242281\tvalid_1's auc: 0.759117\tvalid_1's binary_logloss: 0.247485\n",
            "[200]\ttraining's auc: 0.803399\ttraining's binary_logloss: 0.232707\tvalid_1's auc: 0.769436\tvalid_1's binary_logloss: 0.242907\n",
            "[300]\ttraining's auc: 0.820217\ttraining's binary_logloss: 0.226213\tvalid_1's auc: 0.773667\tvalid_1's binary_logloss: 0.241358\n",
            "[400]\ttraining's auc: 0.834774\ttraining's binary_logloss: 0.220692\tvalid_1's auc: 0.776268\tvalid_1's binary_logloss: 0.240465\n",
            "[500]\ttraining's auc: 0.846897\ttraining's binary_logloss: 0.215994\tvalid_1's auc: 0.777223\tvalid_1's binary_logloss: 0.240096\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.846897\ttraining's binary_logloss: 0.215994\tvalid_1's auc: 0.777223\tvalid_1's binary_logloss: 0.240096\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.7772  \u001b[0m | \u001b[0m 0.8242  \u001b[0m | \u001b[0m 414.0   \u001b[0m | \u001b[0m 12.84   \u001b[0m | \u001b[0m 139.6   \u001b[0m | \u001b[0m 23.64   \u001b[0m | \u001b[0m 62.52   \u001b[0m | \u001b[0m 4.5     \u001b[0m | \u001b[0m 1.024   \u001b[0m | \u001b[0m 0.9178  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.781337\ttraining's binary_logloss: 0.242246\tvalid_1's auc: 0.759542\tvalid_1's binary_logloss: 0.247353\n",
            "[200]\ttraining's auc: 0.80494\ttraining's binary_logloss: 0.232417\tvalid_1's auc: 0.770308\tvalid_1's binary_logloss: 0.242697\n",
            "[300]\ttraining's auc: 0.822298\ttraining's binary_logloss: 0.22572\tvalid_1's auc: 0.774614\tvalid_1's binary_logloss: 0.241111\n",
            "[400]\ttraining's auc: 0.836837\ttraining's binary_logloss: 0.220186\tvalid_1's auc: 0.777086\tvalid_1's binary_logloss: 0.240274\n",
            "[500]\ttraining's auc: 0.848534\ttraining's binary_logloss: 0.21562\tvalid_1's auc: 0.777809\tvalid_1's binary_logloss: 0.23998\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.848534\ttraining's binary_logloss: 0.21562\tvalid_1's auc: 0.777809\tvalid_1's binary_logloss: 0.23998\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.7778  \u001b[0m | \u001b[0m 0.8364  \u001b[0m | \u001b[0m 438.5   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 126.3   \u001b[0m | \u001b[0m 7.767   \u001b[0m | \u001b[0m 63.26   \u001b[0m | \u001b[0m 0.3294  \u001b[0m | \u001b[0m 9.232   \u001b[0m | \u001b[0m 0.6026  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.777309\ttraining's binary_logloss: 0.243448\tvalid_1's auc: 0.75775\tvalid_1's binary_logloss: 0.247903\n",
            "[200]\ttraining's auc: 0.799142\ttraining's binary_logloss: 0.234375\tvalid_1's auc: 0.768792\tvalid_1's binary_logloss: 0.243165\n",
            "[300]\ttraining's auc: 0.814917\ttraining's binary_logloss: 0.22837\tvalid_1's auc: 0.773507\tvalid_1's binary_logloss: 0.241467\n",
            "[400]\ttraining's auc: 0.828058\ttraining's binary_logloss: 0.223386\tvalid_1's auc: 0.775906\tvalid_1's binary_logloss: 0.240582\n",
            "[500]\ttraining's auc: 0.839395\ttraining's binary_logloss: 0.219123\tvalid_1's auc: 0.777117\tvalid_1's binary_logloss: 0.240155\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.839395\ttraining's binary_logloss: 0.219123\tvalid_1's auc: 0.777117\tvalid_1's binary_logloss: 0.240155\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.7771  \u001b[0m | \u001b[0m 0.7062  \u001b[0m | \u001b[0m 438.3   \u001b[0m | \u001b[0m 13.16   \u001b[0m | \u001b[0m 93.45   \u001b[0m | \u001b[0m 3.168   \u001b[0m | \u001b[0m 56.62   \u001b[0m | \u001b[0m 4.489   \u001b[0m | \u001b[0m 5.242   \u001b[0m | \u001b[0m 0.5061  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.778145\ttraining's binary_logloss: 0.243305\tvalid_1's auc: 0.757936\tvalid_1's binary_logloss: 0.2478\n",
            "[200]\ttraining's auc: 0.799265\ttraining's binary_logloss: 0.234379\tvalid_1's auc: 0.768543\tvalid_1's binary_logloss: 0.243239\n",
            "[300]\ttraining's auc: 0.81363\ttraining's binary_logloss: 0.228749\tvalid_1's auc: 0.772821\tvalid_1's binary_logloss: 0.241685\n",
            "[400]\ttraining's auc: 0.825123\ttraining's binary_logloss: 0.224326\tvalid_1's auc: 0.774995\tvalid_1's binary_logloss: 0.240928\n",
            "[500]\ttraining's auc: 0.835617\ttraining's binary_logloss: 0.220326\tvalid_1's auc: 0.776429\tvalid_1's binary_logloss: 0.240431\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.835617\ttraining's binary_logloss: 0.220326\tvalid_1's auc: 0.776429\tvalid_1's binary_logloss: 0.240431\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.7764  \u001b[0m | \u001b[0m 0.9458  \u001b[0m | \u001b[0m 465.0   \u001b[0m | \u001b[0m 7.9     \u001b[0m | \u001b[0m 121.9   \u001b[0m | \u001b[0m 2.539   \u001b[0m | \u001b[0m 62.43   \u001b[0m | \u001b[0m 4.784   \u001b[0m | \u001b[0m 9.659   \u001b[0m | \u001b[0m 0.5056  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.768712\ttraining's binary_logloss: 0.245667\tvalid_1's auc: 0.755179\tvalid_1's binary_logloss: 0.248495\n",
            "[200]\ttraining's auc: 0.788298\ttraining's binary_logloss: 0.237819\tvalid_1's auc: 0.767231\tvalid_1's binary_logloss: 0.243688\n",
            "[300]\ttraining's auc: 0.801509\ttraining's binary_logloss: 0.232922\tvalid_1's auc: 0.772532\tvalid_1's binary_logloss: 0.241848\n",
            "[400]\ttraining's auc: 0.811986\ttraining's binary_logloss: 0.229057\tvalid_1's auc: 0.775444\tvalid_1's binary_logloss: 0.24083\n",
            "[500]\ttraining's auc: 0.820471\ttraining's binary_logloss: 0.225952\tvalid_1's auc: 0.776356\tvalid_1's binary_logloss: 0.24049\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.820471\ttraining's binary_logloss: 0.225952\tvalid_1's auc: 0.776356\tvalid_1's binary_logloss: 0.24049\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7764  \u001b[0m | \u001b[0m 0.9001  \u001b[0m | \u001b[0m 432.8   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 121.5   \u001b[0m | \u001b[0m 4.114   \u001b[0m | \u001b[0m 38.13   \u001b[0m | \u001b[0m 2.886   \u001b[0m | \u001b[0m 7.886   \u001b[0m | \u001b[0m 0.9864  \u001b[0m |\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's auc: 0.78142\ttraining's binary_logloss: 0.242343\tvalid_1's auc: 0.760102\tvalid_1's binary_logloss: 0.247216\n",
            "[200]\ttraining's auc: 0.803636\ttraining's binary_logloss: 0.232854\tvalid_1's auc: 0.770109\tvalid_1's binary_logloss: 0.242694\n",
            "[300]\ttraining's auc: 0.819047\ttraining's binary_logloss: 0.226767\tvalid_1's auc: 0.77406\tvalid_1's binary_logloss: 0.241196\n",
            "[400]\ttraining's auc: 0.831064\ttraining's binary_logloss: 0.22211\tvalid_1's auc: 0.775997\tvalid_1's binary_logloss: 0.240495\n",
            "[500]\ttraining's auc: 0.841456\ttraining's binary_logloss: 0.218102\tvalid_1's auc: 0.777091\tvalid_1's binary_logloss: 0.240091\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\ttraining's auc: 0.841456\ttraining's binary_logloss: 0.218102\tvalid_1's auc: 0.777091\tvalid_1's binary_logloss: 0.240091\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.7771  \u001b[0m | \u001b[0m 0.7396  \u001b[0m | \u001b[0m 442.3   \u001b[0m | \u001b[0m 9.243   \u001b[0m | \u001b[0m 126.5   \u001b[0m | \u001b[0m 31.85   \u001b[0m | \u001b[0m 63.82   \u001b[0m | \u001b[0m 0.6593  \u001b[0m | \u001b[0m 6.771   \u001b[0m | \u001b[0m 0.8922  \u001b[0m |\n",
            "=====================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2L07QsZCQbl",
        "outputId": "94719853-fbbb-482d-af2f-d3529b65d96b"
      },
      "source": [
        "# BayesianOptimization객체의 res는 iteration 수행 시마다 모든 함수 반환결과와 그때의 파라미터 결과값을 가지고 있음. \n",
        "lgbBO.res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'target': 0.7758055093230539,\n",
              "  'params': {'colsample_bytree': 0.7744067519636624,\n",
              "   'max_bin': 360.44278952248555,\n",
              "   'max_depth': 12.027633760716439,\n",
              "   'min_child_samples': 113.52780476941041,\n",
              "   'min_child_weight': 21.75908516760633,\n",
              "   'num_leaves': 49.835764522666246,\n",
              "   'reg_alpha': 21.884984691022,\n",
              "   'reg_lambda': 8.917838234820016,\n",
              "   'subsample': 0.9818313802505146}},\n",
              " {'target': 0.7757909289675659,\n",
              "  'params': {'colsample_bytree': 0.6917207594128889,\n",
              "   'max_bin': 397.94526866050563,\n",
              "   'max_depth': 11.288949197529044,\n",
              "   'min_child_samples': 117.92846660784714,\n",
              "   'min_child_weight': 46.35423527634039,\n",
              "   'num_leaves': 26.841442327915477,\n",
              "   'reg_alpha': 4.36559369208002,\n",
              "   'reg_lambda': 0.20316375600581688,\n",
              "   'subsample': 0.916309922773969}},\n",
              " {'target': 0.7771779879367707,\n",
              "  'params': {'colsample_bytree': 0.8890783754749252,\n",
              "   'max_bin': 436.30595264094137,\n",
              "   'max_depth': 15.78618342232764,\n",
              "   'min_child_samples': 161.8401272011775,\n",
              "   'min_child_weight': 23.61248875039366,\n",
              "   'num_leaves': 55.22116705145822,\n",
              "   'reg_alpha': 5.922538549187972,\n",
              "   'reg_lambda': 6.3995702922539115,\n",
              "   'subsample': 0.5716766437045232}},\n",
              " {'target': 0.7748333416046418,\n",
              "  'params': {'colsample_bytree': 0.972334458524792,\n",
              "   'max_bin': 265.70567765753515,\n",
              "   'max_depth': 10.146619399905235,\n",
              "   'min_child_samples': 60.265566299879126,\n",
              "   'min_child_weight': 38.93745078227661,\n",
              "   'num_leaves': 42.24601328866194,\n",
              "   'reg_alpha': 28.426013103943742,\n",
              "   'reg_lambda': 0.18887921456311507,\n",
              "   'subsample': 0.8088177485379385}},\n",
              " {'target': 0.774035094542375,\n",
              "  'params': {'colsample_bytree': 0.8060478613612108,\n",
              "   'max_bin': 312.2976584686309,\n",
              "   'max_depth': 15.437480785146242,\n",
              "   'min_child_samples': 139.54585682966186,\n",
              "   'min_child_weight': 18.615887128115514,\n",
              "   'num_leaves': 41.481278151973655,\n",
              "   'reg_alpha': 34.88458348440397,\n",
              "   'reg_lambda': 0.6031944908210691,\n",
              "   'subsample': 0.8333833577228338}},\n",
              " {'target': 0.7773908017189786,\n",
              "  'params': {'colsample_bytree': 0.640481830861817,\n",
              "   'max_bin': 435.0379450370509,\n",
              "   'max_depth': 13.497244758196743,\n",
              "   'min_child_samples': 169.30259380663517,\n",
              "   'min_child_weight': 26.924368410534857,\n",
              "   'num_leaves': 57.69153705029583,\n",
              "   'reg_alpha': 5.7675960060342195,\n",
              "   'reg_lambda': 9.196441703351635,\n",
              "   'subsample': 0.6129607288812317}},\n",
              " {'target': 0.7755362516633085,\n",
              "  'params': {'colsample_bytree': 0.6778196691000836,\n",
              "   'max_bin': 477.176698811766,\n",
              "   'max_depth': 11.752369565537183,\n",
              "   'min_child_samples': 194.8929524178145,\n",
              "   'min_child_weight': 46.84607882647774,\n",
              "   'num_leaves': 59.472175385640206,\n",
              "   'reg_alpha': 26.316833854656593,\n",
              "   'reg_lambda': 8.387658534981561,\n",
              "   'subsample': 0.7753185059732159}},\n",
              " {'target': 0.7772142029411041,\n",
              "  'params': {'colsample_bytree': 0.7373732904804776,\n",
              "   'max_bin': 408.1358313138957,\n",
              "   'max_depth': 12.843992667819151,\n",
              "   'min_child_samples': 197.5558677155646,\n",
              "   'min_child_weight': 5.693485177437429,\n",
              "   'num_leaves': 57.72941400376497,\n",
              "   'reg_alpha': 6.171344107340323,\n",
              "   'reg_lambda': 8.007058832279368,\n",
              "   'subsample': 0.6427405880739969}},\n",
              " {'target': 0.7763952615906466,\n",
              "  'params': {'colsample_bytree': 0.7748893532024008,\n",
              "   'max_bin': 388.9169674423393,\n",
              "   'max_depth': 14.298324669842952,\n",
              "   'min_child_samples': 193.91589835191607,\n",
              "   'min_child_weight': 49.909444261894315,\n",
              "   'num_leaves': 62.84933349372062,\n",
              "   'reg_alpha': 13.150761788903786,\n",
              "   'reg_lambda': 9.906371801724482,\n",
              "   'subsample': 0.6811573472682452}},\n",
              " {'target': 0.7734141467631326,\n",
              "  'params': {'colsample_bytree': 0.9461483552083645,\n",
              "   'max_bin': 421.69292633292594,\n",
              "   'max_depth': 6.671600015870614,\n",
              "   'min_child_samples': 189.89049916570497,\n",
              "   'min_child_weight': 10.044424315906738,\n",
              "   'num_leaves': 47.83526701748679,\n",
              "   'reg_alpha': 46.74823035052019,\n",
              "   'reg_lambda': 9.147970169890396,\n",
              "   'subsample': 0.7801135437391282}},\n",
              " {'target': 0.7756008426857747,\n",
              "  'params': {'colsample_bytree': 0.5017469237656516,\n",
              "   'max_bin': 393.72190024490175,\n",
              "   'max_depth': 10.707724066578827,\n",
              "   'min_child_samples': 118.96813016814396,\n",
              "   'min_child_weight': 45.56868876896979,\n",
              "   'num_leaves': 28.201502549830405,\n",
              "   'reg_alpha': 10.952704243299513,\n",
              "   'reg_lambda': 4.662235191571802,\n",
              "   'subsample': 0.9551699402948195}},\n",
              " {'target': 0.7771050305636831,\n",
              "  'params': {'colsample_bytree': 0.6422839773084814,\n",
              "   'max_bin': 400.82150970541625,\n",
              "   'max_depth': 13.131175782431662,\n",
              "   'min_child_samples': 174.45170668076625,\n",
              "   'min_child_weight': 22.20273089632871,\n",
              "   'num_leaves': 53.41657085105421,\n",
              "   'reg_alpha': 0.8135160827380638,\n",
              "   'reg_lambda': 3.856905787752113,\n",
              "   'subsample': 0.6029851974682363}},\n",
              " {'target': 0.7772365047377109,\n",
              "  'params': {'colsample_bytree': 0.6968897778533021,\n",
              "   'max_bin': 388.82764448356886,\n",
              "   'max_depth': 13.691810834514651,\n",
              "   'min_child_samples': 178.80475184834663,\n",
              "   'min_child_weight': 26.23145438502705,\n",
              "   'num_leaves': 50.312746331501124,\n",
              "   'reg_alpha': 0.1285436498176259,\n",
              "   'reg_lambda': 1.032293662925894,\n",
              "   'subsample': 0.8562506227206372}},\n",
              " {'target': 0.7773549981224318,\n",
              "  'params': {'colsample_bytree': 0.9789353469971445,\n",
              "   'max_bin': 363.4354295856673,\n",
              "   'max_depth': 15.315211746017521,\n",
              "   'min_child_samples': 190.73741827012313,\n",
              "   'min_child_weight': 2.003360059766269,\n",
              "   'num_leaves': 57.046953584464674,\n",
              "   'reg_alpha': 1.9824621842535364,\n",
              "   'reg_lambda': 4.416491356182225,\n",
              "   'subsample': 0.9290899949750824}},\n",
              " {'target': 0.7767492867273098,\n",
              "  'params': {'colsample_bytree': 0.8747411340191682,\n",
              "   'max_bin': 398.36524681235744,\n",
              "   'max_depth': 15.320951372373688,\n",
              "   'min_child_samples': 177.08379767055555,\n",
              "   'min_child_weight': 23.052918269012956,\n",
              "   'num_leaves': 56.51625901289448,\n",
              "   'reg_alpha': 2.5331760761204847,\n",
              "   'reg_lambda': 1.0049109599081048,\n",
              "   'subsample': 0.9826775193057701}},\n",
              " {'target': 0.775929160395352,\n",
              "  'params': {'colsample_bytree': 0.6845927664319438,\n",
              "   'max_bin': 368.221991524003,\n",
              "   'max_depth': 14.927073385197513,\n",
              "   'min_child_samples': 197.8030464042996,\n",
              "   'min_child_weight': 25.535853809991778,\n",
              "   'num_leaves': 25.021887939272,\n",
              "   'reg_alpha': 4.2450984307780235,\n",
              "   'reg_lambda': 2.664497197105028,\n",
              "   'subsample': 0.8288176887623219}},\n",
              " {'target': 0.7767777231064307,\n",
              "  'params': {'colsample_bytree': 0.538512544314991,\n",
              "   'max_bin': 375.62078344659466,\n",
              "   'max_depth': 15.712329472869794,\n",
              "   'min_child_samples': 162.22115952435223,\n",
              "   'min_child_weight': 1.156941735685324,\n",
              "   'num_leaves': 31.294569230544013,\n",
              "   'reg_alpha': 0.36916877863456377,\n",
              "   'reg_lambda': 9.229532425566951,\n",
              "   'subsample': 0.5688093468774877}},\n",
              " {'target': 0.7743690019971274,\n",
              "  'params': {'colsample_bytree': 0.7394308873506115,\n",
              "   'max_bin': 417.4989608386593,\n",
              "   'max_depth': 6.128913120786649,\n",
              "   'min_child_samples': 174.74416919268225,\n",
              "   'min_child_weight': 31.59245109049789,\n",
              "   'num_leaves': 28.05211885684618,\n",
              "   'reg_alpha': 1.5180061549611963,\n",
              "   'reg_lambda': 6.418396503524244,\n",
              "   'subsample': 0.8382939851041469}},\n",
              " {'target': 0.7769920411479967,\n",
              "  'params': {'colsample_bytree': 0.8929451865708424,\n",
              "   'max_bin': 380.96408472669606,\n",
              "   'max_depth': 12.24296875089054,\n",
              "   'min_child_samples': 162.4681623947883,\n",
              "   'min_child_weight': 1.124323100936236,\n",
              "   'num_leaves': 62.237143754650376,\n",
              "   'reg_alpha': 9.051348381820413,\n",
              "   'reg_lambda': 8.55903813259641,\n",
              "   'subsample': 0.5756763184847222}},\n",
              " {'target': 0.7754041675256128,\n",
              "  'params': {'colsample_bytree': 0.8395378766641771,\n",
              "   'max_bin': 369.45992318249404,\n",
              "   'max_depth': 7.311653298584847,\n",
              "   'min_child_samples': 179.448902083174,\n",
              "   'min_child_weight': 17.38672526468209,\n",
              "   'num_leaves': 58.6349743249024,\n",
              "   'reg_alpha': 13.707733769542015,\n",
              "   'reg_lambda': 4.132666603445228,\n",
              "   'subsample': 0.8567763982467431}},\n",
              " {'target': 0.7780565770624691,\n",
              "  'params': {'colsample_bytree': 0.596684514717646,\n",
              "   'max_bin': 403.18943474881877,\n",
              "   'max_depth': 12.907058961217114,\n",
              "   'min_child_samples': 158.84286216603994,\n",
              "   'min_child_weight': 7.338287048901049,\n",
              "   'num_leaves': 58.11656509027165,\n",
              "   'reg_alpha': 0.6681705096965275,\n",
              "   'reg_lambda': 9.355145759543333,\n",
              "   'subsample': 0.8010238277727275}},\n",
              " {'target': 0.7775081101221989,\n",
              "  'params': {'colsample_bytree': 0.6983666820490788,\n",
              "   'max_bin': 408.3845530220334,\n",
              "   'max_depth': 15.750233779055964,\n",
              "   'min_child_samples': 136.95205244907152,\n",
              "   'min_child_weight': 1.6965708103918615,\n",
              "   'num_leaves': 63.8506437238174,\n",
              "   'reg_alpha': 11.603475539657392,\n",
              "   'reg_lambda': 0.17633246697484675,\n",
              "   'subsample': 0.8640107135672386}},\n",
              " {'target': 0.7771685255576835,\n",
              "  'params': {'colsample_bytree': 0.9089270943590277,\n",
              "   'max_bin': 407.4077569955801,\n",
              "   'max_depth': 15.54111069785531,\n",
              "   'min_child_samples': 180.7441063643676,\n",
              "   'min_child_weight': 24.16966471165873,\n",
              "   'num_leaves': 58.55774861930196,\n",
              "   'reg_alpha': 5.617770487707293,\n",
              "   'reg_lambda': 9.112524233329468,\n",
              "   'subsample': 0.7879625370264551}},\n",
              " {'target': 0.7769187946775454,\n",
              "  'params': {'colsample_bytree': 0.8291181953449183,\n",
              "   'max_bin': 408.90814062859624,\n",
              "   'max_depth': 10.799903523797198,\n",
              "   'min_child_samples': 198.46670214232182,\n",
              "   'min_child_weight': 10.393808863704972,\n",
              "   'num_leaves': 54.79520213741949,\n",
              "   'reg_alpha': 12.481921001697238,\n",
              "   'reg_lambda': 6.564787886354399,\n",
              "   'subsample': 0.580612232524619}},\n",
              " {'target': 0.7772230140569003,\n",
              "  'params': {'colsample_bytree': 0.8242099119497381,\n",
              "   'max_bin': 414.0195379290514,\n",
              "   'max_depth': 12.838272481918226,\n",
              "   'min_child_samples': 139.5861484641032,\n",
              "   'min_child_weight': 23.638609111440616,\n",
              "   'num_leaves': 62.52080831472535,\n",
              "   'reg_alpha': 4.499765686038743,\n",
              "   'reg_lambda': 1.0239192579133045,\n",
              "   'subsample': 0.9178127659622564}},\n",
              " {'target': 0.777808998528086,\n",
              "  'params': {'colsample_bytree': 0.8364260787813464,\n",
              "   'max_bin': 438.5344980810344,\n",
              "   'max_depth': 12.59368901950138,\n",
              "   'min_child_samples': 126.31456889978566,\n",
              "   'min_child_weight': 7.767143783785291,\n",
              "   'num_leaves': 63.260425772040506,\n",
              "   'reg_alpha': 0.32940336713078977,\n",
              "   'reg_lambda': 9.23233466904062,\n",
              "   'subsample': 0.602582344997781}},\n",
              " {'target': 0.7771167374184544,\n",
              "  'params': {'colsample_bytree': 0.7062460990955068,\n",
              "   'max_bin': 438.3043198858672,\n",
              "   'max_depth': 13.162643244440723,\n",
              "   'min_child_samples': 93.44971252277496,\n",
              "   'min_child_weight': 3.1677333380244557,\n",
              "   'num_leaves': 56.6248859454379,\n",
              "   'reg_alpha': 4.488505501877722,\n",
              "   'reg_lambda': 5.24181749541699,\n",
              "   'subsample': 0.5061422738685454}},\n",
              " {'target': 0.7764292464153193,\n",
              "  'params': {'colsample_bytree': 0.9458015786718881,\n",
              "   'max_bin': 465.00800252186986,\n",
              "   'max_depth': 7.899527803307271,\n",
              "   'min_child_samples': 121.9201901620783,\n",
              "   'min_child_weight': 2.5392614386800236,\n",
              "   'num_leaves': 62.427676297468345,\n",
              "   'reg_alpha': 4.784274264326323,\n",
              "   'reg_lambda': 9.658699354226123,\n",
              "   'subsample': 0.505629085733924}},\n",
              " {'target': 0.7763562620386317,\n",
              "  'params': {'colsample_bytree': 0.900086638538324,\n",
              "   'max_bin': 432.8385783874056,\n",
              "   'max_depth': 12.593459218233793,\n",
              "   'min_child_samples': 121.50703399935037,\n",
              "   'min_child_weight': 4.114137212124302,\n",
              "   'num_leaves': 38.12688146861727,\n",
              "   'reg_alpha': 2.885555306126921,\n",
              "   'reg_lambda': 7.88597353383862,\n",
              "   'subsample': 0.9863664224720974}},\n",
              " {'target': 0.777091004576216,\n",
              "  'params': {'colsample_bytree': 0.7395613472763644,\n",
              "   'max_bin': 442.26433974553163,\n",
              "   'max_depth': 9.243101646454075,\n",
              "   'min_child_samples': 126.50175957072607,\n",
              "   'min_child_weight': 31.84651381717126,\n",
              "   'num_leaves': 63.82367461756897,\n",
              "   'reg_alpha': 0.6593173679660734,\n",
              "   'reg_lambda': 6.770945353052257,\n",
              "   'subsample': 0.8921800827540989}}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56XFO0ZKCQbl"
      },
      "source": [
        "##### Iteration 결과 Dictionary에서 최대 target값을 가지는 index 추출하고 그때의 parameter 값을 추출.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufHu2uxPCQbl",
        "outputId": "8a58e58f-9d28-4aa5-8bc4-59f228f37f2e"
      },
      "source": [
        "# dictionary에 있는 target값을 모두 추출\n",
        "target_list = []\n",
        "for result in lgbBO.res:\n",
        "    target = result['target']\n",
        "    target_list.append(target)\n",
        "print(target_list)\n",
        "# 가장 큰 target 값을 가지는 순번(index)를 추출\n",
        "print('maximum target index:', np.argmax(np.array(target_list)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7758055093230539, 0.7757909289675659, 0.7771779879367707, 0.7748333416046418, 0.774035094542375, 0.7773908017189786, 0.7755362516633085, 0.7772142029411041, 0.7763952615906466, 0.7734141467631326, 0.7756008426857747, 0.7771050305636831, 0.7772365047377109, 0.7773549981224318, 0.7767492867273098, 0.775929160395352, 0.7767777231064307, 0.7743690019971274, 0.7769920411479967, 0.7754041675256128, 0.7780565770624691, 0.7775081101221989, 0.7771685255576835, 0.7769187946775454, 0.7772230140569003, 0.777808998528086, 0.7771167374184544, 0.7764292464153193, 0.7763562620386317, 0.777091004576216]\n",
            "maximum target index: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKjXKpQyCQbl",
        "outputId": "a2410b91-7bde-44da-cf93-d853a1153259"
      },
      "source": [
        "# 가장 큰 target값을 가지는 index값을 기준으로 res에서 해당 parameter 추출. \n",
        "max_dict = lgbBO.res[np.argmax(np.array(target_list))]\n",
        "print(max_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'target': 0.7780565770624691, 'params': {'colsample_bytree': 0.596684514717646, 'max_bin': 403.18943474881877, 'max_depth': 12.907058961217114, 'min_child_samples': 158.84286216603994, 'min_child_weight': 7.338287048901049, 'num_leaves': 58.11656509027165, 'reg_alpha': 0.6681705096965275, 'reg_lambda': 9.355145759543333, 'subsample': 0.8010238277727275}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyKFOdCmCQbl"
      },
      "source": [
        "# 최적화된 하이퍼 파라미터를 기반으로 재 테스트 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkcV7fAHCQbl",
        "outputId": "6cefb958-95b1-4ced-d3f9-dfee51a37cb8"
      },
      "source": [
        "ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "target_app = apps_all_train['TARGET']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020)\n",
        "print('train shape:', train_x.shape, 'valid shape:', valid_x.shape)\n",
        "lgbm_wrapper = LGBMClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.02,\n",
        "    max_depth = 13,\n",
        "    num_leaves=58,\n",
        "    colsample_bytree=0.597,\n",
        "    subsample=0.801,\n",
        "    max_bin=403,\n",
        "    reg_alpha=0.668,\n",
        "    reg_lambda=9.355,\n",
        "    min_child_weight=7,\n",
        "    min_child_samples=159,\n",
        "    silent=-1,\n",
        "    verbose=-1,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "evals = [(X_test, y_test)]\n",
        "lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals, verbose=100)\n",
        "preds = lgbm_wrapper.predict(X_test)\n",
        "pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (215257, 174) valid shape: (92254, 174)\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's auc: 0.759823\tvalid_0's binary_logloss: 0.24777\n",
            "[200]\tvalid_0's auc: 0.769883\tvalid_0's binary_logloss: 0.242862\n",
            "[300]\tvalid_0's auc: 0.773949\tvalid_0's binary_logloss: 0.241297\n",
            "[400]\tvalid_0's auc: 0.776637\tvalid_0's binary_logloss: 0.240371\n",
            "[500]\tvalid_0's auc: 0.77781\tvalid_0's binary_logloss: 0.239972\n",
            "[600]\tvalid_0's auc: 0.778251\tvalid_0's binary_logloss: 0.239807\n",
            "[700]\tvalid_0's auc: 0.778768\tvalid_0's binary_logloss: 0.239639\n",
            "[800]\tvalid_0's auc: 0.778859\tvalid_0's binary_logloss: 0.239603\n",
            "[900]\tvalid_0's auc: 0.779087\tvalid_0's binary_logloss: 0.239534\n",
            "[1000]\tvalid_0's auc: 0.77916\tvalid_0's binary_logloss: 0.239504\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[987]\tvalid_0's auc: 0.779182\tvalid_0's binary_logloss: 0.239501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjunFGiXCQbl",
        "outputId": "6ad71e8e-ede0-4a8e-e56c-aed88785fc8a"
      },
      "source": [
        "# 평가지표\n",
        "from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score \n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
        "    confusion = confusion_matrix( y_test, pred)\n",
        "    accuracy = accuracy_score(y_test , pred)\n",
        "    precision = precision_score(y_test , pred)\n",
        "    recall = recall_score(y_test , pred)\n",
        "    f1 = f1_score(y_test,pred)\n",
        "    # ROC-AUC 추가 \n",
        "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
        "    print('오차 행렬')\n",
        "    print(confusion)\n",
        "    # ROC-AUC print 추가\n",
        "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
        "          F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n",
        "\n",
        "\n",
        "get_clf_eval(y_test, preds, pred_proba)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "오차 행렬\n",
            "[[84653   180]\n",
            " [ 7201   220]]\n",
            "정확도: 0.9200, 정밀도: 0.5500, 재현율: 0.0296,          F1: 0.0563, AUC:0.7792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_bYnswKCQbl",
        "outputId": "e0caedcb-f460-4a0c-9fe8-b47df34e3867"
      },
      "source": [
        "# ROC curve\n",
        "def roc_curve_plot(y_test , pred_proba_c1):\n",
        "    # 임곗값에 따른 FPR, TPR 값을 반환 받음. \n",
        "    fprs , tprs , thresholds = roc_curve(y_test ,pred_proba_c1)\n",
        "\n",
        "    # ROC Curve를 plot 곡선으로 그림. \n",
        "    plt.plot(fprs , tprs, label='ROC')\n",
        "    # 가운데 대각선 직선을 그림. \n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    \n",
        "    # FPR X 축의 Scale을 0.1 단위로 변경, X,Y 축명 설정등   \n",
        "    start, end = plt.xlim()\n",
        "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
        "    plt.xlim(0,1); plt.ylim(0,1)\n",
        "    plt.xlabel('FPR( 1 - Sensitivity )'); plt.ylabel('TPR( Recall )')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "roc_curve_plot(y_test, pred_proba)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"265.995469pt\" version=\"1.1\" viewBox=\"0 0 385.78125 265.995469\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-09-01T09:46:50.009734</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 265.995469 \r\nL 385.78125 265.995469 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 378.58125 228.439219 \r\nL 378.58125 10.999219 \r\nL 43.78125 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mfab695a424\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.52125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.05 -->\r\n      <g transform=\"translate(49.388438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"94.00125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.15 -->\r\n      <g transform=\"translate(82.868437 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"127.48125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.25 -->\r\n      <g transform=\"translate(116.348438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"160.96125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.35 -->\r\n      <g transform=\"translate(149.828438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"194.44125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.45 -->\r\n      <g transform=\"translate(183.308437 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.92125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.55 -->\r\n      <g transform=\"translate(216.788438 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"261.40125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.65 -->\r\n      <g transform=\"translate(250.268438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"294.88125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.75 -->\r\n      <g transform=\"translate(283.748438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"328.36125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.85 -->\r\n      <g transform=\"translate(317.228437 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_10\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"361.84125\" xlink:href=\"#mfab695a424\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.95 -->\r\n      <g transform=\"translate(350.708438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_11\">\r\n     <!-- FPR( 1 - Sensitivity ) -->\r\n     <g transform=\"translate(160.542969 256.715781)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.8125 72.90625 \r\nL 51.703125 72.90625 \r\nL 51.703125 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.109375 \r\nL 48.578125 43.109375 \r\nL 48.578125 34.8125 \r\nL 19.671875 34.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-70\"/>\r\n       <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n       <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n       <path d=\"M 31 75.875 \r\nQ 24.46875 64.65625 21.28125 53.65625 \r\nQ 18.109375 42.671875 18.109375 31.390625 \r\nQ 18.109375 20.125 21.3125 9.0625 \r\nQ 24.515625 -2 31 -13.1875 \r\nL 23.1875 -13.1875 \r\nQ 15.875 -1.703125 12.234375 9.375 \r\nQ 8.59375 20.453125 8.59375 31.390625 \r\nQ 8.59375 42.28125 12.203125 53.3125 \r\nQ 15.828125 64.359375 23.1875 75.875 \r\nz\r\n\" id=\"DejaVuSans-40\"/>\r\n       <path id=\"DejaVuSans-32\"/>\r\n       <path d=\"M 4.890625 31.390625 \r\nL 31.203125 31.390625 \r\nL 31.203125 23.390625 \r\nL 4.890625 23.390625 \r\nz\r\n\" id=\"DejaVuSans-45\"/>\r\n       <path d=\"M 53.515625 70.515625 \r\nL 53.515625 60.890625 \r\nQ 47.90625 63.578125 42.921875 64.890625 \r\nQ 37.9375 66.21875 33.296875 66.21875 \r\nQ 25.25 66.21875 20.875 63.09375 \r\nQ 16.5 59.96875 16.5 54.203125 \r\nQ 16.5 49.359375 19.40625 46.890625 \r\nQ 22.3125 44.4375 30.421875 42.921875 \r\nL 36.375 41.703125 \r\nQ 47.40625 39.59375 52.65625 34.296875 \r\nQ 57.90625 29 57.90625 20.125 \r\nQ 57.90625 9.515625 50.796875 4.046875 \r\nQ 43.703125 -1.421875 29.984375 -1.421875 \r\nQ 24.8125 -1.421875 18.96875 -0.25 \r\nQ 13.140625 0.921875 6.890625 3.21875 \r\nL 6.890625 13.375 \r\nQ 12.890625 10.015625 18.65625 8.296875 \r\nQ 24.421875 6.59375 29.984375 6.59375 \r\nQ 38.421875 6.59375 43.015625 9.90625 \r\nQ 47.609375 13.234375 47.609375 19.390625 \r\nQ 47.609375 24.75 44.3125 27.78125 \r\nQ 41.015625 30.8125 33.5 32.328125 \r\nL 27.484375 33.5 \r\nQ 16.453125 35.6875 11.515625 40.375 \r\nQ 6.59375 45.0625 6.59375 53.421875 \r\nQ 6.59375 63.09375 13.40625 68.65625 \r\nQ 20.21875 74.21875 32.171875 74.21875 \r\nQ 37.3125 74.21875 42.625 73.28125 \r\nQ 47.953125 72.359375 53.515625 70.515625 \r\nz\r\n\" id=\"DejaVuSans-83\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n       <path d=\"M 8.015625 75.875 \r\nL 15.828125 75.875 \r\nQ 23.140625 64.359375 26.78125 53.3125 \r\nQ 30.421875 42.28125 30.421875 31.390625 \r\nQ 30.421875 20.453125 26.78125 9.375 \r\nQ 23.140625 -1.703125 15.828125 -13.1875 \r\nL 8.015625 -13.1875 \r\nQ 14.5 -2 17.703125 9.0625 \r\nQ 20.90625 20.125 20.90625 31.390625 \r\nQ 20.90625 42.671875 17.703125 53.65625 \r\nQ 14.5 64.65625 8.015625 75.875 \r\nz\r\n\" id=\"DejaVuSans-41\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"117.822266\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"187.304688\" xlink:href=\"#DejaVuSans-40\"/>\r\n      <use x=\"226.318359\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"258.105469\" xlink:href=\"#DejaVuSans-49\"/>\r\n      <use x=\"321.728516\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-45\"/>\r\n      <use x=\"389.599609\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"421.386719\" xlink:href=\"#DejaVuSans-83\"/>\r\n      <use x=\"484.863281\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"546.386719\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"609.765625\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"661.865234\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"689.648438\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"728.857422\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"756.640625\" xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"815.820312\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"843.603516\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"882.8125\" xlink:href=\"#DejaVuSans-121\"/>\r\n      <use x=\"941.992188\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"973.779297\" xlink:href=\"#DejaVuSans-41\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_11\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m057f77c40a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m057f77c40a\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(20.878125 232.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m057f77c40a\" y=\"184.951219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 188.750437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m057f77c40a\" y=\"141.463219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 145.262437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m057f77c40a\" y=\"97.975219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 101.774437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m057f77c40a\" y=\"54.487219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 58.286437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m057f77c40a\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_18\">\r\n     <!-- TPR( Recall ) -->\r\n     <g transform=\"translate(14.798438 151.259062)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"61.083984\" xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"121.386719\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"190.869141\" xlink:href=\"#DejaVuSans-40\"/>\r\n      <use x=\"229.882812\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"261.669922\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"326.652344\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"388.175781\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"443.15625\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"504.435547\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"532.21875\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"560.001953\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"591.789062\" xlink:href=\"#DejaVuSans-41\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p0fe17390f5)\" d=\"M 43.78125 228.439219 \r\nL 43.891754 227.003488 \r\nL 43.899647 227.003488 \r\nL 44.010151 226.124469 \r\nL 44.014098 226.124469 \r\nL 44.124602 225.27475 \r\nL 44.144335 225.186848 \r\nL 44.250893 224.161326 \r\nL 44.274572 224.073424 \r\nL 44.365343 223.047902 \r\nL 44.404809 222.96 \r\nL 44.511367 221.905178 \r\nL 44.523207 221.817276 \r\nL 44.613978 221.11406 \r\nL 44.665283 221.055459 \r\nL 44.763948 220.469446 \r\nL 44.787627 220.381545 \r\nL 44.894185 219.678329 \r\nL 44.921811 219.590427 \r\nL 45.032315 219.092317 \r\nL 45.040208 219.004415 \r\nL 45.150712 218.271899 \r\nL 45.182285 218.183997 \r\nL 45.292789 217.510083 \r\nL 45.308575 217.451481 \r\nL 45.41908 217.070573 \r\nL 45.438813 216.982671 \r\nL 45.54537 216.425959 \r\nL 45.580889 216.338057 \r\nL 45.687447 215.605541 \r\nL 45.722966 215.517639 \r\nL 45.83347 214.667921 \r\nL 45.85715 214.580019 \r\nL 45.959761 213.935405 \r\nL 45.9716 213.935405 \r\nL 46.078158 213.261491 \r\nL 46.113677 213.173589 \r\nL 46.220235 212.353171 \r\nL 46.255754 212.265269 \r\nL 46.366258 211.620655 \r\nL 46.42151 211.532753 \r\nL 46.532014 211.151845 \r\nL 46.535961 211.151845 \r\nL 46.638572 210.360728 \r\nL 46.658305 210.272826 \r\nL 46.741183 210.00912 \r\nL 46.772756 210.00912 \r\nL 46.88326 209.218003 \r\nL 46.946405 209.130101 \r\nL 47.056909 208.719893 \r\nL 47.076642 208.631991 \r\nL 47.187146 208.338984 \r\nL 47.206879 208.251082 \r\nL 47.305544 207.723671 \r\nL 47.380529 207.635769 \r\nL 47.491033 207.137658 \r\nL 47.526552 207.049756 \r\nL 47.637056 206.493044 \r\nL 47.672575 206.405142 \r\nL 47.779133 205.877731 \r\nL 47.830438 205.789829 \r\nL 47.905423 204.969411 \r\nL 47.992248 204.88151 \r\nL 48.098806 204.324798 \r\nL 48.173791 204.236896 \r\nL 48.280348 203.562981 \r\nL 48.304028 203.475079 \r\nL 48.414532 203.123472 \r\nL 48.434265 203.03557 \r\nL 48.544769 202.625361 \r\nL 48.600021 202.56676 \r\nL 48.702632 202.156551 \r\nL 48.757884 202.068649 \r\nL 48.868388 201.453335 \r\nL 48.872335 201.453335 \r\nL 48.982839 200.867323 \r\nL 49.002572 200.779421 \r\nL 49.093343 200.369212 \r\nL 49.148595 200.28131 \r\nL 49.231473 200.017604 \r\nL 49.263046 200.017604 \r\nL 49.37355 199.578095 \r\nL 49.393283 199.490193 \r\nL 49.499841 198.87488 \r\nL 49.53536 198.786978 \r\nL 49.641917 198.259566 \r\nL 49.68533 198.171665 \r\nL 49.795834 197.790756 \r\nL 49.807674 197.702854 \r\nL 49.886605 197.439149 \r\nL 49.94975 197.380547 \r\nL 50.052361 196.765234 \r\nL 50.087881 196.677332 \r\nL 50.182599 196.355025 \r\nL 50.218118 196.296424 \r\nL 50.328622 196.003418 \r\nL 50.356248 195.915516 \r\nL 50.454912 195.446706 \r\nL 50.490432 195.358804 \r\nL 50.53779 195.183 \r\nL 50.640402 195.095098 \r\nL 50.746959 194.479785 \r\nL 50.794318 194.391883 \r\nL 50.87325 194.128177 \r\nL 50.956128 194.040275 \r\nL 51.015326 193.747269 \r\nL 51.102151 193.659367 \r\nL 51.204762 193.190557 \r\nL 51.236335 193.102655 \r\nL 51.342892 192.663145 \r\nL 51.382358 192.575244 \r\nL 51.492862 192.165035 \r\nL 51.544168 192.077133 \r\nL 51.642832 191.666924 \r\nL 51.694138 191.579022 \r\nL 51.792802 191.315316 \r\nL 51.863841 191.256715 \r\nL 51.974345 190.846506 \r\nL 51.990131 190.817206 \r\nL 52.088795 190.172592 \r\nL 52.16378 190.08469 \r\nL 52.274285 189.64518 \r\nL 52.341376 189.557278 \r\nL 52.447934 189.205671 \r\nL 52.47556 189.147069 \r\nL 52.586064 188.531756 \r\nL 52.664996 188.443854 \r\nL 52.767607 188.297351 \r\nL 52.803126 188.209449 \r\nL 52.909684 187.682038 \r\nL 52.968882 187.594136 \r\nL 53.059653 187.008123 \r\nL 53.142532 186.920221 \r\nL 53.21357 186.685816 \r\nL 53.268822 186.656516 \r\nL 53.371433 186.158405 \r\nL 53.414845 186.099804 \r\nL 53.481937 185.777497 \r\nL 53.564815 185.689595 \r\nL 53.671373 185.044981 \r\nL 53.746358 184.957079 \r\nL 53.856862 184.576171 \r\nL 53.904221 184.517569 \r\nL 54.002885 184.07806 \r\nL 54.07787 183.990158 \r\nL 54.188375 183.667851 \r\nL 54.231787 183.579949 \r\nL 54.318612 183.286943 \r\nL 54.417276 183.228342 \r\nL 54.523834 182.730231 \r\nL 54.555406 182.642329 \r\nL 54.658017 182.349323 \r\nL 54.729056 182.261421 \r\nL 54.81588 181.880512 \r\nL 54.894812 181.79261 \r\nL 55.005316 181.587506 \r\nL 55.028996 181.499604 \r\nL 55.107927 181.2945 \r\nL 55.230271 181.206598 \r\nL 55.332882 180.972193 \r\nL 55.40392 180.913591 \r\nL 55.510478 180.620585 \r\nL 55.569677 180.532683 \r\nL 55.680181 180.151775 \r\nL 55.699914 180.063873 \r\nL 55.806471 179.800167 \r\nL 55.826204 179.741566 \r\nL 55.932762 179.44856 \r\nL 55.99196 179.360658 \r\nL 56.078785 179.184854 \r\nL 56.126144 179.126253 \r\nL 56.236648 178.716044 \r\nL 56.244541 178.686743 \r\nL 56.351099 178.042129 \r\nL 56.418191 177.954228 \r\nL 56.485283 177.749123 \r\nL 56.583947 177.690522 \r\nL 56.694451 177.251012 \r\nL 56.72997 177.192411 \r\nL 56.816795 176.899405 \r\nL 56.911513 176.870104 \r\nL 57.022017 176.459895 \r\nL 57.06543 176.401294 \r\nL 57.175934 176.108288 \r\nL 57.223293 176.020386 \r\nL 57.333797 175.668778 \r\nL 57.385102 175.580876 \r\nL 57.487713 175.258569 \r\nL 57.546912 175.170667 \r\nL 57.602164 175.024164 \r\nL 57.720561 174.936262 \r\nL 57.823172 174.613955 \r\nL 57.909997 174.526053 \r\nL 57.953409 174.35025 \r\nL 58.123112 174.262348 \r\nL 58.22967 173.998642 \r\nL 58.284922 173.91074 \r\nL 58.383586 173.734936 \r\nL 58.458571 173.647034 \r\nL 58.569075 173.119623 \r\nL 58.612488 173.031721 \r\nL 58.722992 172.738715 \r\nL 58.750618 172.650813 \r\nL 58.857176 172.269905 \r\nL 58.892695 172.182003 \r\nL 58.983466 171.947598 \r\nL 59.054505 171.859696 \r\nL 59.145276 171.654591 \r\nL 59.263673 171.566689 \r\nL 59.342605 171.332284 \r\nL 59.41759 171.244382 \r\nL 59.520201 170.804873 \r\nL 59.571506 170.746272 \r\nL 59.68201 170.21886 \r\nL 59.776728 170.130958 \r\nL 59.867499 169.72075 \r\nL 59.938538 169.632848 \r\nL 60.049042 169.369142 \r\nL 60.100348 169.28124 \r\nL 60.210852 168.929632 \r\nL 60.234531 168.871031 \r\nL 60.333196 168.519424 \r\nL 60.384501 168.490123 \r\nL 60.479219 168.138515 \r\nL 60.554204 168.050613 \r\nL 60.660762 167.728306 \r\nL 60.696281 167.640405 \r\nL 60.779159 167.288797 \r\nL 60.917289 167.200895 \r\nL 61.015953 166.732085 \r\nL 61.098832 166.644183 \r\nL 61.193549 166.468379 \r\nL 61.236962 166.380477 \r\nL 61.347466 165.970268 \r\nL 61.394825 165.911667 \r\nL 61.493489 165.677262 \r\nL 61.568474 165.58936 \r\nL 61.643459 165.501458 \r\nL 61.694765 165.442857 \r\nL 61.801322 165.091249 \r\nL 61.828948 165.003348 \r\nL 61.931559 164.475936 \r\nL 61.955239 164.388034 \r\nL 62.030224 164.095028 \r\nL 62.172301 164.007126 \r\nL 62.243339 163.772721 \r\nL 62.373576 163.684819 \r\nL 62.440668 163.479715 \r\nL 62.551172 163.391813 \r\nL 62.64589 163.040205 \r\nL 62.689302 162.952303 \r\nL 62.744554 162.747199 \r\nL 62.874791 162.659297 \r\nL 62.969509 162.33699 \r\nL 63.0958 162.249088 \r\nL 63.190518 162.043984 \r\nL 63.285235 161.956082 \r\nL 63.39574 161.721677 \r\nL 63.466778 161.633775 \r\nL 63.529923 161.457971 \r\nL 63.644374 161.370069 \r\nL 63.727252 161.135664 \r\nL 63.818023 161.047762 \r\nL 63.916688 160.784056 \r\nL 63.964047 160.725455 \r\nL 64.066658 160.49105 \r\nL 64.169269 160.403148 \r\nL 64.267933 160.256645 \r\nL 64.370544 160.168743 \r\nL 64.457369 159.817135 \r\nL 64.579713 159.729234 \r\nL 64.690217 159.406927 \r\nL 64.702057 159.348325 \r\nL 64.804668 159.055319 \r\nL 64.923065 158.967417 \r\nL 65.025676 158.64511 \r\nL 65.061195 158.557208 \r\nL 65.14802 158.205601 \r\nL 65.230898 158.117699 \r\nL 65.270364 158.029797 \r\nL 65.372975 157.941895 \r\nL 65.4598 157.619588 \r\nL 65.538731 157.531686 \r\nL 65.593983 157.385183 \r\nL 65.692648 157.297281 \r\nL 65.779472 157.150778 \r\nL 65.862351 157.121477 \r\nL 65.972855 156.740569 \r\nL 66.04784 156.652667 \r\nL 66.142558 156.506164 \r\nL 66.241222 156.418262 \r\nL 66.343833 156.271759 \r\nL 66.399085 156.183857 \r\nL 66.489856 155.920151 \r\nL 66.572734 155.832249 \r\nL 66.639826 155.656446 \r\nL 66.734544 155.568544 \r\nL 66.833209 155.246237 \r\nL 66.959499 155.158335 \r\nL 67.06211 154.982531 \r\nL 67.144988 154.92393 \r\nL 67.247599 154.396518 \r\nL 67.318638 154.308616 \r\nL 67.417302 154.103512 \r\nL 67.52386 154.01561 \r\nL 67.622524 153.927708 \r\nL 67.689616 153.869107 \r\nL 67.80012 153.634702 \r\nL 67.839586 153.576101 \r\nL 67.918517 153.341696 \r\nL 68.08822 153.253794 \r\nL 68.175045 153.165892 \r\nL 68.230297 153.10729 \r\nL 68.321068 152.726382 \r\nL 68.356587 152.63848 \r\nL 68.451305 152.316173 \r\nL 68.534183 152.228271 \r\nL 68.605222 151.993866 \r\nL 68.684153 151.905964 \r\nL 68.778871 151.671559 \r\nL 68.917001 151.583658 \r\nL 68.9762 151.378553 \r\nL 69.066971 151.290651 \r\nL 69.173529 151.056246 \r\nL 69.244567 150.968344 \r\nL 69.355072 150.675338 \r\nL 69.398484 150.587436 \r\nL 69.465576 150.528835 \r\nL 69.560294 150.440933 \r\nL 69.643172 150.118626 \r\nL 69.71421 150.030724 \r\nL 69.808928 149.85492 \r\nL 69.899699 149.767018 \r\nL 69.958898 149.561914 \r\nL 70.053616 149.474012 \r\nL 70.144387 149.268907 \r\nL 70.29041 149.181006 \r\nL 70.365395 149.005202 \r\nL 70.424594 148.9173 \r\nL 70.515365 148.770797 \r\nL 70.606137 148.682895 \r\nL 70.704801 148.47779 \r\nL 70.807412 148.389888 \r\nL 70.874504 148.155483 \r\nL 71.087619 148.067582 \r\nL 71.17839 147.774575 \r\nL 71.312574 147.686673 \r\nL 71.387559 147.481569 \r\nL 71.482277 147.393667 \r\nL 71.588834 147.100661 \r\nL 71.624354 147.012759 \r\nL 71.726965 146.749053 \r\nL 71.798003 146.690452 \r\nL 71.904561 146.397445 \r\nL 71.94008 146.338844 \r\nL 72.030851 145.987237 \r\nL 72.09005 145.957936 \r\nL 72.180821 145.811433 \r\nL 72.275539 145.752831 \r\nL 72.386043 145.577028 \r\nL 72.433402 145.518426 \r\nL 72.543906 145.254721 \r\nL 72.583372 145.166819 \r\nL 72.638624 145.078917 \r\nL 72.741235 144.991015 \r\nL 72.839899 144.639407 \r\nL 72.903045 144.551506 \r\nL 72.950404 144.405002 \r\nL 73.053015 144.346401 \r\nL 73.163519 144.053395 \r\nL 73.250343 143.965493 \r\nL 73.321382 143.84829 \r\nL 73.39242 143.760388 \r\nL 73.471352 143.584585 \r\nL 73.550283 143.496683 \r\nL 73.652894 143.35018 \r\nL 73.743666 143.262278 \r\nL 73.755505 143.203676 \r\nL 73.988353 143.115774 \r\nL 74.090964 142.969271 \r\nL 74.150163 142.91067 \r\nL 74.181736 142.793468 \r\nL 74.276454 142.764167 \r\nL 74.386958 142.44186 \r\nL 74.41853 142.353958 \r\nL 74.469836 142.178154 \r\nL 74.552714 142.119553 \r\nL 74.663218 141.797246 \r\nL 74.753989 141.709344 \r\nL 74.864494 141.474939 \r\nL 74.963158 141.387037 \r\nL 75.057876 140.976828 \r\nL 75.164433 140.888926 \r\nL 75.274938 140.654521 \r\nL 75.499893 140.566619 \r\nL 75.602504 140.302914 \r\nL 75.673542 140.215012 \r\nL 75.776153 139.980607 \r\nL 75.926123 139.892705 \r\nL 76.024787 139.746202 \r\nL 76.123452 139.6583 \r\nL 76.222116 139.511797 \r\nL 76.308941 139.423895 \r\nL 76.407605 139.18949 \r\nL 76.466804 139.101588 \r\nL 76.565468 138.896483 \r\nL 76.668079 138.837882 \r\nL 76.774637 138.515575 \r\nL 76.810156 138.427673 \r\nL 76.877248 138.28117 \r\nL 76.995645 138.193268 \r\nL 77.08247 138.046765 \r\nL 77.216654 137.958863 \r\nL 77.327158 137.81236 \r\nL 77.362677 137.753759 \r\nL 77.473181 137.402151 \r\nL 77.5087 137.314249 \r\nL 77.603418 137.109145 \r\nL 77.745495 137.021243 \r\nL 77.83232 136.845439 \r\nL 77.919144 136.757537 \r\nL 78.029649 136.669635 \r\nL 78.084901 136.611034 \r\nL 78.191458 136.200825 \r\nL 78.234871 136.112923 \r\nL 78.286176 135.995721 \r\nL 78.475612 135.907819 \r\nL 78.578223 135.732015 \r\nL 78.668994 135.644113 \r\nL 78.775552 135.439009 \r\nL 78.913682 135.351107 \r\nL 79.000507 135.263205 \r\nL 79.079438 135.204603 \r\nL 79.189942 134.911597 \r\nL 79.304393 134.823695 \r\nL 79.395164 134.647891 \r\nL 79.493829 134.55999 \r\nL 79.5846 134.325584 \r\nL 79.667478 134.237683 \r\nL 79.766143 134.003278 \r\nL 79.864807 133.973977 \r\nL 79.971365 133.65167 \r\nL 80.042403 133.563768 \r\nL 80.141068 133.358664 \r\nL 80.208159 133.270762 \r\nL 80.212106 133.21216 \r\nL 80.401542 133.124259 \r\nL 80.508099 133.065657 \r\nL 80.559405 133.007056 \r\nL 80.654123 132.71405 \r\nL 80.76068 132.626148 \r\nL 80.808039 132.538246 \r\nL 80.965902 132.479645 \r\nL 81.07246 132.303841 \r\nL 81.261896 132.215939 \r\nL 81.356613 131.952233 \r\nL 81.463171 131.864331 \r\nL 81.538156 131.747129 \r\nL 81.699966 131.659227 \r\nL 81.802577 131.307619 \r\nL 81.877562 131.219717 \r\nL 81.988066 130.985312 \r\nL 82.08673 130.89741 \r\nL 82.169608 130.809508 \r\nL 82.288006 130.750907 \r\nL 82.394563 130.340698 \r\nL 82.441922 130.252796 \r\nL 82.520854 130.106293 \r\nL 82.643198 130.047692 \r\nL 82.749755 129.842588 \r\nL 82.927351 129.754686 \r\nL 83.037855 129.578882 \r\nL 83.164146 129.49098 \r\nL 83.27465 129.344477 \r\nL 83.452246 129.256575 \r\nL 83.543017 129.139372 \r\nL 83.653521 129.05147 \r\nL 83.728506 128.963569 \r\nL 83.831117 128.875667 \r\nL 83.894263 128.611961 \r\nL 84.044233 128.55336 \r\nL 84.111324 128.348255 \r\nL 84.221829 128.260353 \r\nL 84.332333 128.055249 \r\nL 84.383638 127.967347 \r\nL 84.490196 127.850145 \r\nL 84.573074 127.762243 \r\nL 84.675685 127.586439 \r\nL 84.774349 127.498537 \r\nL 84.884854 127.322733 \r\nL 84.940106 127.234831 \r\nL 85.022984 127.000426 \r\nL 85.090076 126.912524 \r\nL 85.161114 126.766021 \r\nL 85.35055 126.678119 \r\nL 85.417641 126.560917 \r\nL 85.611024 126.502315 \r\nL 85.717581 126.209309 \r\nL 85.891231 126.150708 \r\nL 85.950429 126.004205 \r\nL 86.029361 125.916303 \r\nL 86.139865 125.740499 \r\nL 86.163545 125.652597 \r\nL 86.266156 125.388891 \r\nL 86.629241 125.300989 \r\nL 86.723959 125.095885 \r\nL 86.783157 125.007983 \r\nL 86.842356 124.920081 \r\nL 87.110723 124.832179 \r\nL 87.213334 124.627075 \r\nL 87.304105 124.539173 \r\nL 87.40277 124.216866 \r\nL 87.489595 124.128964 \r\nL 87.525114 124.011762 \r\nL 87.623778 123.92386 \r\nL 87.651404 123.806657 \r\nL 87.825054 123.718755 \r\nL 87.923718 123.542951 \r\nL 88.046062 123.45505 \r\nL 88.15262 123.279246 \r\nL 88.211818 123.191344 \r\nL 88.26707 123.103442 \r\nL 88.42888 123.01554 \r\nL 88.527544 122.751834 \r\nL 88.606476 122.663932 \r\nL 88.713034 122.576031 \r\nL 88.752499 122.488129 \r\nL 88.85511 122.283024 \r\nL 88.937989 122.195122 \r\nL 88.989294 122.10722 \r\nL 89.194516 122.019318 \r\nL 89.297127 121.814214 \r\nL 89.356326 121.726312 \r\nL 89.462883 121.550508 \r\nL 89.565494 121.462606 \r\nL 89.672052 121.316103 \r\nL 89.707571 121.228201 \r\nL 89.774663 121.081698 \r\nL 89.822022 121.081698 \r\nL 89.908847 120.847293 \r\nL 90.078549 120.759391 \r\nL 90.185107 120.495686 \r\nL 90.275878 120.407784 \r\nL 90.279825 120.319882 \r\nL 90.473207 120.23198 \r\nL 90.571872 120.085477 \r\nL 90.674483 119.997575 \r\nL 90.773147 119.821771 \r\nL 90.816559 119.733869 \r\nL 90.907331 119.499464 \r\nL 91.195431 119.411562 \r\nL 91.282256 119.118556 \r\nL 91.475638 119.030654 \r\nL 91.562462 118.85485 \r\nL 91.850563 118.766948 \r\nL 91.953174 118.532543 \r\nL 92.122876 118.444641 \r\nL 92.221541 118.210236 \r\nL 92.335992 118.122334 \r\nL 92.43071 117.91723 \r\nL 92.529374 117.858629 \r\nL 92.608306 117.712125 \r\nL 92.817474 117.624223 \r\nL 92.927978 117.419119 \r\nL 93.074002 117.331217 \r\nL 93.164773 117.272616 \r\nL 93.279224 117.184714 \r\nL 93.381835 116.97961 \r\nL 93.452873 116.891708 \r\nL 93.559431 116.715904 \r\nL 93.768599 116.628002 \r\nL 93.843584 116.422897 \r\nL 93.993554 116.334996 \r\nL 94.092219 116.07129 \r\nL 94.155364 115.983388 \r\nL 94.261921 115.866185 \r\nL 94.348746 115.778284 \r\nL 94.439517 115.661081 \r\nL 94.569754 115.63178 \r\nL 94.676312 115.426676 \r\nL 94.774976 115.338774 \r\nL 94.846015 115.192271 \r\nL 95.003878 115.13367 \r\nL 95.090703 114.752761 \r\nL 95.232779 114.664859 \r\nL 95.343284 114.430454 \r\nL 95.453788 114.342553 \r\nL 95.564292 114.137448 \r\nL 95.698476 114.049546 \r\nL 95.753728 113.932344 \r\nL 96.08524 113.844442 \r\nL 96.156279 113.727239 \r\nL 96.341768 113.639337 \r\nL 96.38518 113.551435 \r\nL 96.566723 113.463534 \r\nL 96.586456 113.404932 \r\nL 96.732479 113.31703 \r\nL 96.839036 113.141227 \r\nL 96.894289 113.053325 \r\nL 96.957434 112.84822 \r\nL 97.087671 112.760318 \r\nL 97.194228 112.584515 \r\nL 97.348145 112.496613 \r\nL 97.41129 112.320809 \r\nL 97.521794 112.232907 \r\nL 97.620459 112.057103 \r\nL 97.671764 111.969201 \r\nL 97.778322 111.793397 \r\nL 97.896719 111.705496 \r\nL 97.98749 111.529692 \r\nL 98.074315 111.47109 \r\nL 98.13746 111.324587 \r\nL 98.275591 111.236685 \r\nL 98.362415 111.119483 \r\nL 98.445293 111.031581 \r\nL 98.520278 110.797176 \r\nL 98.630783 110.709274 \r\nL 98.729447 110.621372 \r\nL 98.938616 110.53347 \r\nL 98.989921 110.416268 \r\nL 99.218823 110.328366 \r\nL 99.313541 110.181863 \r\nL 99.431938 110.093961 \r\nL 99.542442 109.830255 \r\nL 99.716091 109.742353 \r\nL 99.79897 109.537249 \r\nL 99.976566 109.449347 \r\nL 100.043657 109.244242 \r\nL 100.276505 109.15634 \r\nL 100.379116 109.039138 \r\nL 100.438315 108.980537 \r\nL 100.509353 108.863334 \r\nL 100.600125 108.804733 \r\nL 100.643537 108.68753 \r\nL 100.809293 108.599628 \r\nL 100.809293 108.541027 \r\nL 101.050035 108.453125 \r\nL 101.160539 108.394524 \r\nL 101.421013 108.306622 \r\nL 101.484158 108.21872 \r\nL 101.543357 108.130818 \r\nL 101.630181 108.013616 \r\nL 101.736739 107.925714 \r\nL 101.847243 107.720609 \r\nL 101.965641 107.632707 \r\nL 102.076145 107.2811 \r\nL 102.159023 107.193198 \r\nL 102.182702 107.134597 \r\nL 102.380031 107.075995 \r\nL 102.458963 106.782989 \r\nL 102.561574 106.695087 \r\nL 102.664185 106.607185 \r\nL 102.798368 106.519283 \r\nL 102.900979 106.34348 \r\nL 103.074629 106.255578 \r\nL 103.173293 106.109075 \r\nL 103.331156 106.021173 \r\nL 103.437714 105.845369 \r\nL 103.595577 105.786768 \r\nL 103.678455 105.581663 \r\nL 103.75344 105.493761 \r\nL 103.856051 105.376559 \r\nL 103.91525 105.288657 \r\nL 104.025754 105.054252 \r\nL 104.116525 104.96635 \r\nL 104.223083 104.761245 \r\nL 104.270442 104.673344 \r\nL 104.373053 104.52684 \r\nL 104.499343 104.438938 \r\nL 104.594061 104.292435 \r\nL 104.732191 104.204533 \r\nL 104.838749 104.116631 \r\nL 104.901894 104.02873 \r\nL 104.937413 103.882226 \r\nL 105.040024 103.823625 \r\nL 105.146582 103.501318 \r\nL 105.446522 103.413416 \r\nL 105.509667 103.325514 \r\nL 105.616225 103.237612 \r\nL 105.718836 103.061809 \r\nL 105.963523 102.973907 \r\nL 106.022722 102.886005 \r\nL 106.180585 102.798103 \r\nL 106.291089 102.622299 \r\nL 106.425273 102.534397 \r\nL 106.496311 102.358593 \r\nL 106.666014 102.270692 \r\nL 106.737053 102.065587 \r\nL 106.890969 101.977685 \r\nL 106.997527 101.772581 \r\nL 107.104084 101.684679 \r\nL 107.163283 101.538176 \r\nL 107.329039 101.450274 \r\nL 107.415864 101.333071 \r\nL 107.530315 101.245169 \r\nL 107.613193 101.127967 \r\nL 107.802628 101.040065 \r\nL 107.913133 100.80566 \r\nL 107.996011 100.717758 \r\nL 108.106515 100.512654 \r\nL 108.280164 100.424752 \r\nL 108.374882 100.36615 \r\nL 108.627463 100.278249 \r\nL 108.737967 100.131745 \r\nL 108.816899 100.043843 \r\nL 108.91951 99.89734 \r\nL 109.041854 99.809438 \r\nL 109.148411 99.692236 \r\nL 109.278648 99.604334 \r\nL 109.377313 99.457831 \r\nL 109.428618 99.369929 \r\nL 109.539123 99.194125 \r\nL 109.606214 99.106223 \r\nL 109.685146 98.989021 \r\nL 109.882475 98.901119 \r\nL 109.965353 98.783916 \r\nL 110.064017 98.696014 \r\nL 110.158735 98.49091 \r\nL 110.391583 98.403008 \r\nL 110.419209 98.344407 \r\nL 110.715203 98.256505 \r\nL 110.802027 97.992799 \r\nL 110.916478 97.934198 \r\nL 111.011196 97.787695 \r\nL 111.188792 97.699793 \r\nL 111.232204 97.58259 \r\nL 111.429533 97.494688 \r\nL 111.528198 97.377486 \r\nL 111.666328 97.289584 \r\nL 111.72158 97.201682 \r\nL 111.824191 97.172381 \r\nL 111.911016 97.025878 \r\nL 112.053092 96.937976 \r\nL 112.112291 96.820774 \r\nL 112.44775 96.732872 \r\nL 112.558254 96.64497 \r\nL 112.696384 96.557068 \r\nL 112.696384 96.527767 \r\nL 113.043683 96.439866 \r\nL 113.04763 96.381264 \r\nL 113.244959 96.293362 \r\nL 113.339677 96.146859 \r\nL 113.513326 96.058957 \r\nL 113.61199 95.883154 \r\nL 113.655403 95.795252 \r\nL 113.750121 95.590147 \r\nL 113.888251 95.502245 \r\nL 113.998755 95.385043 \r\nL 114.109259 95.297141 \r\nL 114.140832 95.209239 \r\nL 114.334214 95.121337 \r\nL 114.432878 95.062736 \r\nL 114.51181 94.974834 \r\nL 114.598635 94.769729 \r\nL 114.859109 94.681828 \r\nL 114.914361 94.623226 \r\nL 115.052491 94.564625 \r\nL 115.135369 94.388821 \r\nL 115.23798 94.300919 \r\nL 115.340591 94.037214 \r\nL 115.447149 93.949312 \r\nL 115.557653 93.773508 \r\nL 115.664211 93.685606 \r\nL 115.664211 93.656305 \r\nL 115.814181 93.568403 \r\nL 115.904952 93.333998 \r\nL 115.97599 93.246097 \r\nL 116.074655 93.040992 \r\nL 116.244358 92.95309 \r\nL 116.331182 92.865188 \r\nL 116.418007 92.777286 \r\nL 116.496938 92.660084 \r\nL 116.757413 92.572182 \r\nL 116.820558 92.454979 \r\nL 116.927115 92.396378 \r\nL 116.974474 92.249875 \r\nL 117.092872 92.161973 \r\nL 117.199429 92.044771 \r\nL 117.487529 91.956869 \r\nL 117.566461 91.868967 \r\nL 117.755897 91.781065 \r\nL 117.854561 91.722464 \r\nL 117.965065 91.634562 \r\nL 118.059783 91.429457 \r\nL 118.284738 91.341555 \r\nL 118.391296 91.224353 \r\nL 118.718862 91.136451 \r\nL 118.829366 91.019248 \r\nL 118.856992 90.960647 \r\nL 118.947763 90.696941 \r\nL 119.078 90.60904 \r\nL 119.156932 90.433236 \r\nL 119.362154 90.345334 \r\nL 119.448978 90.286733 \r\nL 119.614735 90.198831 \r\nL 119.626574 90.110929 \r\nL 119.772598 90.023027 \r\nL 119.875209 89.905824 \r\nL 120.048858 89.817922 \r\nL 120.076484 89.730021 \r\nL 120.281706 89.642119 \r\nL 120.368531 89.554217 \r\nL 120.569806 89.466315 \r\nL 120.625058 89.349112 \r\nL 120.838174 89.26121 \r\nL 120.909212 89.173308 \r\nL 121.071022 89.085407 \r\nL 121.181526 88.997505 \r\nL 121.359122 88.909603 \r\nL 121.461733 88.821701 \r\nL 121.710367 88.733799 \r\nL 121.805085 88.587296 \r\nL 121.978734 88.499394 \r\nL 122.061613 88.382191 \r\nL 122.160277 88.352891 \r\nL 122.247102 88.059884 \r\nL 122.381285 87.971983 \r\nL 122.47995 87.796179 \r\nL 122.661492 87.708277 \r\nL 122.736477 87.620375 \r\nL 122.941699 87.532473 \r\nL 123.004845 87.444571 \r\nL 123.190334 87.356669 \r\nL 123.296891 87.239467 \r\nL 123.383716 87.151565 \r\nL 123.474487 87.063663 \r\nL 123.521846 86.975761 \r\nL 123.620511 86.858558 \r\nL 123.884931 86.770657 \r\nL 123.959916 86.682755 \r\nL 124.113833 86.594853 \r\nL 124.212497 86.506951 \r\nL 124.358521 86.419049 \r\nL 124.445345 86.331147 \r\nL 124.615048 86.243245 \r\nL 124.721606 86.067441 \r\nL 124.883415 85.979539 \r\nL 124.986026 85.803736 \r\nL 125.187302 85.715834 \r\nL 125.246501 85.598631 \r\nL 125.404364 85.510729 \r\nL 125.447776 85.452128 \r\nL 125.621425 85.364226 \r\nL 125.704304 85.276324 \r\nL 126.035816 85.188422 \r\nL 126.14632 85.041919 \r\nL 126.335756 84.954017 \r\nL 126.442314 84.807514 \r\nL 126.72252 84.719612 \r\nL 126.769879 84.426606 \r\nL 126.951422 84.368005 \r\nL 126.979048 84.280103 \r\nL 127.160591 84.192201 \r\nL 127.259255 83.987096 \r\nL 127.377652 83.899194 \r\nL 127.409225 83.811293 \r\nL 127.63418 83.723391 \r\nL 127.693379 83.606188 \r\nL 127.831509 83.518286 \r\nL 127.902547 83.401084 \r\nL 128.080143 83.313182 \r\nL 128.166968 83.22528 \r\nL 128.545839 83.137378 \r\nL 128.628717 83.020175 \r\nL 128.723435 82.932274 \r\nL 128.833939 82.844372 \r\nL 128.940497 82.75647 \r\nL 129.039161 82.609967 \r\nL 129.327262 82.522065 \r\nL 129.433819 82.346261 \r\nL 129.52459 82.258359 \r\nL 129.599575 82.170457 \r\nL 129.678507 82.111856 \r\nL 129.789011 81.936052 \r\nL 130.049485 81.877451 \r\nL 130.104737 81.760248 \r\nL 130.266547 81.672346 \r\nL 130.365211 81.555144 \r\nL 130.428357 81.467242 \r\nL 130.534914 81.320739 \r\nL 130.728297 81.232837 \r\nL 130.732243 81.174236 \r\nL 131.040076 81.086334 \r\nL 131.079542 80.969131 \r\nL 131.288711 80.881229 \r\nL 131.288711 80.851929 \r\nL 131.620223 80.764027 \r\nL 131.718887 80.705425 \r\nL 131.947789 80.617524 \r\nL 132.058293 80.412419 \r\nL 132.299034 80.324517 \r\nL 132.405592 80.207315 \r\nL 132.642387 80.119413 \r\nL 132.685799 80.060812 \r\nL 132.989685 80.00221 \r\nL 133.068617 79.885008 \r\nL 133.325144 79.797106 \r\nL 133.384343 79.738505 \r\nL 133.668497 79.650603 \r\nL 133.755321 79.474799 \r\nL 133.893452 79.386897 \r\nL 133.893452 79.357596 \r\nL 134.193392 79.269694 \r\nL 134.292056 79.181793 \r\nL 134.591996 79.093891 \r\nL 134.591996 79.06459 \r\nL 134.781432 78.976688 \r\nL 134.884043 78.800884 \r\nL 135.081371 78.742283 \r\nL 135.160303 78.537179 \r\nL 135.432617 78.449277 \r\nL 135.495762 78.390675 \r\nL 135.712824 78.302774 \r\nL 135.803595 78.15627 \r\nL 136.004871 78.068368 \r\nL 136.103535 77.951166 \r\nL 136.170627 77.863264 \r\nL 136.221932 77.746061 \r\nL 136.415315 77.65816 \r\nL 136.521872 77.570258 \r\nL 136.699468 77.482356 \r\nL 136.806026 77.394454 \r\nL 137.133592 77.306552 \r\nL 137.224363 77.21865 \r\nL 137.319081 77.130748 \r\nL 137.421692 77.042846 \r\nL 137.508517 76.954944 \r\nL 137.619021 76.837742 \r\nL 137.772937 76.74984 \r\nL 137.840029 76.632637 \r\nL 138.088663 76.544736 \r\nL 138.183381 76.486134 \r\nL 138.487268 76.398232 \r\nL 138.550413 76.339631 \r\nL 138.870086 76.251729 \r\nL 138.976643 76.163827 \r\nL 139.158186 76.075925 \r\nL 139.260797 75.958723 \r\nL 139.371301 75.870821 \r\nL 139.446286 75.724318 \r\nL 139.576523 75.636416 \r\nL 139.671241 75.519213 \r\nL 139.809371 75.431311 \r\nL 139.904089 75.37271 \r\nL 140.054059 75.284808 \r\nL 140.125097 75.196906 \r\nL 140.472396 75.109004 \r\nL 140.563167 74.991802 \r\nL 140.863107 74.9039 \r\nL 140.942039 74.757397 \r\nL 140.993344 74.698796 \r\nL 141.092009 74.493691 \r\nL 141.241979 74.405789 \r\nL 141.297231 74.317887 \r\nL 141.695835 74.229985 \r\nL 141.719515 74.112783 \r\nL 141.948416 74.024881 \r\nL 142.054974 73.849077 \r\nL 142.173371 73.761175 \r\nL 142.272035 73.643973 \r\nL 142.433845 73.556071 \r\nL 142.528563 73.468169 \r\nL 142.761411 73.380267 \r\nL 142.800877 73.292365 \r\nL 143.002152 73.204463 \r\nL 143.10871 73.02866 \r\nL 143.24684 72.940758 \r\nL 143.357344 72.823555 \r\nL 143.641498 72.735653 \r\nL 143.748055 72.501248 \r\nL 143.913812 72.413346 \r\nL 144.008529 72.296144 \r\nL 144.308469 72.208242 \r\nL 144.391347 72.091039 \r\nL 144.59657 72.003137 \r\nL 144.695234 71.827334 \r\nL 144.939922 71.739432 \r\nL 145.007014 71.592928 \r\nL 145.121464 71.505027 \r\nL 145.192503 71.446425 \r\nL 145.354312 71.358523 \r\nL 145.445084 71.270622 \r\nL 145.535855 71.18272 \r\nL 145.595054 71.094818 \r\nL 145.741077 71.006916 \r\nL 145.827902 70.919014 \r\nL 145.993658 70.831112 \r\nL 146.104162 70.772511 \r\nL 146.258079 70.713909 \r\nL 146.305438 70.596707 \r\nL 146.700095 70.508805 \r\nL 146.731668 70.420903 \r\nL 146.932943 70.333001 \r\nL 147.000035 70.215799 \r\nL 147.299975 70.127897 \r\nL 147.382853 70.069296 \r\nL 147.769618 69.981394 \r\nL 147.880122 69.776289 \r\nL 148.065611 69.688387 \r\nL 148.168222 69.512584 \r\nL 148.345818 69.424682 \r\nL 148.420803 69.307479 \r\nL 148.779942 69.219577 \r\nL 148.886499 69.102375 \r\nL 149.00095 69.043773 \r\nL 149.040416 68.89727 \r\nL 149.356142 68.809368 \r\nL 149.415341 68.721466 \r\nL 149.616616 68.633565 \r\nL 149.691601 68.574963 \r\nL 149.869197 68.516362 \r\nL 149.916556 68.340558 \r\nL 150.082312 68.281957 \r\nL 150.169137 68.194055 \r\nL 150.299374 68.106153 \r\nL 150.390145 67.95965 \r\nL 150.571688 67.871748 \r\nL 150.642726 67.754546 \r\nL 150.733497 67.666644 \r\nL 150.792696 67.578742 \r\nL 150.91504 67.49084 \r\nL 150.997918 67.432239 \r\nL 151.06501 67.344337 \r\nL 151.132102 67.256435 \r\nL 151.270232 67.168533 \r\nL 151.376789 66.992729 \r\nL 151.507027 66.904827 \r\nL 151.617531 66.816925 \r\nL 151.874058 66.729023 \r\nL 151.960883 66.611821 \r\nL 152.095067 66.523919 \r\nL 152.134532 66.465318 \r\nL 152.477885 66.377416 \r\nL 152.509457 66.318814 \r\nL 152.718626 66.230913 \r\nL 152.730466 66.172311 \r\nL 153.121177 66.084409 \r\nL 153.196162 65.967207 \r\nL 153.622392 65.879305 \r\nL 153.669751 65.762102 \r\nL 153.99337 65.674201 \r\nL 154.103874 65.556998 \r\nL 154.28147 65.469096 \r\nL 154.380135 65.381194 \r\nL 154.652449 65.293292 \r\nL 154.660342 65.234691 \r\nL 155.090519 65.146789 \r\nL 155.19313 65.000286 \r\nL 155.33126 64.912384 \r\nL 155.406245 64.73658 \r\nL 155.745651 64.648678 \r\nL 155.848262 64.531476 \r\nL 156.298171 64.443574 \r\nL 156.404729 64.384973 \r\nL 156.708616 64.297071 \r\nL 156.716509 64.23847 \r\nL 156.921731 64.150568 \r\nL 157.016449 64.033365 \r\nL 157.292709 63.945463 \r\nL 157.363747 63.886862 \r\nL 157.537397 63.79896 \r\nL 157.636061 63.652457 \r\nL 157.868909 63.564555 \r\nL 157.975467 63.418052 \r\nL 158.346445 63.33015 \r\nL 158.441163 63.242248 \r\nL 158.626652 63.154346 \r\nL 158.737156 63.037144 \r\nL 158.867393 62.949242 \r\nL 158.970004 62.89064 \r\nL 159.17128 62.802738 \r\nL 159.254158 62.744137 \r\nL 159.427807 62.656235 \r\nL 159.538312 62.568333 \r\nL 159.846145 62.480432 \r\nL 159.952702 62.333928 \r\nL 160.225016 62.246026 \r\nL 160.307894 62.158125 \r\nL 160.465757 62.070223 \r\nL 160.568368 61.95302 \r\nL 160.726231 61.865118 \r\nL 160.832789 61.777216 \r\nL 161.349791 61.689314 \r\nL 161.397149 61.630713 \r\nL 161.728662 61.542811 \r\nL 161.787861 61.454909 \r\nL 162.040442 61.367007 \r\nL 162.150946 61.220504 \r\nL 162.391687 61.132602 \r\nL 162.391687 61.103302 \r\nL 162.628482 61.0154 \r\nL 162.727146 60.898197 \r\nL 162.967887 60.810295 \r\nL 163.070498 60.634492 \r\nL 163.374385 60.54659 \r\nL 163.374385 60.487988 \r\nL 163.658538 60.400087 \r\nL 163.71379 60.341485 \r\nL 163.970318 60.253583 \r\nL 163.970318 60.224283 \r\nL 164.605717 60.136381 \r\nL 164.605717 60.10708 \r\nL 164.988535 60.019178 \r\nL 164.988535 59.989878 \r\nL 165.548949 59.901976 \r\nL 165.560789 59.843375 \r\nL 165.805476 59.784773 \r\nL 165.908087 59.696871 \r\nL 166.065951 59.608969 \r\nL 166.10147 59.550368 \r\nL 166.247493 59.462466 \r\nL 166.354051 59.374564 \r\nL 166.571112 59.286662 \r\nL 166.571112 59.257362 \r\nL 167.273603 59.16946 \r\nL 167.352535 59.110859 \r\nL 167.549864 59.022957 \r\nL 167.609062 58.935055 \r\nL 167.782712 58.847153 \r\nL 167.885323 58.72995 \r\nL 168.011613 58.642049 \r\nL 168.114224 58.524846 \r\nL 168.386538 58.436944 \r\nL 168.386538 58.407643 \r\nL 168.765409 58.349042 \r\nL 168.871967 58.23184 \r\nL 169.164014 58.143938 \r\nL 169.207426 58.085337 \r\nL 169.396862 58.026735 \r\nL 169.416595 57.938833 \r\nL 169.649443 57.850931 \r\nL 169.724428 57.733729 \r\nL 170.091459 57.645827 \r\nL 170.111192 57.587226 \r\nL 170.446651 57.499324 \r\nL 170.446651 57.440723 \r\nL 170.809737 57.352821 \r\nL 170.813683 57.294219 \r\nL 171.054424 57.206318 \r\nL 171.085997 57.089115 \r\nL 171.366204 57.030514 \r\nL 171.366204 56.971912 \r\nL 171.618785 56.913311 \r\nL 171.662197 56.825409 \r\nL 172.230504 56.766808 \r\nL 172.317329 56.532403 \r\nL 172.510711 56.444501 \r\nL 172.538337 56.327299 \r\nL 172.751453 56.239397 \r\nL 172.84617 56.151495 \r\nL 173.00798 56.063593 \r\nL 173.043499 56.004992 \r\nL 173.252668 55.91709 \r\nL 173.260561 55.858488 \r\nL 173.615753 55.799887 \r\nL 173.639433 55.711985 \r\nL 174.049877 55.624083 \r\nL 174.089342 55.536181 \r\nL 174.562932 55.44828 \r\nL 174.590558 55.360378 \r\nL 174.854978 55.272476 \r\nL 174.957589 55.125973 \r\nL 175.281209 55.038071 \r\nL 175.38382 54.891567 \r\nL 175.561416 54.803666 \r\nL 175.561416 54.774365 \r\nL 175.94818 54.686463 \r\nL 176.058684 54.569261 \r\nL 176.437556 54.481359 \r\nL 176.54806 54.393457 \r\nL 176.68619 54.305555 \r\nL 176.69803 54.188352 \r\nL 176.934825 54.10045 \r\nL 176.994023 54.012548 \r\nL 177.143993 53.924647 \r\nL 177.151886 53.866045 \r\nL 177.361055 53.778143 \r\nL 177.471559 53.660941 \r\nL 177.775446 53.573039 \r\nL 177.854377 53.485137 \r\nL 178.028027 53.397235 \r\nL 178.122744 53.338634 \r\nL 178.371379 53.250732 \r\nL 178.458204 53.16283 \r\nL 178.6358 53.074928 \r\nL 178.679212 52.987026 \r\nL 179.022564 52.899124 \r\nL 179.046244 52.811223 \r\nL 179.314611 52.723321 \r\nL 179.322504 52.664719 \r\nL 179.796093 52.576817 \r\nL 179.796093 52.547517 \r\nL 180.174965 52.459615 \r\nL 180.273629 52.371713 \r\nL 180.573569 52.283811 \r\nL 180.660394 52.195909 \r\nL 180.873509 52.108007 \r\nL 180.97612 52.020105 \r\nL 181.220808 51.932204 \r\nL 181.307632 51.844302 \r\nL 181.429976 51.7564 \r\nL 181.508908 51.639197 \r\nL 182.144307 51.551295 \r\nL 182.246918 51.375491 \r\nL 182.444247 51.28759 \r\nL 182.471873 51.228988 \r\nL 182.775759 51.141086 \r\nL 182.842851 51.082485 \r\nL 182.961248 50.994583 \r\nL 183.0165 50.906681 \r\nL 183.16647 50.818779 \r\nL 183.16647 50.789479 \r\nL 183.620327 50.701577 \r\nL 183.632167 50.584374 \r\nL 184.212313 50.496472 \r\nL 184.322818 50.349969 \r\nL 184.776674 50.262067 \r\nL 184.81614 50.203466 \r\nL 185.305515 50.144865 \r\nL 185.40418 50.027662 \r\nL 185.944861 49.93976 \r\nL 185.944861 49.91046 \r\nL 186.375038 49.822558 \r\nL 186.450023 49.734656 \r\nL 186.793375 49.646754 \r\nL 186.793375 49.617453 \r\nL 187.807645 49.529552 \r\nL 187.815539 49.47095 \r\nL 188.383846 49.383048 \r\nL 188.383846 49.324447 \r\nL 188.865328 49.236545 \r\nL 188.94426 49.148643 \r\nL 189.161321 49.060741 \r\nL 189.23236 49.00214 \r\nL 189.638857 48.914238 \r\nL 189.698056 48.826336 \r\nL 189.926957 48.738434 \r\nL 189.954583 48.679833 \r\nL 190.266363 48.591931 \r\nL 190.266363 48.562631 \r\nL 190.688647 48.474729 \r\nL 190.783365 48.386827 \r\nL 191.103038 48.328226 \r\nL 191.174076 48.240324 \r\nL 191.592413 48.152422 \r\nL 191.671345 48.035219 \r\nL 192.129148 47.947317 \r\nL 192.219919 47.859415 \r\nL 192.464607 47.771514 \r\nL 192.4725 47.712912 \r\nL 192.744814 47.62501 \r\nL 192.819799 47.537109 \r\nL 193.040807 47.449207 \r\nL 193.064486 47.332004 \r\nL 193.182884 47.244102 \r\nL 193.285495 47.1562 \r\nL 193.557809 47.068298 \r\nL 193.644633 46.921795 \r\nL 194.08665 46.833893 \r\nL 194.169528 46.775292 \r\nL 194.493147 46.68739 \r\nL 194.493147 46.65809 \r\nL 194.828607 46.570188 \r\nL 194.828607 46.540887 \r\nL 195.345608 46.452985 \r\nL 195.365341 46.394384 \r\nL 195.708693 46.306482 \r\nL 195.736319 46.21858 \r\nL 196.233588 46.130678 \r\nL 196.292787 46.042776 \r\nL 196.651925 45.954874 \r\nL 196.695338 45.896273 \r\nL 196.896613 45.808371 \r\nL 196.896613 45.779071 \r\nL 197.243912 45.691169 \r\nL 197.287324 45.603267 \r\nL 197.792486 45.515365 \r\nL 197.792486 45.486064 \r\nL 198.135838 45.398162 \r\nL 198.246343 45.28096 \r\nL 198.435778 45.193058 \r\nL 198.483137 45.105156 \r\nL 198.936994 45.017254 \r\nL 199.015925 44.900052 \r\nL 199.343491 44.81215 \r\nL 199.438209 44.724248 \r\nL 199.730256 44.636346 \r\nL 199.730256 44.607045 \r\nL 200.026249 44.519143 \r\nL 200.026249 44.489843 \r\nL 200.484052 44.401941 \r\nL 200.574823 44.284738 \r\nL 200.862923 44.226137 \r\nL 200.862923 44.167536 \r\nL 201.2931 44.079634 \r\nL 201.372032 43.991732 \r\nL 201.802209 43.90383 \r\nL 201.802209 43.845229 \r\nL 202.062683 43.786627 \r\nL 202.062683 43.728026 \r\nL 202.851998 43.640124 \r\nL 202.851998 43.610824 \r\nL 203.428199 43.522922 \r\nL 203.475558 43.43502 \r\nL 203.672887 43.347118 \r\nL 203.755765 43.288517 \r\nL 204.055705 43.200615 \r\nL 204.158316 43.112713 \r\nL 204.746356 43.024811 \r\nL 204.746356 42.99551 \r\nL 205.448846 42.907608 \r\nL 205.555404 42.790406 \r\nL 206.163177 42.702504 \r\nL 206.273681 42.614602 \r\nL 207.043264 42.5267 \r\nL 207.066943 42.468099 \r\nL 207.587891 42.380197 \r\nL 207.678663 42.262995 \r\nL 207.943083 42.175093 \r\nL 207.943083 42.145792 \r\nL 208.140412 42.05789 \r\nL 208.187771 41.999289 \r\nL 208.594269 41.911387 \r\nL 208.637681 41.823485 \r\nL 208.996819 41.735583 \r\nL 209.107324 41.58908 \r\nL 209.375691 41.501178 \r\nL 209.375691 41.442577 \r\nL 209.746669 41.354675 \r\nL 209.746669 41.325374 \r\nL 210.295243 41.237472 \r\nL 210.358389 41.178871 \r\nL 210.713581 41.090969 \r\nL 210.76094 41.003067 \r\nL 211.384499 40.915165 \r\nL 211.455537 40.856564 \r\nL 211.78705 40.768662 \r\nL 211.873874 40.68076 \r\nL 212.083043 40.592858 \r\nL 212.189601 40.475656 \r\nL 212.730282 40.387754 \r\nL 212.730282 40.358453 \r\nL 213.606422 40.270551 \r\nL 213.606422 40.241251 \r\nL 214.154996 40.153349 \r\nL 214.210248 40.065447 \r\nL 214.70357 39.977545 \r\nL 214.814074 39.889643 \r\nL 215.244251 39.801741 \r\nL 215.283717 39.74314 \r\nL 215.559978 39.655238 \r\nL 215.559978 39.625938 \r\nL 216.266415 39.538036 \r\nL 216.266415 39.508735 \r\nL 216.586088 39.420833 \r\nL 216.601874 39.362232 \r\nL 217.063624 39.27433 \r\nL 217.138609 39.215729 \r\nL 217.651664 39.127827 \r\nL 217.683236 39.069225 \r\nL 218.287063 38.981324 \r\nL 218.287063 38.952023 \r\nL 218.835637 38.864121 \r\nL 218.835637 38.83482 \r\nL 219.257921 38.746919 \r\nL 219.325012 38.659017 \r\nL 219.897266 38.571115 \r\nL 219.952518 38.453912 \r\nL 220.33139 38.36601 \r\nL 220.406375 38.307409 \r\nL 220.556345 38.248808 \r\nL 220.651062 38.160906 \r\nL 220.879964 38.073004 \r\nL 220.951002 38.014403 \r\nL 221.558775 37.926501 \r\nL 221.6456 37.8679 \r\nL 222.07183 37.779998 \r\nL 222.07183 37.750697 \r\nL 222.442809 37.662795 \r\nL 222.442809 37.633494 \r\nL 222.80984 37.545593 \r\nL 222.80984 37.516292 \r\nL 223.224231 37.42839 \r\nL 223.263697 37.369789 \r\nL 223.658354 37.281887 \r\nL 223.693874 37.223286 \r\nL 224.151677 37.135384 \r\nL 224.250341 37.047482 \r\nL 224.613426 36.95958 \r\nL 224.72393 36.871678 \r\nL 225.193573 36.813077 \r\nL 225.221199 36.695874 \r\nL 225.588231 36.607972 \r\nL 225.671109 36.49077 \r\nL 225.90001 36.402868 \r\nL 225.939476 36.344267 \r\nL 226.507783 36.256365 \r\nL 226.535409 36.197763 \r\nL 226.894548 36.109862 \r\nL 226.993212 36.05126 \r\nL 227.320778 35.963358 \r\nL 227.431282 35.846156 \r\nL 227.715436 35.758254 \r\nL 227.758848 35.699653 \r\nL 228.165346 35.611751 \r\nL 228.165346 35.58245 \r\nL 228.907302 35.494548 \r\nL 228.950715 35.435947 \r\nL 229.645312 35.348045 \r\nL 229.75187 35.260143 \r\nL 230.31623 35.172241 \r\nL 230.407002 35.084339 \r\nL 230.738514 34.996437 \r\nL 230.738514 34.967137 \r\nL 231.117386 34.879235 \r\nL 231.22789 34.791333 \r\nL 231.535723 34.703431 \r\nL 231.567296 34.64483 \r\nL 232.250053 34.556928 \r\nL 232.250053 34.527627 \r\nL 232.597352 34.439725 \r\nL 232.668391 34.322523 \r\nL 233.339309 34.234621 \r\nL 233.378775 34.17602 \r\nL 233.844471 34.088118 \r\nL 233.887883 34.000216 \r\nL 234.400938 33.912314 \r\nL 234.471976 33.824412 \r\nL 234.645626 33.73651 \r\nL 234.645626 33.70721 \r\nL 235.095536 33.619308 \r\nL 235.095536 33.590007 \r\nL 235.711202 33.502105 \r\nL 235.774347 33.414203 \r\nL 236.709686 33.326301 \r\nL 236.733365 33.238399 \r\nL 237.206955 33.150498 \r\nL 237.266153 33.091896 \r\nL 237.589773 33.003994 \r\nL 237.621345 32.945393 \r\nL 238.371195 32.857491 \r\nL 238.438287 32.769589 \r\nL 238.982914 32.681687 \r\nL 239.014487 32.623086 \r\nL 239.346 32.535184 \r\nL 239.420985 32.476583 \r\nL 239.594634 32.417982 \r\nL 239.689352 32.212877 \r\nL 239.930093 32.124975 \r\nL 240.005078 31.949172 \r\nL 240.695729 31.86127 \r\nL 240.770714 31.773368 \r\nL 241.216677 31.685466 \r\nL 241.216677 31.656165 \r\nL 241.946794 31.568263 \r\nL 242.053352 31.451061 \r\nL 242.199375 31.363159 \r\nL 242.199375 31.333858 \r\nL 242.582193 31.245956 \r\nL 242.582193 31.216656 \r\nL 243.470173 31.128754 \r\nL 243.541211 31.070153 \r\nL 243.742487 31.011551 \r\nL 243.742487 30.95295 \r\nL 244.05032 30.865048 \r\nL 244.156877 30.806447 \r\nL 244.46471 30.718545 \r\nL 244.575215 30.572042 \r\nL 244.859368 30.48414 \r\nL 244.950139 30.425539 \r\nL 245.104056 30.337637 \r\nL 245.135629 30.279035 \r\nL 245.577645 30.191134 \r\nL 245.67631 30.132532 \r\nL 246.031502 30.04463 \r\nL 246.031502 30.01533 \r\nL 246.647168 29.927428 \r\nL 246.726099 29.868827 \r\nL 246.994467 29.780925 \r\nL 247.101024 29.663722 \r\nL 247.357552 29.57582 \r\nL 247.412804 29.517219 \r\nL 247.57856 29.429317 \r\nL 247.637759 29.341415 \r\nL 248.150814 29.253513 \r\nL 248.241585 29.165611 \r\nL 248.825678 29.07771 \r\nL 248.853305 29.019108 \r\nL 249.240069 28.931206 \r\nL 249.240069 28.901906 \r\nL 249.749178 28.814004 \r\nL 249.749178 28.784703 \r\nL 250.534546 28.696801 \r\nL 250.593745 28.6382 \r\nL 251.071281 28.550298 \r\nL 251.071281 28.520997 \r\nL 251.367274 28.433096 \r\nL 251.367274 28.374494 \r\nL 251.951368 28.286592 \r\nL 252.057925 28.198691 \r\nL 252.389438 28.110789 \r\nL 252.389438 28.081488 \r\nL 252.843294 27.993586 \r\nL 252.843294 27.964285 \r\nL 253.297151 27.876384 \r\nL 253.407655 27.788482 \r\nL 254.039107 27.70058 \r\nL 254.039107 27.671279 \r\nL 254.548216 27.583377 \r\nL 254.548216 27.554077 \r\nL 255.242813 27.466175 \r\nL 255.325691 27.407573 \r\nL 255.724296 27.319672 \r\nL 255.826907 27.26107 \r\nL 256.332069 27.173168 \r\nL 256.332069 27.143868 \r\nL 257.070079 27.055966 \r\nL 257.164796 26.938763 \r\nL 257.717317 26.850861 \r\nL 257.717317 26.821561 \r\nL 258.47506 26.733659 \r\nL 258.506633 26.675058 \r\nL 258.944703 26.587156 \r\nL 259.039421 26.469953 \r\nL 259.820843 26.382051 \r\nL 259.911614 26.32345 \r\nL 261.024549 26.235548 \r\nL 261.024549 26.206247 \r\nL 261.794132 26.118346 \r\nL 261.837544 26.059744 \r\nL 262.717631 25.971842 \r\nL 262.717631 25.942542 \r\nL 262.993891 25.85464 \r\nL 263.076769 25.796039 \r\nL 263.617451 25.708137 \r\nL 263.680596 25.649535 \r\nL 264.075254 25.561634 \r\nL 264.075254 25.532333 \r\nL 264.686973 25.444431 \r\nL 264.694866 25.38583 \r\nL 265.709137 25.297928 \r\nL 265.709137 25.268627 \r\nL 266.541864 25.180725 \r\nL 266.541864 25.151425 \r\nL 267.118065 25.063523 \r\nL 267.185156 24.975621 \r\nL 268.006045 24.887719 \r\nL 268.006045 24.858418 \r\nL 268.309931 24.770516 \r\nL 268.309931 24.741216 \r\nL 268.842719 24.653314 \r\nL 268.846666 24.594713 \r\nL 269.426812 24.506811 \r\nL 269.426812 24.47751 \r\nL 269.900402 24.389608 \r\nL 269.900402 24.360308 \r\nL 270.472655 24.272406 \r\nL 270.472655 24.243105 \r\nL 271.013337 24.155203 \r\nL 271.048856 24.067301 \r\nL 271.534285 23.979399 \r\nL 271.534285 23.950099 \r\nL 272.157844 23.862197 \r\nL 272.157844 23.832896 \r\nL 272.907694 23.744994 \r\nL 272.915587 23.686393 \r\nL 273.669383 23.598491 \r\nL 273.775941 23.481289 \r\nL 274.162705 23.393387 \r\nL 274.162705 23.364086 \r\nL 275.121724 23.276184 \r\nL 275.145403 23.217583 \r\nL 275.926825 23.129681 \r\nL 275.926825 23.10038 \r\nL 276.321483 23.012478 \r\nL 276.341216 22.953877 \r\nL 277.06344 22.865975 \r\nL 277.114745 22.807374 \r\nL 277.537029 22.719472 \r\nL 277.647533 22.63157 \r\nL 278.204001 22.543668 \r\nL 278.310558 22.485067 \r\nL 278.444742 22.397165 \r\nL 278.444742 22.367864 \r\nL 279.013049 22.279963 \r\nL 279.080141 22.221361 \r\nL 279.514264 22.133459 \r\nL 279.514264 22.104159 \r\nL 280.098358 22.016257 \r\nL 280.181236 21.928355 \r\nL 280.875833 21.840453 \r\nL 280.911353 21.781852 \r\nL 281.708561 21.69395 \r\nL 281.799333 21.606048 \r\nL 282.501823 21.518146 \r\nL 282.564969 21.459545 \r\nL 282.860962 21.371643 \r\nL 282.860962 21.342342 \r\nL 283.283246 21.25444 \r\nL 283.283246 21.22514 \r\nL 284.585616 21.137238 \r\nL 284.585616 21.107937 \r\nL 285.453863 21.020035 \r\nL 285.453863 20.990735 \r\nL 285.947185 20.902833 \r\nL 286.010331 20.814931 \r\nL 286.7957 20.727029 \r\nL 286.874631 20.668428 \r\nL 287.22193 20.580526 \r\nL 287.22193 20.551225 \r\nL 287.723145 20.463323 \r\nL 287.723145 20.434023 \r\nL 288.508514 20.346121 \r\nL 288.516407 20.258219 \r\nL 288.978157 20.170317 \r\nL 288.978157 20.141016 \r\nL 290.114771 20.053114 \r\nL 290.114771 20.023814 \r\nL 290.915926 19.935912 \r\nL 291.010644 19.789409 \r\nL 291.227706 19.701507 \r\nL 291.322424 19.642906 \r\nL 291.894678 19.555004 \r\nL 291.894678 19.525703 \r\nL 292.861589 19.437801 \r\nL 292.861589 19.408501 \r\nL 293.761409 19.320599 \r\nL 293.761409 19.291298 \r\nL 294.554671 19.203396 \r\nL 294.59019 19.115494 \r\nL 295.292681 19.027592 \r\nL 295.292681 18.998292 \r\nL 295.845202 18.91039 \r\nL 295.880721 18.851788 \r\nL 296.745021 18.763887 \r\nL 296.745021 18.734586 \r\nL 297.305435 18.646684 \r\nL 297.305435 18.617383 \r\nL 297.976353 18.529482 \r\nL 297.976353 18.500181 \r\nL 298.99457 18.412279 \r\nL 299.01825 18.353678 \r\nL 300.071986 18.265776 \r\nL 300.099612 18.207175 \r\nL 301.433555 18.119273 \r\nL 301.480914 18.031371 \r\nL 302.333375 17.943469 \r\nL 302.333375 17.914168 \r\nL 303.813341 17.826266 \r\nL 303.8607 17.738364 \r\nL 305.316987 17.650463 \r\nL 305.368293 17.591861 \r\nL 305.944493 17.503959 \r\nL 305.987905 17.445358 \r\nL 306.840366 17.357456 \r\nL 306.840366 17.298855 \r\nL 307.811224 17.210953 \r\nL 307.811224 17.181652 \r\nL 308.387424 17.09375 \r\nL 308.466356 17.035149 \r\nL 309.567451 16.947247 \r\nL 309.567451 16.917947 \r\nL 310.19101 16.830045 \r\nL 310.19101 16.800744 \r\nL 310.684333 16.712842 \r\nL 310.711959 16.654241 \r\nL 311.363144 16.566339 \r\nL 311.363144 16.537038 \r\nL 312.732606 16.449137 \r\nL 312.732606 16.419836 \r\nL 313.462723 16.331934 \r\nL 313.462723 16.302633 \r\nL 314.800613 16.214731 \r\nL 314.800613 16.185431 \r\nL 316.193755 16.097529 \r\nL 316.292419 16.038928 \r\nL 317.752653 15.951026 \r\nL 317.752653 15.921725 \r\nL 318.502502 15.833823 \r\nL 318.601167 15.716621 \r\nL 319.954843 15.628719 \r\nL 319.978522 15.570118 \r\nL 320.562616 15.482216 \r\nL 320.562616 15.452915 \r\nL 321.801841 15.365013 \r\nL 321.801841 15.335712 \r\nL 322.677981 15.247811 \r\nL 322.677981 15.21851 \r\nL 323.329166 15.130608 \r\nL 323.396258 15.042706 \r\nL 324.106642 14.954804 \r\nL 324.165841 14.896203 \r\nL 325.641861 14.808301 \r\nL 325.716846 14.7497 \r\nL 327.323103 14.661798 \r\nL 327.421767 14.603197 \r\nL 327.847998 14.515295 \r\nL 327.847998 14.485994 \r\nL 329.008291 14.398092 \r\nL 329.008291 14.368792 \r\nL 330.342235 14.28089 \r\nL 330.342235 14.251589 \r\nL 332.311577 14.163687 \r\nL 332.311577 14.134387 \r\nL 333.310061 14.046485 \r\nL 333.310061 14.017184 \r\nL 333.617894 13.929282 \r\nL 333.617894 13.899981 \r\nL 335.034715 13.81208 \r\nL 335.062341 13.753478 \r\nL 336.384445 13.665576 \r\nL 336.384445 13.636276 \r\nL 337.59999 13.548374 \r\nL 337.59999 13.519073 \r\nL 338.657673 13.431171 \r\nL 338.748444 13.37257 \r\nL 339.462775 13.284668 \r\nL 339.462775 13.255368 \r\nL 341.743897 13.167466 \r\nL 341.743897 13.138165 \r\nL 343.701399 13.050263 \r\nL 343.701399 13.020962 \r\nL 344.834067 12.933061 \r\nL 344.834067 12.90376 \r\nL 345.927269 12.815858 \r\nL 346.01804 12.757257 \r\nL 346.550828 12.669355 \r\nL 346.645546 12.581453 \r\nL 347.636137 12.493551 \r\nL 347.651923 12.43495 \r\nL 348.97008 12.347048 \r\nL 348.97008 12.317747 \r\nL 349.99619 12.229845 \r\nL 349.99619 12.200545 \r\nL 354.408463 12.112643 \r\nL 354.408463 12.083342 \r\nL 356.077866 11.99544 \r\nL 356.077866 11.96614 \r\nL 357.818306 11.878238 \r\nL 357.909078 11.819636 \r\nL 360.288864 11.731735 \r\nL 360.288864 11.702434 \r\nL 361.595181 11.614532 \r\nL 361.595181 11.585231 \r\nL 363.236957 11.49733 \r\nL 363.236957 11.468029 \r\nL 364.665618 11.380127 \r\nL 364.665618 11.350826 \r\nL 367.518994 11.262924 \r\nL 367.518994 11.233624 \r\nL 369.440977 11.145722 \r\nL 369.440977 11.116421 \r\nL 374.007167 11.028519 \r\nL 374.007167 10.999219 \r\nL 378.58125 10.999219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p0fe17390f5)\" d=\"M 43.78125 228.439219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 43.78125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 228.439219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 378.58125 228.439219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 10.999219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 50.78125 48.355469 \r\nL 124.178125 48.355469 \r\nQ 126.178125 48.355469 126.178125 46.355469 \r\nL 126.178125 17.999219 \r\nQ 126.178125 15.999219 124.178125 15.999219 \r\nL 50.78125 15.999219 \r\nQ 48.78125 15.999219 48.78125 17.999219 \r\nL 48.78125 46.355469 \r\nQ 48.78125 48.355469 50.78125 48.355469 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 52.78125 24.097656 \r\nL 72.78125 24.097656 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_19\">\r\n     <!-- ROC -->\r\n     <g transform=\"translate(80.78125 27.597656)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 39.40625 66.21875 \r\nQ 28.65625 66.21875 22.328125 58.203125 \r\nQ 16.015625 50.203125 16.015625 36.375 \r\nQ 16.015625 22.609375 22.328125 14.59375 \r\nQ 28.65625 6.59375 39.40625 6.59375 \r\nQ 50.140625 6.59375 56.421875 14.59375 \r\nQ 62.703125 22.609375 62.703125 36.375 \r\nQ 62.703125 50.203125 56.421875 58.203125 \r\nQ 50.140625 66.21875 39.40625 66.21875 \r\nz\r\nM 39.40625 74.21875 \r\nQ 54.734375 74.21875 63.90625 63.9375 \r\nQ 73.09375 53.65625 73.09375 36.375 \r\nQ 73.09375 19.140625 63.90625 8.859375 \r\nQ 54.734375 -1.421875 39.40625 -1.421875 \r\nQ 24.03125 -1.421875 14.8125 8.828125 \r\nQ 5.609375 19.09375 5.609375 36.375 \r\nQ 5.609375 53.65625 14.8125 63.9375 \r\nQ 24.03125 74.21875 39.40625 74.21875 \r\nz\r\n\" id=\"DejaVuSans-79\"/>\r\n       <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-79\"/>\r\n      <use x=\"148.193359\" xlink:href=\"#DejaVuSans-67\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_21\">\r\n     <path d=\"M 52.78125 38.775781 \r\nL 72.78125 38.775781 \r\n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_22\"/>\r\n    <g id=\"text_20\">\r\n     <!-- Random -->\r\n     <g transform=\"translate(80.78125 42.275781)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"67.232422\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"128.511719\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"191.890625\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"255.367188\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"316.548828\" xlink:href=\"#DejaVuSans-109\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p0fe17390f5\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm_JTvSpCQbm"
      },
      "source": [
        "##### cross validation 으로 hyper parameter 재 tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1YVqvdaCQbm"
      },
      "source": [
        "bayesian_params = {\n",
        "    'max_depth': (6, 16), \n",
        "    'num_leaves': (24, 64), \n",
        "    'min_data_in_leaf': (10, 200), # min_child_samples\n",
        "    'min_child_weight':(1, 50),\n",
        "    'bagging_fraction':(0.5, 1.0), # subsample\n",
        "    'feature_fraction': (0.5, 1.0), # colsample_bytree\n",
        "    'max_bin':(10, 500),\n",
        "    'lambda_l2':(0.001, 10), # reg_lambda\n",
        "    'lambda_l1': (0.01, 50) # reg_alpha\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4ZWKiVoCQbm"
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "train_data = lgb.Dataset(data=ftr_app, label=target_app, free_raw_data=False)\n",
        "def lgb_roc_eval_cv(max_depth, num_leaves, min_data_in_leaf, min_child_weight, bagging_fraction, \n",
        "                 feature_fraction,  max_bin, lambda_l2, lambda_l1):   \n",
        "    params = {\n",
        "        \"num_iterations\":500, \"learning_rate\":0.02,\n",
        "        'early_stopping_rounds':100, 'metric':'auc',\n",
        "        'max_depth': int(round(max_depth)), #  호출 시 실수형 값이 들어오므로 실수형 하이퍼 파라미터는 정수형으로 변경 \n",
        "        'num_leaves': int(round(num_leaves)), \n",
        "        'min_data_in_leaf': int(round(min_data_in_leaf)),\n",
        "        'min_child_weight': int(round(min_child_weight)),\n",
        "        'bagging_fraction': max(min(bagging_fraction, 1), 0), \n",
        "        'feature_fraction': max(min(feature_fraction, 1), 0),\n",
        "        'max_bin':  max(int(round(max_bin)),10),\n",
        "        'lambda_l2': max(lambda_l2,0),\n",
        "        'lambda_l1': max(lambda_l1, 0)\n",
        "    }\n",
        "    # 파이썬 lightgbm의 cv 메소드를 사용. \n",
        "    \n",
        "    cv_result = lgb.cv(params, train_data, nfold=3, seed=0,  verbose_eval =100,  early_stopping_rounds=50, metrics=['auc'])\n",
        "    return max(cv_result['auc-mean'])   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn_axH3LCQbm",
        "outputId": "50cc8dbe-e856-4041-d29b-5f744c7d56b2"
      },
      "source": [
        "# BayesianOptimization객체를 수행할 함수와 search할 parameter 범위를 설정하여 생성. \n",
        "lgbBO = BayesianOptimization(lgb_roc_eval_cv,bayesian_params , random_state=0)\n",
        "# 함수 반환값이 최대가 되는 입력값 유추를 위한 iteration 수행. \n",
        "lgbBO.maximize(init_points=5, n_iter=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 |  max_bin  | max_depth | min_ch... | min_da... | num_le... |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163181 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 20217\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229890 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 20217\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170975 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 20217\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.752413 + 0.00209345\n",
            "[200]\tcv_agg's auc: 0.761932 + 0.00172974\n",
            "[300]\tcv_agg's auc: 0.766578 + 0.00170894\n",
            "[400]\tcv_agg's auc: 0.768997 + 0.00156106\n",
            "[500]\tcv_agg's auc: 0.770413 + 0.0014655\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7704  \u001b[0m | \u001b[0m 0.7744  \u001b[0m | \u001b[0m 0.8576  \u001b[0m | \u001b[0m 30.14   \u001b[0m | \u001b[0m 5.449   \u001b[0m | \u001b[0m 217.6   \u001b[0m | \u001b[0m 12.46   \u001b[0m | \u001b[0m 22.44   \u001b[0m | \u001b[0m 179.4   \u001b[0m | \u001b[0m 62.55   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150128 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 39937\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269771 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 39937\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220449 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 39937\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.752486 + 0.00223274\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\tcv_agg's auc: 0.762232 + 0.00215819\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[300]\tcv_agg's auc: 0.767065 + 0.00202812\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[400]\tcv_agg's auc: 0.76932 + 0.0018439\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tcv_agg's auc: 0.770732 + 0.00167646\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.7707  \u001b[0m | \u001b[95m 0.6917  \u001b[0m | \u001b[95m 0.8959  \u001b[0m | \u001b[95m 26.45   \u001b[0m | \u001b[95m 5.681   \u001b[0m | \u001b[95m 463.5   \u001b[0m | \u001b[95m 6.71    \u001b[0m | \u001b[95m 5.269   \u001b[0m | \u001b[95m 13.84   \u001b[0m | \u001b[95m 57.3    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031859 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 21674\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145476 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 21674\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217018 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 21674\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.747033 + 0.00241826\n",
            "[200]\tcv_agg's auc: 0.757622 + 0.00238554\n",
            "[300]\tcv_agg's auc: 0.763026 + 0.00240661\n",
            "[400]\tcv_agg's auc: 0.765727 + 0.00246971\n",
            "[500]\tcv_agg's auc: 0.767406 + 0.00235341\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7674  \u001b[0m | \u001b[0m 0.8891  \u001b[0m | \u001b[0m 0.935   \u001b[0m | \u001b[0m 48.93   \u001b[0m | \u001b[0m 7.992   \u001b[0m | \u001b[0m 236.1   \u001b[0m | \u001b[0m 13.81   \u001b[0m | \u001b[0m 6.795   \u001b[0m | \u001b[0m 131.6   \u001b[0m | \u001b[0m 29.73   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216285 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 34163\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147815 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 34163\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213175 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 34163\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.751849 + 0.00212158\n",
            "[200]\tcv_agg's auc: 0.762101 + 0.00196294\n",
            "[300]\tcv_agg's auc: 0.766926 + 0.00203444\n",
            "[400]\tcv_agg's auc: 0.769328 + 0.00189603\n",
            "[500]\tcv_agg's auc: 0.770617 + 0.00193616\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7706  \u001b[0m | \u001b[0m 0.9723  \u001b[0m | \u001b[0m 0.7609  \u001b[0m | \u001b[0m 20.74   \u001b[0m | \u001b[0m 2.646   \u001b[0m | \u001b[0m 389.4   \u001b[0m | \u001b[0m 10.56   \u001b[0m | \u001b[0m 28.85   \u001b[0m | \u001b[0m 13.57   \u001b[0m | \u001b[0m 48.71   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223108 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 17490\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163106 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 17490\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019565 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 17490\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.750094 + 0.00231875\n",
            "[200]\tcv_agg's auc: 0.760001 + 0.00216013\n",
            "[300]\tcv_agg's auc: 0.765002 + 0.00214597\n",
            "[400]\tcv_agg's auc: 0.767619 + 0.00204746\n",
            "[500]\tcv_agg's auc: 0.769264 + 0.00198502\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7693  \u001b[0m | \u001b[0m 0.806   \u001b[0m | \u001b[0m 0.8085  \u001b[0m | \u001b[0m 47.19   \u001b[0m | \u001b[0m 6.819   \u001b[0m | \u001b[0m 186.2   \u001b[0m | \u001b[0m 10.37   \u001b[0m | \u001b[0m 35.18   \u001b[0m | \u001b[0m 21.44   \u001b[0m | \u001b[0m 50.67   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.231705 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 20299\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 20299\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017201 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 20299\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.75385 + 0.00200506\n",
            "[200]\tcv_agg's auc: 0.762312 + 0.00169187\n",
            "[300]\tcv_agg's auc: 0.766827 + 0.00187033\n",
            "[400]\tcv_agg's auc: 0.769265 + 0.00182896\n",
            "[500]\tcv_agg's auc: 0.770744 + 0.00172514\n",
            "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.7707  \u001b[0m | \u001b[95m 0.8999  \u001b[0m | \u001b[95m 0.5994  \u001b[0m | \u001b[95m 31.07   \u001b[0m | \u001b[95m 4.228   \u001b[0m | \u001b[95m 219.1   \u001b[0m | \u001b[95m 11.43   \u001b[0m | \u001b[95m 15.67   \u001b[0m | \u001b[95m 183.4   \u001b[0m | \u001b[95m 63.65   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292215 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 37203\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 169\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169085 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 37203\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 169\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222382 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 37203\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 169\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.751501 + 0.00221409\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\tcv_agg's auc: 0.761105 + 0.0020891\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[300]\tcv_agg's auc: 0.766057 + 0.00201994\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[400]\tcv_agg's auc: 0.768684 + 0.00192914\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tcv_agg's auc: 0.77023 + 0.00185669\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7702  \u001b[0m | \u001b[0m 0.7404  \u001b[0m | \u001b[0m 0.7668  \u001b[0m | \u001b[0m 40.5    \u001b[0m | \u001b[0m 6.329   \u001b[0m | \u001b[0m 428.5   \u001b[0m | \u001b[0m 7.581   \u001b[0m | \u001b[0m 18.72   \u001b[0m | \u001b[0m 36.92   \u001b[0m | \u001b[0m 49.96   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166676 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 17390\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228066 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 17390\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020179 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 17390\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.753772 + 0.00271324\n",
            "[200]\tcv_agg's auc: 0.764642 + 0.00241158\n",
            "[300]\tcv_agg's auc: 0.768816 + 0.00221752\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[400]\tcv_agg's auc: 0.770854 + 0.00201828\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tcv_agg's auc: 0.77191 + 0.0020515\n",
            "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.7719  \u001b[0m | \u001b[95m 0.8143  \u001b[0m | \u001b[95m 0.8885  \u001b[0m | \u001b[95m 3.55    \u001b[0m | \u001b[95m 4.612   \u001b[0m | \u001b[95m 184.9   \u001b[0m | \u001b[95m 9.342   \u001b[0m | \u001b[95m 4.441   \u001b[0m | \u001b[95m 198.5   \u001b[0m | \u001b[95m 57.95   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021471 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12896\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024637 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12896\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021677 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12896\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.750032 + 0.00214251\n",
            "[200]\tcv_agg's auc: 0.760098 + 0.00198386\n",
            "[300]\tcv_agg's auc: 0.765089 + 0.00206596\n",
            "[400]\tcv_agg's auc: 0.76748 + 0.00205654\n",
            "[500]\tcv_agg's auc: 0.769124 + 0.00185619\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7691  \u001b[0m | \u001b[0m 0.8068  \u001b[0m | \u001b[0m 0.9537  \u001b[0m | \u001b[0m 32.45   \u001b[0m | \u001b[0m 0.03474 \u001b[0m | \u001b[0m 133.3   \u001b[0m | \u001b[0m 14.63   \u001b[0m | \u001b[0m 4.227   \u001b[0m | \u001b[0m 196.7   \u001b[0m | \u001b[0m 46.35   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017705 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 19499\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019306 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 19499\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258748 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 19499\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.752892 + 0.00239651\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\tcv_agg's auc: 0.762432 + 0.00238515\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[300]\tcv_agg's auc: 0.766805 + 0.0024799\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[400]\tcv_agg's auc: 0.769205 + 0.00226882\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tcv_agg's auc: 0.770527 + 0.00217943\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7705  \u001b[0m | \u001b[0m 0.5315  \u001b[0m | \u001b[0m 0.6123  \u001b[0m | \u001b[0m 0.7358  \u001b[0m | \u001b[0m 7.801   \u001b[0m | \u001b[0m 209.7   \u001b[0m | \u001b[0m 6.136   \u001b[0m | \u001b[0m 16.85   \u001b[0m | \u001b[0m 187.7   \u001b[0m | \u001b[0m 39.45   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021861 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 19007\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025572 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 19007\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023354 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 19007\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.751214 + 0.002638\n",
            "[200]\tcv_agg's auc: 0.762887 + 0.00256294\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[300]\tcv_agg's auc: 0.76752 + 0.00242286\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[400]\tcv_agg's auc: 0.769755 + 0.00229775\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tcv_agg's auc: 0.771039 + 0.00218057\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.771   \u001b[0m | \u001b[0m 0.5941  \u001b[0m | \u001b[0m 0.8808  \u001b[0m | \u001b[0m 3.598   \u001b[0m | \u001b[0m 6.108   \u001b[0m | \u001b[0m 203.7   \u001b[0m | \u001b[0m 7.426   \u001b[0m | \u001b[0m 11.11   \u001b[0m | \u001b[0m 183.6   \u001b[0m | \u001b[0m 38.16   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264825 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16402\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224450 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16402\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222576 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16402\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 165\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.753336 + 0.00242392\n",
            "[200]\tcv_agg's auc: 0.764042 + 0.0019804\n",
            "[300]\tcv_agg's auc: 0.768487 + 0.00181753\n",
            "[400]\tcv_agg's auc: 0.770666 + 0.00166117\n",
            "[500]\tcv_agg's auc: 0.771877 + 0.00164313\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7719  \u001b[0m | \u001b[0m 0.6576  \u001b[0m | \u001b[0m 0.6812  \u001b[0m | \u001b[0m 4.784   \u001b[0m | \u001b[0m 0.958   \u001b[0m | \u001b[0m 173.0   \u001b[0m | \u001b[0m 9.47    \u001b[0m | \u001b[0m 8.141   \u001b[0m | \u001b[0m 167.8   \u001b[0m | \u001b[0m 49.13   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220884 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 19579\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166716 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 19579\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153352 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 19579\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.752887 + 0.0024333\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\tcv_agg's auc: 0.762859 + 0.00250737\n",
            "[300]\tcv_agg's auc: 0.767329 + 0.00227619\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[400]\tcv_agg's auc: 0.7697 + 0.00213193\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tcv_agg's auc: 0.77114 + 0.00205365\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7711  \u001b[0m | \u001b[0m 0.8786  \u001b[0m | \u001b[0m 0.6958  \u001b[0m | \u001b[0m 7.338   \u001b[0m | \u001b[0m 2.273   \u001b[0m | \u001b[0m 211.3   \u001b[0m | \u001b[0m 7.188   \u001b[0m | \u001b[0m 17.38   \u001b[0m | \u001b[0m 192.5   \u001b[0m | \u001b[0m 39.91   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170049 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16894\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144410 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16894\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235024 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16894\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.749206 + 0.00268119\n",
            "[200]\tcv_agg's auc: 0.760618 + 0.00261398\n",
            "[300]\tcv_agg's auc: 0.765772 + 0.00265961\n",
            "[400]\tcv_agg's auc: 0.76807 + 0.00249413\n",
            "[500]\tcv_agg's auc: 0.769569 + 0.0023889\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7696  \u001b[0m | \u001b[0m 0.6999  \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 16.83   \u001b[0m | \u001b[0m 4.11    \u001b[0m | \u001b[0m 179.0   \u001b[0m | \u001b[0m 12.26   \u001b[0m | \u001b[0m 10.37   \u001b[0m | \u001b[0m 197.7   \u001b[0m | \u001b[0m 31.16   \u001b[0m |\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019978 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16816\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022800 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16816\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221621 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16816\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 165\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.755504 + 0.00217257\n",
            "[200]\tcv_agg's auc: 0.764391 + 0.00204597\n",
            "[300]\tcv_agg's auc: 0.768847 + 0.00201129\n",
            "[400]\tcv_agg's auc: 0.771043 + 0.00189954\n",
            "[500]\tcv_agg's auc: 0.772446 + 0.00191014\n",
            "| \u001b[95m 15      \u001b[0m | \u001b[95m 0.7724  \u001b[0m | \u001b[95m 0.9299  \u001b[0m | \u001b[95m 0.5747  \u001b[0m | \u001b[95m 7.543   \u001b[0m | \u001b[95m 3.292   \u001b[0m | \u001b[95m 177.9   \u001b[0m | \u001b[95m 10.62   \u001b[0m | \u001b[95m 1.398   \u001b[0m | \u001b[95m 167.0   \u001b[0m | \u001b[95m 54.19   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161626 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 34319\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036407 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 34319\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275589 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 34319\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.751498 + 0.00213142\n",
            "[200]\tcv_agg's auc: 0.76134 + 0.00190147\n",
            "[300]\tcv_agg's auc: 0.766449 + 0.00197641\n",
            "[400]\tcv_agg's auc: 0.768736 + 0.00198658\n",
            "[500]\tcv_agg's auc: 0.770273 + 0.00196995\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.7703  \u001b[0m | \u001b[0m 0.8845  \u001b[0m | \u001b[0m 0.727   \u001b[0m | \u001b[0m 23.44   \u001b[0m | \u001b[0m 0.6011  \u001b[0m | \u001b[0m 390.9   \u001b[0m | \u001b[0m 13.53   \u001b[0m | \u001b[0m 29.49   \u001b[0m | \u001b[0m 16.74   \u001b[0m | \u001b[0m 42.66   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222346 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 36283\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226372 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 36283\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266766 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 36283\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.750135 + 0.00234214\n",
            "[200]\tcv_agg's auc: 0.760181 + 0.00219461\n",
            "[300]\tcv_agg's auc: 0.765124 + 0.00222087\n",
            "[400]\tcv_agg's auc: 0.767708 + 0.00229372\n",
            "[500]\tcv_agg's auc: 0.769155 + 0.0020001\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.7692  \u001b[0m | \u001b[0m 0.5286  \u001b[0m | \u001b[0m 0.9555  \u001b[0m | \u001b[0m 37.34   \u001b[0m | \u001b[0m 4.565   \u001b[0m | \u001b[0m 415.6   \u001b[0m | \u001b[0m 15.38   \u001b[0m | \u001b[0m 30.31   \u001b[0m | \u001b[0m 77.81   \u001b[0m | \u001b[0m 51.73   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020385 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 17800\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017535 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 17800\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 17800\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.756489 + 0.00222134\n",
            "[200]\tcv_agg's auc: 0.764369 + 0.00182719\n",
            "[300]\tcv_agg's auc: 0.768363 + 0.00176986\n",
            "[400]\tcv_agg's auc: 0.770592 + 0.00173656\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tcv_agg's auc: 0.771856 + 0.00183132\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7719  \u001b[0m | \u001b[0m 0.6943  \u001b[0m | \u001b[0m 0.5035  \u001b[0m | \u001b[0m 14.91   \u001b[0m | \u001b[0m 3.789   \u001b[0m | \u001b[0m 190.4   \u001b[0m | \u001b[0m 9.341   \u001b[0m | \u001b[0m 7.834   \u001b[0m | \u001b[0m 183.6   \u001b[0m | \u001b[0m 61.8    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221313 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 27374\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219466 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 27374\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153893 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 27374\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.751297 + 0.00232016\n",
            "[200]\tcv_agg's auc: 0.761509 + 0.00220102\n",
            "[300]\tcv_agg's auc: 0.76659 + 0.0021112\n",
            "[400]\tcv_agg's auc: 0.769188 + 0.00192386\n",
            "[500]\tcv_agg's auc: 0.77086 + 0.00163407\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.7709  \u001b[0m | \u001b[0m 0.9644  \u001b[0m | \u001b[0m 0.8266  \u001b[0m | \u001b[0m 23.37   \u001b[0m | \u001b[0m 0.6431  \u001b[0m | \u001b[0m 304.5   \u001b[0m | \u001b[0m 10.6    \u001b[0m | \u001b[0m 41.3    \u001b[0m | \u001b[0m 144.9   \u001b[0m | \u001b[0m 44.4    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028651 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16818\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018467 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16818\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017772 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16818\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.755812 + 0.00217874\n",
            "[200]\tcv_agg's auc: 0.764044 + 0.0019529\n",
            "[300]\tcv_agg's auc: 0.768441 + 0.00186193\n",
            "[400]\tcv_agg's auc: 0.770725 + 0.00169801\n",
            "[500]\tcv_agg's auc: 0.77197 + 0.00163797\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.772   \u001b[0m | \u001b[0m 0.959   \u001b[0m | \u001b[0m 0.5257  \u001b[0m | \u001b[0m 10.8    \u001b[0m | \u001b[0m 1.577   \u001b[0m | \u001b[0m 177.5   \u001b[0m | \u001b[0m 15.64   \u001b[0m | \u001b[0m 3.17    \u001b[0m | \u001b[0m 144.9   \u001b[0m | \u001b[0m 56.8    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169408 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 41759\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215377 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 41759\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221710 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 41759\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.751072 + 0.00218472\n",
            "[200]\tcv_agg's auc: 0.761439 + 0.00212142\n",
            "[300]\tcv_agg's auc: 0.766515 + 0.00204473\n",
            "[400]\tcv_agg's auc: 0.769194 + 0.00183068\n",
            "[500]\tcv_agg's auc: 0.77054 + 0.00173042\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.7705  \u001b[0m | \u001b[0m 0.6587  \u001b[0m | \u001b[0m 0.7454  \u001b[0m | \u001b[0m 19.94   \u001b[0m | \u001b[0m 4.78    \u001b[0m | \u001b[0m 487.5   \u001b[0m | \u001b[0m 9.144   \u001b[0m | \u001b[0m 41.32   \u001b[0m | \u001b[0m 192.7   \u001b[0m | \u001b[0m 33.83   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021007 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 5318\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 5318\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020216 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 5318\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.750244 + 0.00221396\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\tcv_agg's auc: 0.759543 + 0.0021569\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[300]\tcv_agg's auc: 0.764436 + 0.00205619\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[400]\tcv_agg's auc: 0.766954 + 0.0020125\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tcv_agg's auc: 0.768461 + 0.00193603\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.7685  \u001b[0m | \u001b[0m 0.9983  \u001b[0m | \u001b[0m 0.749   \u001b[0m | \u001b[0m 48.25   \u001b[0m | \u001b[0m 8.54    \u001b[0m | \u001b[0m 47.5    \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 34.54   \u001b[0m | \u001b[0m 164.5   \u001b[0m | \u001b[0m 49.58   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163681 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 33376\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270704 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 33376\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225304 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 33376\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.748988 + 0.00258915\n",
            "[200]\tcv_agg's auc: 0.758788 + 0.00222831\n",
            "[300]\tcv_agg's auc: 0.764116 + 0.00232042\n",
            "[400]\tcv_agg's auc: 0.76702 + 0.00218378\n",
            "[500]\tcv_agg's auc: 0.768711 + 0.00216329\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.7687  \u001b[0m | \u001b[0m 0.9707  \u001b[0m | \u001b[0m 0.6446  \u001b[0m | \u001b[0m 36.9    \u001b[0m | \u001b[0m 1.905   \u001b[0m | \u001b[0m 378.9   \u001b[0m | \u001b[0m 9.577   \u001b[0m | \u001b[0m 47.94   \u001b[0m | \u001b[0m 108.4   \u001b[0m | \u001b[0m 25.81   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261242 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 17478\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221486 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 17478\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022389 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 17478\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.754393 + 0.00249219\n",
            "[200]\tcv_agg's auc: 0.764248 + 0.00225796\n",
            "[300]\tcv_agg's auc: 0.768461 + 0.00200145\n",
            "[400]\tcv_agg's auc: 0.77067 + 0.00182478\n",
            "[500]\tcv_agg's auc: 0.771811 + 0.00171913\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.7718  \u001b[0m | \u001b[0m 0.5754  \u001b[0m | \u001b[0m 0.7482  \u001b[0m | \u001b[0m 6.943   \u001b[0m | \u001b[0m 9.527   \u001b[0m | \u001b[0m 185.9   \u001b[0m | \u001b[0m 11.16   \u001b[0m | \u001b[0m 2.699   \u001b[0m | \u001b[0m 195.7   \u001b[0m | \u001b[0m 54.58   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264848 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 14077\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217247 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 14077\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223460 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 14077\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.754153 + 0.00222357\n",
            "[200]\tcv_agg's auc: 0.764278 + 0.00232934\n",
            "[300]\tcv_agg's auc: 0.768641 + 0.00214161\n",
            "[400]\tcv_agg's auc: 0.770555 + 0.00209644\n",
            "[500]\tcv_agg's auc: 0.771691 + 0.00197203\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.7717  \u001b[0m | \u001b[0m 0.7917  \u001b[0m | \u001b[0m 0.6605  \u001b[0m | \u001b[0m 3.41    \u001b[0m | \u001b[0m 8.116   \u001b[0m | \u001b[0m 146.5   \u001b[0m | \u001b[0m 10.42   \u001b[0m | \u001b[0m 1.418   \u001b[0m | \u001b[0m 102.0   \u001b[0m | \u001b[0m 56.14   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.267075 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 11051\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221536 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 11051\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169461 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 11051\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.750838 + 0.00256294\n",
            "[200]\tcv_agg's auc: 0.761322 + 0.00237721\n",
            "[300]\tcv_agg's auc: 0.766301 + 0.00235791\n",
            "[400]\tcv_agg's auc: 0.768735 + 0.00221622\n",
            "[500]\tcv_agg's auc: 0.77019 + 0.00218553\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.7702  \u001b[0m | \u001b[0m 0.5176  \u001b[0m | \u001b[0m 0.6145  \u001b[0m | \u001b[0m 4.366   \u001b[0m | \u001b[0m 2.571   \u001b[0m | \u001b[0m 111.4   \u001b[0m | \u001b[0m 6.679   \u001b[0m | \u001b[0m 6.432   \u001b[0m | \u001b[0m 86.05   \u001b[0m | \u001b[0m 29.8    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023574 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 18842\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024873 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 18842\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024455 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 18842\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.752761 + 0.00253328\n",
            "[200]\tcv_agg's auc: 0.764295 + 0.00214627\n",
            "[300]\tcv_agg's auc: 0.768268 + 0.00157287\n",
            "[400]\tcv_agg's auc: 0.769987 + 0.00150729\n",
            "[500]\tcv_agg's auc: 0.770816 + 0.00144921\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.7708  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 201.9   \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 189.9   \u001b[0m | \u001b[0m 64.0    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022653 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 14825\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 14825\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022400 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 14825\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.753842 + 0.00219322\n",
            "[200]\tcv_agg's auc: 0.764597 + 0.00227203\n",
            "[300]\tcv_agg's auc: 0.768912 + 0.00188056\n",
            "[400]\tcv_agg's auc: 0.770975 + 0.00181401\n",
            "[500]\tcv_agg's auc: 0.772 + 0.001822\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.772   \u001b[0m | \u001b[0m 0.6115  \u001b[0m | \u001b[0m 0.9252  \u001b[0m | \u001b[0m 6.227   \u001b[0m | \u001b[0m 9.844   \u001b[0m | \u001b[0m 153.7   \u001b[0m | \u001b[0m 9.391   \u001b[0m | \u001b[0m 4.001   \u001b[0m | \u001b[0m 131.8   \u001b[0m | \u001b[0m 61.4    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018247 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 15329\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018926 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 15329\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021225 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 15329\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.755952 + 0.00217505\n",
            "[200]\tcv_agg's auc: 0.764695 + 0.00210075\n",
            "[300]\tcv_agg's auc: 0.769001 + 0.00207877\n",
            "[400]\tcv_agg's auc: 0.770975 + 0.00221316\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tcv_agg's auc: 0.772213 + 0.00214666\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7722  \u001b[0m | \u001b[0m 0.6475  \u001b[0m | \u001b[0m 0.5026  \u001b[0m | \u001b[0m 0.8679  \u001b[0m | \u001b[0m 2.108   \u001b[0m | \u001b[0m 159.8   \u001b[0m | \u001b[0m 9.651   \u001b[0m | \u001b[0m 45.07   \u001b[0m | \u001b[0m 121.8   \u001b[0m | \u001b[0m 60.14   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "C:\\Users\\channee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 15248\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020154 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 15248\n",
            "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016801 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 15248\n",
            "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[LightGBM] [Info] Start training from score 0.080729\n",
            "[100]\tcv_agg's auc: 0.755049 + 0.00200925\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\tcv_agg's auc: 0.762787 + 0.00190172\n",
            "[300]\tcv_agg's auc: 0.767045 + 0.00186358\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[400]\tcv_agg's auc: 0.769573 + 0.00165805\n",
            "[500]\tcv_agg's auc: 0.770932 + 0.00161966\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.7709  \u001b[0m | \u001b[0m 0.9025  \u001b[0m | \u001b[0m 0.5039  \u001b[0m | \u001b[0m 26.8    \u001b[0m | \u001b[0m 6.035   \u001b[0m | \u001b[0m 159.5   \u001b[0m | \u001b[0m 9.297   \u001b[0m | \u001b[0m 36.25   \u001b[0m | \u001b[0m 103.7   \u001b[0m | \u001b[0m 61.16   \u001b[0m |\n",
            "=====================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApbbkFd9CQbm",
        "outputId": "449902ea-6f76-47bd-9038-fe46b74da286"
      },
      "source": [
        "lgbBO.res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'target': 0.7704130128940919,\n",
              "  'params': {'bagging_fraction': 0.7744067519636624,\n",
              "   'feature_fraction': 0.8575946831862098,\n",
              "   'lambda_l1': 30.142141169821482,\n",
              "   'lambda_l2': 5.449286946785972,\n",
              "   'max_bin': 217.5908516760633,\n",
              "   'max_depth': 12.458941130666561,\n",
              "   'min_child_weight': 22.441773351871934,\n",
              "   'min_data_in_leaf': 179.43687014859515,\n",
              "   'num_leaves': 62.54651042004117}},\n",
              " {'target': 0.7707320615580647,\n",
              "  'params': {'bagging_fraction': 0.6917207594128889,\n",
              "   'feature_fraction': 0.8958625190413323,\n",
              "   'lambda_l1': 26.449457038447697,\n",
              "   'lambda_l2': 5.680877566378229,\n",
              "   'max_bin': 463.5423527634039,\n",
              "   'max_depth': 6.710360581978869,\n",
              "   'min_child_weight': 5.269335685375495,\n",
              "   'min_data_in_leaf': 13.841495513661886,\n",
              "   'num_leaves': 57.30479382191752}},\n",
              " {'target': 0.7674060125927115,\n",
              "  'params': {'bagging_fraction': 0.8890783754749252,\n",
              "   'feature_fraction': 0.9350060741234096,\n",
              "   'lambda_l1': 48.93113092821587,\n",
              "   'lambda_l2': 7.99178648360302,\n",
              "   'max_bin': 236.1248875039366,\n",
              "   'max_depth': 13.805291762864554,\n",
              "   'min_child_weight': 6.795446867577728,\n",
              "   'min_data_in_leaf': 131.58499405222955,\n",
              "   'num_leaves': 29.734131496361854}},\n",
              " {'target': 0.7706174479634317,\n",
              "  'params': {'bagging_fraction': 0.972334458524792,\n",
              "   'feature_fraction': 0.7609241608750359,\n",
              "   'lambda_l1': 20.738950380126276,\n",
              "   'lambda_l2': 2.646291565434165,\n",
              "   'max_bin': 389.3745078227662,\n",
              "   'max_depth': 10.561503322165485,\n",
              "   'min_child_weight': 28.853263494563777,\n",
              "   'min_data_in_leaf': 13.570062082907477,\n",
              "   'num_leaves': 48.705419883035084}},\n",
              " {'target': 0.7692640265604144,\n",
              "  'params': {'bagging_fraction': 0.8060478613612108,\n",
              "   'feature_fraction': 0.8084669984373785,\n",
              "   'lambda_l1': 47.18796644494606,\n",
              "   'lambda_l2': 6.818521170735731,\n",
              "   'max_bin': 186.15887128115514,\n",
              "   'max_depth': 10.370319537993414,\n",
              "   'min_child_weight': 35.183928600435976,\n",
              "   'min_data_in_leaf': 21.44283960956127,\n",
              "   'num_leaves': 50.670668617826706}},\n",
              " {'target': 0.7707441174483378,\n",
              "  'params': {'bagging_fraction': 0.8999235532397036,\n",
              "   'feature_fraction': 0.5994369290340391,\n",
              "   'lambda_l1': 31.074749286460722,\n",
              "   'lambda_l2': 4.228074498468137,\n",
              "   'max_bin': 219.10229236574725,\n",
              "   'max_depth': 11.427010587833749,\n",
              "   'min_child_weight': 15.667080789571736,\n",
              "   'min_data_in_leaf': 183.41711356263576,\n",
              "   'num_leaves': 63.64711067026588}},\n",
              " {'target': 0.770229710649176,\n",
              "  'params': {'bagging_fraction': 0.7404321772522139,\n",
              "   'feature_fraction': 0.7668315626022122,\n",
              "   'lambda_l1': 40.496397840921915,\n",
              "   'lambda_l2': 6.329038251811815,\n",
              "   'max_bin': 428.47974892655657,\n",
              "   'max_depth': 7.581108392237857,\n",
              "   'min_child_weight': 18.721900794608175,\n",
              "   'min_data_in_leaf': 36.91555073964587,\n",
              "   'num_leaves': 49.96227208627961}},\n",
              " {'target': 0.7719096310120473,\n",
              "  'params': {'bagging_fraction': 0.8143033572205088,\n",
              "   'feature_fraction': 0.8884974616228736,\n",
              "   'lambda_l1': 3.5504085208024487,\n",
              "   'lambda_l2': 4.611830458102663,\n",
              "   'max_bin': 184.9398381966357,\n",
              "   'max_depth': 9.34172001478695,\n",
              "   'min_child_weight': 4.440733035446478,\n",
              "   'min_data_in_leaf': 198.5155850624426,\n",
              "   'num_leaves': 57.94763994734239}},\n",
              " {'target': 0.7691251381694691,\n",
              "  'params': {'bagging_fraction': 0.8067814387656114,\n",
              "   'feature_fraction': 0.9536571212037808,\n",
              "   'lambda_l1': 32.4521675349507,\n",
              "   'lambda_l2': 0.034735067421668264,\n",
              "   'max_bin': 133.30349225834965,\n",
              "   'max_depth': 14.62576290350622,\n",
              "   'min_child_weight': 4.227256072865901,\n",
              "   'min_data_in_leaf': 196.70462703040053,\n",
              "   'num_leaves': 46.351397650073146}},\n",
              " {'target': 0.7705267701934897,\n",
              "  'params': {'bagging_fraction': 0.5315401804588187,\n",
              "   'feature_fraction': 0.612303641148811,\n",
              "   'lambda_l1': 0.7358255076011369,\n",
              "   'lambda_l2': 7.800929367645439,\n",
              "   'max_bin': 209.6908259343996,\n",
              "   'max_depth': 6.135772933697576,\n",
              "   'min_child_weight': 16.849896162166722,\n",
              "   'min_data_in_leaf': 187.67343516011584,\n",
              "   'num_leaves': 39.44552475790631}},\n",
              " {'target': 0.7710389366063817,\n",
              "  'params': {'bagging_fraction': 0.5940850954373089,\n",
              "   'feature_fraction': 0.8807926141911452,\n",
              "   'lambda_l1': 3.5979735041387433,\n",
              "   'lambda_l2': 6.108496637444428,\n",
              "   'max_bin': 203.72096015715022,\n",
              "   'max_depth': 7.425928691480058,\n",
              "   'min_child_weight': 11.111565381111847,\n",
              "   'min_data_in_leaf': 183.58886753504038,\n",
              "   'num_leaves': 38.16261823863786}},\n",
              " {'target': 0.7718773672753726,\n",
              "  'params': {'bagging_fraction': 0.657614492893178,\n",
              "   'feature_fraction': 0.6811813427672369,\n",
              "   'lambda_l1': 4.783749362922549,\n",
              "   'lambda_l2': 0.957997706400652,\n",
              "   'max_bin': 173.03662669577741,\n",
              "   'max_depth': 9.470274418940324,\n",
              "   'min_child_weight': 8.141437429408178,\n",
              "   'min_data_in_leaf': 167.75970713753094,\n",
              "   'num_leaves': 49.13015644124545}},\n",
              " {'target': 0.7711454074315656,\n",
              "  'params': {'bagging_fraction': 0.8786355335852942,\n",
              "   'feature_fraction': 0.6958187117248162,\n",
              "   'lambda_l1': 7.338172390732913,\n",
              "   'lambda_l2': 2.272677319027393,\n",
              "   'max_bin': 211.27555567684178,\n",
              "   'max_depth': 7.187955460578591,\n",
              "   'min_child_weight': 17.382076324484828,\n",
              "   'min_data_in_leaf': 192.54729357889843,\n",
              "   'num_leaves': 39.9125263869441}},\n",
              " {'target': 0.7695691149218623,\n",
              "  'params': {'bagging_fraction': 0.6998973669239097,\n",
              "   'feature_fraction': 0.9419379137870297,\n",
              "   'lambda_l1': 16.828686125736517,\n",
              "   'lambda_l2': 4.109532131663239,\n",
              "   'max_bin': 178.95633418021208,\n",
              "   'max_depth': 12.259221410586013,\n",
              "   'min_child_weight': 10.36621015246892,\n",
              "   'min_data_in_leaf': 197.6663134909162,\n",
              "   'num_leaves': 31.1594353080567}},\n",
              " {'target': 0.7724457191365559,\n",
              "  'params': {'bagging_fraction': 0.9299169469599182,\n",
              "   'feature_fraction': 0.5746809011453164,\n",
              "   'lambda_l1': 7.543441207233135,\n",
              "   'lambda_l2': 3.292195078359306,\n",
              "   'max_bin': 177.94521484058205,\n",
              "   'max_depth': 10.61966768561966,\n",
              "   'min_child_weight': 1.397663097770165,\n",
              "   'min_data_in_leaf': 166.95618212603793,\n",
              "   'num_leaves': 54.18985368611588}},\n",
              " {'target': 0.7702728801817355,\n",
              "  'params': {'bagging_fraction': 0.8844562637903797,\n",
              "   'feature_fraction': 0.7269989688983634,\n",
              "   'lambda_l1': 23.43573646623273,\n",
              "   'lambda_l2': 0.601050164341131,\n",
              "   'max_bin': 390.8777598211938,\n",
              "   'max_depth': 13.531193778598972,\n",
              "   'min_child_weight': 29.486064774397775,\n",
              "   'min_data_in_leaf': 16.738983494464513,\n",
              "   'num_leaves': 42.66429176206504}},\n",
              " {'target': 0.7691546772195966,\n",
              "  'params': {'bagging_fraction': 0.5285839690801153,\n",
              "   'feature_fraction': 0.9554784176155467,\n",
              "   'lambda_l1': 37.33730516793895,\n",
              "   'lambda_l2': 4.564719396185809,\n",
              "   'max_bin': 415.6458449914903,\n",
              "   'max_depth': 15.378391843299484,\n",
              "   'min_child_weight': 30.31421249555068,\n",
              "   'min_data_in_leaf': 77.81456840599282,\n",
              "   'num_leaves': 51.72987878906692}},\n",
              " {'target': 0.7718560657970831,\n",
              "  'params': {'bagging_fraction': 0.6943325913373878,\n",
              "   'feature_fraction': 0.5034545166040366,\n",
              "   'lambda_l1': 14.911628058528853,\n",
              "   'lambda_l2': 3.7888209125712415,\n",
              "   'max_bin': 190.37970682243528,\n",
              "   'max_depth': 9.340892785972903,\n",
              "   'min_child_weight': 7.833820755151955,\n",
              "   'min_data_in_leaf': 183.59628053956956,\n",
              "   'num_leaves': 61.80346524240569}},\n",
              " {'target': 0.7708603583870698,\n",
              "  'params': {'bagging_fraction': 0.9644030062523238,\n",
              "   'feature_fraction': 0.8266253858261036,\n",
              "   'lambda_l1': 23.3655535588359,\n",
              "   'lambda_l2': 0.6430770731307863,\n",
              "   'max_bin': 304.49321845606374,\n",
              "   'max_depth': 10.603479817877357,\n",
              "   'min_child_weight': 41.299919645601506,\n",
              "   'min_data_in_leaf': 144.92832924801024,\n",
              "   'num_leaves': 44.39976983311924}},\n",
              " {'target': 0.7719703571205899,\n",
              "  'params': {'bagging_fraction': 0.958982625395157,\n",
              "   'feature_fraction': 0.5256535956136048,\n",
              "   'lambda_l1': 10.796315561196417,\n",
              "   'lambda_l2': 1.576964383196764,\n",
              "   'max_bin': 177.52508011858072,\n",
              "   'max_depth': 15.636191927411867,\n",
              "   'min_child_weight': 3.1704301617341315,\n",
              "   'min_data_in_leaf': 144.90957631393655,\n",
              "   'num_leaves': 56.801621269683345}},\n",
              " {'target': 0.77054013792762,\n",
              "  'params': {'bagging_fraction': 0.6587283396471605,\n",
              "   'feature_fraction': 0.7454373863647559,\n",
              "   'lambda_l1': 19.94077731253563,\n",
              "   'lambda_l2': 4.780423949040658,\n",
              "   'max_bin': 487.5445679344283,\n",
              "   'max_depth': 9.144439579692584,\n",
              "   'min_child_weight': 41.323096983850455,\n",
              "   'min_data_in_leaf': 192.74672138313088,\n",
              "   'num_leaves': 33.83493453930372}},\n",
              " {'target': 0.7684607098023677,\n",
              "  'params': {'bagging_fraction': 0.9982660890256111,\n",
              "   'feature_fraction': 0.7490195259582262,\n",
              "   'lambda_l1': 48.254390571730376,\n",
              "   'lambda_l2': 8.539953109394675,\n",
              "   'max_bin': 47.49826121182029,\n",
              "   'max_depth': 8.999627090062027,\n",
              "   'min_child_weight': 34.5372274617776,\n",
              "   'min_data_in_leaf': 164.45701046555462,\n",
              "   'num_leaves': 49.57867373971419}},\n",
              " {'target': 0.7687114634910718,\n",
              "  'params': {'bagging_fraction': 0.9707101482347569,\n",
              "   'feature_fraction': 0.6446304084751359,\n",
              "   'lambda_l1': 36.895675572499385,\n",
              "   'lambda_l2': 1.9051562879071906,\n",
              "   'max_bin': 378.88812663146865,\n",
              "   'max_depth': 9.576771523517493,\n",
              "   'min_child_weight': 47.93924951072747,\n",
              "   'min_data_in_leaf': 108.42458883918025,\n",
              "   'num_leaves': 25.81430654629507}},\n",
              " {'target': 0.7718111662970083,\n",
              "  'params': {'bagging_fraction': 0.5754069027269129,\n",
              "   'feature_fraction': 0.7482186706912217,\n",
              "   'lambda_l1': 6.943057799845263,\n",
              "   'lambda_l2': 9.52747316232688,\n",
              "   'max_bin': 185.8683083359433,\n",
              "   'max_depth': 11.15539259574,\n",
              "   'min_child_weight': 2.698545324998308,\n",
              "   'min_data_in_leaf': 195.68310244680038,\n",
              "   'num_leaves': 54.58265366779938}},\n",
              " {'target': 0.7716905284261153,\n",
              "  'params': {'bagging_fraction': 0.791661982638599,\n",
              "   'feature_fraction': 0.6605051402067922,\n",
              "   'lambda_l1': 3.4104212745018185,\n",
              "   'lambda_l2': 8.115945070358096,\n",
              "   'max_bin': 146.54012416035053,\n",
              "   'max_depth': 10.417648175230251,\n",
              "   'min_child_weight': 1.4180074553193076,\n",
              "   'min_data_in_leaf': 102.01504347366142,\n",
              "   'num_leaves': 56.140917586018034}},\n",
              " {'target': 0.7701900669922322,\n",
              "  'params': {'bagging_fraction': 0.5176386777761051,\n",
              "   'feature_fraction': 0.6145074911288368,\n",
              "   'lambda_l1': 4.366000163341913,\n",
              "   'lambda_l2': 2.5707335917583762,\n",
              "   'max_bin': 111.39585114915086,\n",
              "   'max_depth': 6.678620073549007,\n",
              "   'min_child_weight': 6.431766588419492,\n",
              "   'min_data_in_leaf': 86.05285708510192,\n",
              "   'num_leaves': 29.799311383594773}},\n",
              " {'target': 0.7708307863985207,\n",
              "  'params': {'bagging_fraction': 1.0,\n",
              "   'feature_fraction': 1.0,\n",
              "   'lambda_l1': 0.01,\n",
              "   'lambda_l2': 0.001,\n",
              "   'max_bin': 201.87175711924215,\n",
              "   'max_depth': 16.0,\n",
              "   'min_child_weight': 1.0,\n",
              "   'min_data_in_leaf': 189.8553861436057,\n",
              "   'num_leaves': 64.0}},\n",
              " {'target': 0.7719997542250262,\n",
              "  'params': {'bagging_fraction': 0.6114748246676402,\n",
              "   'feature_fraction': 0.9252070326413618,\n",
              "   'lambda_l1': 6.227021904304498,\n",
              "   'lambda_l2': 9.844477214259657,\n",
              "   'max_bin': 153.69685844257907,\n",
              "   'max_depth': 9.391343257415311,\n",
              "   'min_child_weight': 4.001203263458588,\n",
              "   'min_data_in_leaf': 131.7722947221485,\n",
              "   'num_leaves': 61.40198317680489}},\n",
              " {'target': 0.7722128809560068,\n",
              "  'params': {'bagging_fraction': 0.6475157210105189,\n",
              "   'feature_fraction': 0.5026014053686318,\n",
              "   'lambda_l1': 0.8678952276657708,\n",
              "   'lambda_l2': 2.108427218491435,\n",
              "   'max_bin': 159.7514426083027,\n",
              "   'max_depth': 9.651405177349304,\n",
              "   'min_child_weight': 45.072924694401834,\n",
              "   'min_data_in_leaf': 121.77348343784082,\n",
              "   'num_leaves': 60.13997800642714}},\n",
              " {'target': 0.7709315329107215,\n",
              "  'params': {'bagging_fraction': 0.9024808966462303,\n",
              "   'feature_fraction': 0.5038753407648007,\n",
              "   'lambda_l1': 26.801718133436975,\n",
              "   'lambda_l2': 6.034542815576746,\n",
              "   'max_bin': 159.45233398204377,\n",
              "   'max_depth': 9.296591640023367,\n",
              "   'min_child_weight': 36.24576576521278,\n",
              "   'min_data_in_leaf': 103.70752753815785,\n",
              "   'num_leaves': 61.159466432090326}}]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OVWJDtrCQbm",
        "outputId": "fba7c113-b452-48ce-dfcf-dfefd28dfa74"
      },
      "source": [
        "target_list = []\n",
        "for result in lgbBO.res:\n",
        "    target = result['target']\n",
        "    target_list.append(target)\n",
        "print(target_list)\n",
        "# 가장 큰 target 값을 가지는 순번(index)를 추출\n",
        "print('maximum target index:', np.argmax(np.array(target_list)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7704130128940919, 0.7707320615580647, 0.7674060125927115, 0.7706174479634317, 0.7692640265604144, 0.7707441174483378, 0.770229710649176, 0.7719096310120473, 0.7691251381694691, 0.7705267701934897, 0.7710389366063817, 0.7718773672753726, 0.7711454074315656, 0.7695691149218623, 0.7724457191365559, 0.7702728801817355, 0.7691546772195966, 0.7718560657970831, 0.7708603583870698, 0.7719703571205899, 0.77054013792762, 0.7684607098023677, 0.7687114634910718, 0.7718111662970083, 0.7716905284261153, 0.7701900669922322, 0.7708307863985207, 0.7719997542250262, 0.7722128809560068, 0.7709315329107215]\n",
            "maximum target index: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mDtnA98CQbm",
        "outputId": "bbbf1350-c190-49dc-e03c-3ecefd282423"
      },
      "source": [
        "max_dict = lgbBO.res[np.argmax(np.array(target_list))]\n",
        "print(max_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'target': 0.7724457191365559, 'params': {'bagging_fraction': 0.9299169469599182, 'feature_fraction': 0.5746809011453164, 'lambda_l1': 7.543441207233135, 'lambda_l2': 3.292195078359306, 'max_bin': 177.94521484058205, 'max_depth': 10.61966768561966, 'min_child_weight': 1.397663097770165, 'min_data_in_leaf': 166.95618212603793, 'num_leaves': 54.18985368611588}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz0aF9WECQbm",
        "outputId": "d5507f45-608f-4990-827f-0a6d78bae2da"
      },
      "source": [
        "ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "target_app = apps_all_train['TARGET']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020)\n",
        "print('train shape:', train_x.shape, 'valid shape:', valid_x.shape)\n",
        "lgbm_wrapper = LGBMClassifier(\n",
        "    n_jobs=-1,\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.02,\n",
        "    max_depth = 11,\n",
        "    num_leaves=54,\n",
        "    colsample_bytree=0.574,\n",
        "    subsample=0.930,\n",
        "    max_bin=403,\n",
        "    reg_alpha=7.543,\n",
        "    reg_lambda=3.292,\n",
        "    min_child_weight=1,\n",
        "    min_child_samples=167,\n",
        "    silent=-1,\n",
        "    verbose=-1,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "evals = [(X_test, y_test)]\n",
        "lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals, verbose=100)\n",
        "preds = lgbm_wrapper.predict(X_test)\n",
        "pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (215257, 174) valid shape: (92254, 174)\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's auc: 0.759222\tvalid_0's binary_logloss: 0.248113\n",
            "[200]\tvalid_0's auc: 0.769117\tvalid_0's binary_logloss: 0.24319\n",
            "[300]\tvalid_0's auc: 0.773816\tvalid_0's binary_logloss: 0.241396\n",
            "[400]\tvalid_0's auc: 0.77635\tvalid_0's binary_logloss: 0.24053\n",
            "[500]\tvalid_0's auc: 0.777994\tvalid_0's binary_logloss: 0.239917\n",
            "[600]\tvalid_0's auc: 0.77869\tvalid_0's binary_logloss: 0.239651\n",
            "[700]\tvalid_0's auc: 0.779292\tvalid_0's binary_logloss: 0.239452\n",
            "[800]\tvalid_0's auc: 0.779592\tvalid_0's binary_logloss: 0.239369\n",
            "[900]\tvalid_0's auc: 0.77968\tvalid_0's binary_logloss: 0.239334\n",
            "[1000]\tvalid_0's auc: 0.779873\tvalid_0's binary_logloss: 0.239287\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[998]\tvalid_0's auc: 0.779896\tvalid_0's binary_logloss: 0.239281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcMCkgh_CQbm",
        "outputId": "02806692-ae0f-4ec6-b8dc-dc9853646608"
      },
      "source": [
        "get_clf_eval(y_test, preds, pred_proba)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "오차 행렬\n",
            "[[84646   187]\n",
            " [ 7200   221]]\n",
            "정확도: 0.9199, 정밀도: 0.5417, 재현율: 0.0298,          F1: 0.0565, AUC:0.7799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PCGudgWCQbm",
        "outputId": "caf1ee74-6a4f-404a-e826-085b8a5e8c56"
      },
      "source": [
        "roc_curve_plot(y_test, pred_proba)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"265.995469pt\" version=\"1.1\" viewBox=\"0 0 385.78125 265.995469\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-09-01T10:27:44.518246</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 265.995469 \r\nL 385.78125 265.995469 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 378.58125 228.439219 \r\nL 378.58125 10.999219 \r\nL 43.78125 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"md928ba9a49\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.52125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.05 -->\r\n      <g transform=\"translate(49.388438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"94.00125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.15 -->\r\n      <g transform=\"translate(82.868437 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"127.48125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.25 -->\r\n      <g transform=\"translate(116.348438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"160.96125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.35 -->\r\n      <g transform=\"translate(149.828438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"194.44125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.45 -->\r\n      <g transform=\"translate(183.308437 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.92125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.55 -->\r\n      <g transform=\"translate(216.788438 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"261.40125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.65 -->\r\n      <g transform=\"translate(250.268438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"294.88125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.75 -->\r\n      <g transform=\"translate(283.748438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"328.36125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.85 -->\r\n      <g transform=\"translate(317.228437 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_10\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"361.84125\" xlink:href=\"#md928ba9a49\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.95 -->\r\n      <g transform=\"translate(350.708438 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_11\">\r\n     <!-- FPR( 1 - Sensitivity ) -->\r\n     <g transform=\"translate(160.542969 256.715781)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.8125 72.90625 \r\nL 51.703125 72.90625 \r\nL 51.703125 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.109375 \r\nL 48.578125 43.109375 \r\nL 48.578125 34.8125 \r\nL 19.671875 34.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-70\"/>\r\n       <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n       <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n       <path d=\"M 31 75.875 \r\nQ 24.46875 64.65625 21.28125 53.65625 \r\nQ 18.109375 42.671875 18.109375 31.390625 \r\nQ 18.109375 20.125 21.3125 9.0625 \r\nQ 24.515625 -2 31 -13.1875 \r\nL 23.1875 -13.1875 \r\nQ 15.875 -1.703125 12.234375 9.375 \r\nQ 8.59375 20.453125 8.59375 31.390625 \r\nQ 8.59375 42.28125 12.203125 53.3125 \r\nQ 15.828125 64.359375 23.1875 75.875 \r\nz\r\n\" id=\"DejaVuSans-40\"/>\r\n       <path id=\"DejaVuSans-32\"/>\r\n       <path d=\"M 4.890625 31.390625 \r\nL 31.203125 31.390625 \r\nL 31.203125 23.390625 \r\nL 4.890625 23.390625 \r\nz\r\n\" id=\"DejaVuSans-45\"/>\r\n       <path d=\"M 53.515625 70.515625 \r\nL 53.515625 60.890625 \r\nQ 47.90625 63.578125 42.921875 64.890625 \r\nQ 37.9375 66.21875 33.296875 66.21875 \r\nQ 25.25 66.21875 20.875 63.09375 \r\nQ 16.5 59.96875 16.5 54.203125 \r\nQ 16.5 49.359375 19.40625 46.890625 \r\nQ 22.3125 44.4375 30.421875 42.921875 \r\nL 36.375 41.703125 \r\nQ 47.40625 39.59375 52.65625 34.296875 \r\nQ 57.90625 29 57.90625 20.125 \r\nQ 57.90625 9.515625 50.796875 4.046875 \r\nQ 43.703125 -1.421875 29.984375 -1.421875 \r\nQ 24.8125 -1.421875 18.96875 -0.25 \r\nQ 13.140625 0.921875 6.890625 3.21875 \r\nL 6.890625 13.375 \r\nQ 12.890625 10.015625 18.65625 8.296875 \r\nQ 24.421875 6.59375 29.984375 6.59375 \r\nQ 38.421875 6.59375 43.015625 9.90625 \r\nQ 47.609375 13.234375 47.609375 19.390625 \r\nQ 47.609375 24.75 44.3125 27.78125 \r\nQ 41.015625 30.8125 33.5 32.328125 \r\nL 27.484375 33.5 \r\nQ 16.453125 35.6875 11.515625 40.375 \r\nQ 6.59375 45.0625 6.59375 53.421875 \r\nQ 6.59375 63.09375 13.40625 68.65625 \r\nQ 20.21875 74.21875 32.171875 74.21875 \r\nQ 37.3125 74.21875 42.625 73.28125 \r\nQ 47.953125 72.359375 53.515625 70.515625 \r\nz\r\n\" id=\"DejaVuSans-83\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n       <path d=\"M 8.015625 75.875 \r\nL 15.828125 75.875 \r\nQ 23.140625 64.359375 26.78125 53.3125 \r\nQ 30.421875 42.28125 30.421875 31.390625 \r\nQ 30.421875 20.453125 26.78125 9.375 \r\nQ 23.140625 -1.703125 15.828125 -13.1875 \r\nL 8.015625 -13.1875 \r\nQ 14.5 -2 17.703125 9.0625 \r\nQ 20.90625 20.125 20.90625 31.390625 \r\nQ 20.90625 42.671875 17.703125 53.65625 \r\nQ 14.5 64.65625 8.015625 75.875 \r\nz\r\n\" id=\"DejaVuSans-41\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"117.822266\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"187.304688\" xlink:href=\"#DejaVuSans-40\"/>\r\n      <use x=\"226.318359\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"258.105469\" xlink:href=\"#DejaVuSans-49\"/>\r\n      <use x=\"321.728516\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-45\"/>\r\n      <use x=\"389.599609\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"421.386719\" xlink:href=\"#DejaVuSans-83\"/>\r\n      <use x=\"484.863281\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"546.386719\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"609.765625\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"661.865234\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"689.648438\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"728.857422\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"756.640625\" xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"815.820312\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"843.603516\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"882.8125\" xlink:href=\"#DejaVuSans-121\"/>\r\n      <use x=\"941.992188\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"973.779297\" xlink:href=\"#DejaVuSans-41\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_11\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m06e0c079f7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m06e0c079f7\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(20.878125 232.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m06e0c079f7\" y=\"184.951219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 188.750437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m06e0c079f7\" y=\"141.463219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 145.262437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m06e0c079f7\" y=\"97.975219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 101.774437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m06e0c079f7\" y=\"54.487219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 58.286437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m06e0c079f7\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_18\">\r\n     <!-- TPR( Recall ) -->\r\n     <g transform=\"translate(14.798438 151.259062)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"61.083984\" xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"121.386719\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"190.869141\" xlink:href=\"#DejaVuSans-40\"/>\r\n      <use x=\"229.882812\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"261.669922\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"326.652344\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"388.175781\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"443.15625\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"504.435547\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"532.21875\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"560.001953\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"591.789062\" xlink:href=\"#DejaVuSans-41\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p14b7ad9762)\" d=\"M 43.78125 228.439219 \r\nL 43.891754 227.325795 \r\nL 43.899647 227.237893 \r\nL 44.006205 225.860763 \r\nL 44.025938 225.772861 \r\nL 44.136442 224.747339 \r\nL 44.152228 224.659437 \r\nL 44.262732 223.956222 \r\nL 44.290358 223.86832 \r\nL 44.400863 222.842798 \r\nL 44.420596 222.813497 \r\nL 44.523207 221.641472 \r\nL 44.546886 221.55357 \r\nL 44.65739 220.879655 \r\nL 44.692909 220.791753 \r\nL 44.799467 220.000636 \r\nL 44.823146 219.912734 \r\nL 44.933651 219.561127 \r\nL 44.941544 219.473225 \r\nL 45.048101 219.033715 \r\nL 45.091514 218.945814 \r\nL 45.202018 217.890991 \r\nL 45.221751 217.86169 \r\nL 45.332255 217.187776 \r\nL 45.355934 217.099874 \r\nL 45.446706 216.894769 \r\nL 45.478278 216.836168 \r\nL 45.561156 216.250155 \r\nL 45.628248 216.162253 \r\nL 45.738752 215.312535 \r\nL 45.778218 215.253934 \r\nL 45.880829 214.63862 \r\nL 45.912402 214.550719 \r\nL 46.022906 214.16981 \r\nL 46.042639 214.111209 \r\nL 46.14525 213.525196 \r\nL 46.164983 213.525196 \r\nL 46.263647 212.997785 \r\nL 46.29522 212.909883 \r\nL 46.401777 212.206668 \r\nL 46.42151 212.118766 \r\nL 46.532014 211.474152 \r\nL 46.55964 211.38625 \r\nL 46.622786 210.946741 \r\nL 46.697771 210.858839 \r\nL 46.808275 209.833317 \r\nL 46.828008 209.745415 \r\nL 46.938512 209.305905 \r\nL 46.946405 209.218003 \r\nL 47.049016 208.866396 \r\nL 47.072696 208.778494 \r\nL 47.175307 208.221782 \r\nL 47.206879 208.16318 \r\nL 47.313437 207.606468 \r\nL 47.33317 207.518567 \r\nL 47.435781 207.137658 \r\nL 47.48314 207.108358 \r\nL 47.577857 206.375842 \r\nL 47.60943 206.317241 \r\nL 47.712041 205.936332 \r\nL 47.723881 205.936332 \r\nL 47.818599 204.969411 \r\nL 47.865958 204.91081 \r\nL 47.976462 204.442 \r\nL 48.015928 204.354098 \r\nL 48.126432 203.592282 \r\nL 48.18563 203.50438 \r\nL 48.296135 203.06487 \r\nL 48.323761 203.03557 \r\nL 48.430318 202.56676 \r\nL 48.465837 202.478858 \r\nL 48.576342 202.010047 \r\nL 48.627647 201.922146 \r\nL 48.738151 201.570538 \r\nL 48.761831 201.511937 \r\nL 48.872335 200.896623 \r\nL 48.892068 200.808722 \r\nL 48.978892 200.339911 \r\nL 49.042038 200.252009 \r\nL 49.124916 199.959003 \r\nL 49.199901 199.871101 \r\nL 49.306458 199.402291 \r\nL 49.353817 199.314389 \r\nL 49.452482 198.845579 \r\nL 49.491947 198.757677 \r\nL 49.598505 198.40607 \r\nL 49.630078 198.347468 \r\nL 49.728742 197.96656 \r\nL 49.795834 197.907959 \r\nL 49.898445 197.439149 \r\nL 49.957644 197.351247 \r\nL 50.052361 196.823835 \r\nL 50.115507 196.765234 \r\nL 50.222064 196.12062 \r\nL 50.300996 196.032718 \r\nL 50.407553 195.65181 \r\nL 50.435179 195.563908 \r\nL 50.522004 195.212301 \r\nL 50.577256 195.124399 \r\nL 50.675921 194.831392 \r\nL 50.743013 194.772791 \r\nL 50.845624 194.216079 \r\nL 50.87325 194.128177 \r\nL 50.956128 193.835171 \r\nL 51.019273 193.747269 \r\nL 51.121884 193.395661 \r\nL 51.14951 193.33706 \r\nL 51.252121 192.985452 \r\nL 51.28764 192.897551 \r\nL 51.390251 192.282237 \r\nL 51.445503 192.194335 \r\nL 51.500755 191.989231 \r\nL 51.595473 191.901329 \r\nL 51.694138 191.608323 \r\nL 51.729657 191.520421 \r\nL 51.836214 190.993009 \r\nL 51.859894 190.993009 \r\nL 51.958558 190.494899 \r\nL 52.001971 190.465598 \r\nL 52.084849 189.820984 \r\nL 52.136154 189.733082 \r\nL 52.222979 189.234971 \r\nL 52.32559 189.147069 \r\nL 52.436094 188.736861 \r\nL 52.518972 188.648959 \r\nL 52.62553 188.180149 \r\nL 52.716301 188.092247 \r\nL 52.818912 187.682038 \r\nL 52.870218 187.594136 \r\nL 52.976775 187.125326 \r\nL 53.035974 187.066724 \r\nL 53.142532 186.510012 \r\nL 53.253036 186.422111 \r\nL 53.33986 186.217006 \r\nL 53.418792 186.129104 \r\nL 53.52535 185.865399 \r\nL 53.560869 185.777497 \r\nL 53.655587 185.308686 \r\nL 53.730572 185.220785 \r\nL 53.809503 184.927778 \r\nL 53.888435 184.839876 \r\nL 53.987099 184.664073 \r\nL 54.054191 184.576171 \r\nL 54.144962 184.195262 \r\nL 54.196268 184.107361 \r\nL 54.306772 183.755753 \r\nL 54.322558 183.697152 \r\nL 54.425169 183.023237 \r\nL 54.476475 182.935335 \r\nL 54.583032 182.583728 \r\nL 54.622498 182.495826 \r\nL 54.725109 182.056316 \r\nL 54.780361 181.968414 \r\nL 54.890865 181.558205 \r\nL 54.906652 181.528905 \r\nL 55.00137 181.235898 \r\nL 55.048728 181.147997 \r\nL 55.155286 180.679186 \r\nL 55.210538 180.591285 \r\nL 55.313149 180.474082 \r\nL 55.368401 180.38618 \r\nL 55.411814 180.181076 \r\nL 55.545997 180.093174 \r\nL 55.652555 179.653664 \r\nL 55.668341 179.624364 \r\nL 55.743326 179.331357 \r\nL 55.818311 179.243455 \r\nL 55.920922 178.97975 \r\nL 55.972228 178.891848 \r\nL 56.070892 178.481639 \r\nL 56.134037 178.393737 \r\nL 56.228755 178.012829 \r\nL 56.272167 177.954228 \r\nL 56.382672 177.631921 \r\nL 56.430031 177.544019 \r\nL 56.536588 177.368215 \r\nL 56.607627 177.280313 \r\nL 56.702344 176.870104 \r\nL 56.75365 176.782202 \r\nL 56.836528 176.577098 \r\nL 56.954925 176.489196 \r\nL 57.049643 176.254791 \r\nL 57.116735 176.166889 \r\nL 57.219346 175.932484 \r\nL 57.250919 175.844582 \r\nL 57.34169 175.463674 \r\nL 57.416675 175.375772 \r\nL 57.523232 175.082765 \r\nL 57.606111 174.994864 \r\nL 57.688989 174.701857 \r\nL 57.783707 174.613955 \r\nL 57.894211 174.35025 \r\nL 57.925783 174.262348 \r\nL 58.036288 173.91074 \r\nL 58.099433 173.852139 \r\nL 58.209937 173.383329 \r\nL 58.221777 173.324727 \r\nL 58.316495 172.855917 \r\nL 58.39148 172.768015 \r\nL 58.498037 172.240604 \r\nL 58.545396 172.152702 \r\nL 58.648007 171.801095 \r\nL 58.722992 171.713193 \r\nL 58.81771 171.449487 \r\nL 58.849282 171.420186 \r\nL 58.959787 170.980677 \r\nL 59.015039 170.892775 \r\nL 59.125543 170.629069 \r\nL 59.161062 170.570468 \r\nL 59.247887 170.365363 \r\nL 59.318925 170.277462 \r\nL 59.318925 170.21886 \r\nL 59.480735 170.130958 \r\nL 59.591239 169.837952 \r\nL 59.674117 169.75005 \r\nL 59.784621 169.486344 \r\nL 59.847767 169.398443 \r\nL 59.938538 168.958933 \r\nL 59.997737 168.871031 \r\nL 60.080615 168.519424 \r\nL 60.12008 168.490123 \r\nL 60.226638 168.138515 \r\nL 60.266104 168.050613 \r\nL 60.368715 167.786908 \r\nL 60.423967 167.757607 \r\nL 60.522631 167.347398 \r\nL 60.59367 167.259496 \r\nL 60.696281 167.054392 \r\nL 60.723907 166.96649 \r\nL 60.818625 166.585582 \r\nL 60.850197 166.49768 \r\nL 60.960701 166.204674 \r\nL 61.035686 166.116772 \r\nL 61.146191 165.823765 \r\nL 61.221176 165.735863 \r\nL 61.327733 165.208452 \r\nL 61.367199 165.12055 \r\nL 61.46981 164.856844 \r\nL 61.576367 164.768943 \r\nL 61.686872 164.446636 \r\nL 61.702658 164.446636 \r\nL 61.805269 164.007126 \r\nL 61.899987 163.948525 \r\nL 61.998651 163.655518 \r\nL 62.120995 163.596917 \r\nL 62.223606 163.303911 \r\nL 62.365683 163.216009 \r\nL 62.468294 162.893702 \r\nL 62.677463 162.8058 \r\nL 62.776127 162.542094 \r\nL 62.847165 162.454192 \r\nL 62.95767 162.131886 \r\nL 63.052387 162.043984 \r\nL 63.162892 161.809579 \r\nL 63.206304 161.721677 \r\nL 63.304968 161.457971 \r\nL 63.435205 161.370069 \r\nL 63.541763 161.164965 \r\nL 63.600962 161.077063 \r\nL 63.711466 160.901259 \r\nL 63.908795 160.813357 \r\nL 64.007459 160.549651 \r\nL 64.062711 160.49105 \r\nL 64.09823 160.344547 \r\nL 64.232414 160.285946 \r\nL 64.335025 159.934338 \r\nL 64.374491 159.846436 \r\nL 64.473155 159.670632 \r\nL 64.55998 159.58273 \r\nL 64.603392 159.260423 \r\nL 64.729683 159.172522 \r\nL 64.812561 159.026018 \r\nL 64.974371 158.938116 \r\nL 65.080928 158.61581 \r\nL 65.13618 158.527908 \r\nL 65.203272 158.440006 \r\nL 65.28615 158.352104 \r\nL 65.372975 158.029797 \r\nL 65.483479 157.941895 \r\nL 65.593983 157.560987 \r\nL 65.641342 157.473085 \r\nL 65.7479 157.209379 \r\nL 65.826831 157.121477 \r\nL 65.937335 156.740569 \r\nL 66.004427 156.652667 \r\nL 66.095199 156.418262 \r\nL 66.253062 156.33036 \r\nL 66.34778 156.125256 \r\nL 66.48591 156.037354 \r\nL 66.572734 155.802949 \r\nL 66.659559 155.715047 \r\nL 66.746384 155.42204 \r\nL 66.805583 155.334139 \r\nL 66.916087 155.041132 \r\nL 66.995018 154.982531 \r\nL 67.089736 154.836028 \r\nL 67.129202 154.777427 \r\nL 67.216027 154.543021 \r\nL 67.298905 154.45512 \r\nL 67.401516 154.191414 \r\nL 67.484394 154.103512 \r\nL 67.594898 153.722604 \r\nL 67.701456 153.634702 \r\nL 67.808013 153.283094 \r\nL 67.855372 153.253794 \r\nL 67.965876 152.872885 \r\nL 68.032968 152.784983 \r\nL 68.143472 152.404075 \r\nL 68.25003 152.374775 \r\nL 68.352641 152.16967 \r\nL 68.447359 152.111069 \r\nL 68.534183 151.905964 \r\nL 68.695993 151.818063 \r\nL 68.806497 151.349252 \r\nL 68.83807 151.290651 \r\nL 68.948574 151.114847 \r\nL 69.015666 151.026945 \r\nL 69.12617 150.733939 \r\nL 69.185369 150.646037 \r\nL 69.284033 150.265129 \r\nL 69.410324 150.177227 \r\nL 69.516881 149.884221 \r\nL 69.599759 149.796319 \r\nL 69.686584 149.620515 \r\nL 69.804981 149.532613 \r\nL 69.915485 149.268907 \r\nL 69.998364 149.181006 \r\nL 70.065455 149.063803 \r\nL 70.235158 148.975901 \r\nL 70.321983 148.712195 \r\nL 70.408808 148.624294 \r\nL 70.475899 148.419189 \r\nL 70.582457 148.331287 \r\nL 70.669282 148.096882 \r\nL 70.866611 148.038281 \r\nL 70.953435 147.833176 \r\nL 71.079726 147.745275 \r\nL 71.166551 147.54017 \r\nL 71.217856 147.510869 \r\nL 71.32836 147.247164 \r\nL 71.438864 147.159262 \r\nL 71.529636 146.954157 \r\nL 71.64014 146.895556 \r\nL 71.750644 146.514648 \r\nL 71.79011 146.426746 \r\nL 71.884828 146.104439 \r\nL 71.959813 146.045838 \r\nL 72.022958 145.899335 \r\nL 72.097943 145.811433 \r\nL 72.2045 145.518426 \r\nL 72.322898 145.459825 \r\nL 72.413669 145.166819 \r\nL 72.512333 145.078917 \r\nL 72.595212 144.903113 \r\nL 72.682036 144.815211 \r\nL 72.768861 144.610107 \r\nL 72.899098 144.522205 \r\nL 73.009602 144.3171 \r\nL 73.037228 144.229199 \r\nL 73.143786 144.170597 \r\nL 73.218771 144.082695 \r\nL 73.313489 143.84829 \r\nL 73.39242 143.760388 \r\nL 73.495031 143.37948 \r\nL 73.558176 143.291578 \r\nL 73.605535 143.145075 \r\nL 73.71604 143.057173 \r\nL 73.779185 142.881369 \r\nL 73.877849 142.793468 \r\nL 73.937048 142.705566 \r\nL 74.11859 142.617664 \r\nL 74.229095 142.295357 \r\nL 74.276454 142.207455 \r\nL 74.379065 141.914449 \r\nL 74.434317 141.826547 \r\nL 74.532981 141.562841 \r\nL 74.59218 141.50424 \r\nL 74.698737 141.240534 \r\nL 74.769776 141.152632 \r\nL 74.781615 141.094031 \r\nL 75.03025 141.006129 \r\nL 75.121021 140.801024 \r\nL 75.267044 140.713123 \r\nL 75.365709 140.449417 \r\nL 75.405175 140.361515 \r\nL 75.495946 140.156411 \r\nL 75.574877 140.097809 \r\nL 75.685382 139.922005 \r\nL 75.760367 139.863404 \r\nL 75.870871 139.599698 \r\nL 75.926123 139.511797 \r\nL 76.001108 139.160189 \r\nL 76.095826 139.072287 \r\nL 76.162918 138.867183 \r\nL 76.308941 138.779281 \r\nL 76.407605 138.515575 \r\nL 76.49443 138.427673 \r\nL 76.577308 138.222569 \r\nL 76.743064 138.134667 \r\nL 76.841729 137.958863 \r\nL 76.893034 137.870961 \r\nL 77.003538 137.607255 \r\nL 77.10615 137.519354 \r\nL 77.204814 137.402151 \r\nL 77.342944 137.34355 \r\nL 77.38241 137.197047 \r\nL 77.524487 137.138445 \r\nL 77.634991 136.816138 \r\nL 77.678403 136.728236 \r\nL 77.788907 136.523132 \r\nL 77.816533 136.43523 \r\nL 77.903358 136.259426 \r\nL 77.986236 136.171524 \r\nL 78.080954 136.025021 \r\nL 78.317749 135.937119 \r\nL 78.341428 135.819917 \r\nL 78.590063 135.732015 \r\nL 78.69662 135.409708 \r\nL 78.811071 135.321806 \r\nL 78.878163 135.146002 \r\nL 79.059705 135.0581 \r\nL 79.150477 134.882297 \r\nL 79.229408 134.794395 \r\nL 79.320179 134.55999 \r\nL 79.387271 134.472088 \r\nL 79.478043 134.325584 \r\nL 79.545134 134.237683 \r\nL 79.620119 134.091179 \r\nL 79.718784 134.003278 \r\nL 79.821395 133.768872 \r\nL 79.947685 133.680971 \r\nL 80.02267 133.563768 \r\nL 80.168694 133.475866 \r\nL 80.259465 133.21216 \r\nL 80.354183 133.124259 \r\nL 80.444954 132.919154 \r\nL 80.571244 132.831252 \r\nL 80.650176 132.596847 \r\nL 80.740947 132.508945 \r\nL 80.808039 132.333141 \r\nL 80.91065 132.24524 \r\nL 81.009315 131.952233 \r\nL 81.068513 131.864331 \r\nL 81.163231 131.542024 \r\nL 81.250056 131.454122 \r\nL 81.34872 131.307619 \r\nL 81.411865 131.219717 \r\nL 81.52237 130.86811 \r\nL 81.632874 130.780208 \r\nL 81.735485 130.487202 \r\nL 81.893348 130.3993 \r\nL 81.976226 130.223496 \r\nL 82.102516 130.135594 \r\nL 82.165662 129.930489 \r\nL 82.37483 129.842588 \r\nL 82.469548 129.608183 \r\nL 82.587946 129.520281 \r\nL 82.66293 129.432379 \r\nL 82.733969 129.344477 \r\nL 82.820794 129.168673 \r\nL 83.002336 129.080771 \r\nL 83.108894 128.846366 \r\nL 83.168092 128.758464 \r\nL 83.254917 128.641262 \r\nL 83.432513 128.55336 \r\nL 83.515391 128.318955 \r\nL 83.578536 128.231053 \r\nL 83.677201 127.996648 \r\nL 83.748239 127.908746 \r\nL 83.842957 127.674341 \r\nL 84.063965 127.586439 \r\nL 84.15079 127.527838 \r\nL 84.340226 127.439936 \r\nL 84.430997 127.234831 \r\nL 84.553341 127.146929 \r\nL 84.655952 127.029727 \r\nL 84.76251 126.941825 \r\nL 84.853281 126.824622 \r\nL 84.940106 126.73672 \r\nL 85.046663 126.502315 \r\nL 85.208473 126.414413 \r\nL 85.311084 126.23861 \r\nL 85.417641 126.150708 \r\nL 85.48868 125.974904 \r\nL 85.622864 125.887002 \r\nL 85.670222 125.7698 \r\nL 85.859658 125.681898 \r\nL 85.970162 125.506094 \r\nL 86.135919 125.418192 \r\nL 86.20301 125.300989 \r\nL 86.317461 125.213088 \r\nL 86.3885 125.037284 \r\nL 86.581882 124.978682 \r\nL 86.656867 124.832179 \r\nL 86.948914 124.744277 \r\nL 87.055471 124.656375 \r\nL 87.106777 124.568474 \r\nL 87.169922 124.480572 \r\nL 87.268586 124.39267 \r\nL 87.323838 124.275467 \r\nL 87.505381 124.187565 \r\nL 87.600099 124.041062 \r\nL 87.765855 123.95316 \r\nL 87.856626 123.660154 \r\nL 87.975024 123.601553 \r\nL 88.081581 123.425749 \r\nL 88.14078 123.337847 \r\nL 88.239444 123.162043 \r\nL 88.357842 123.074141 \r\nL 88.448613 122.869037 \r\nL 88.56701 122.781135 \r\nL 88.669621 122.634632 \r\nL 88.720927 122.54673 \r\nL 88.819591 122.312325 \r\nL 88.922202 122.224423 \r\nL 89.012974 122.07792 \r\nL 89.257661 121.990018 \r\nL 89.340539 121.755613 \r\nL 89.482616 121.667711 \r\nL 89.581281 121.579809 \r\nL 89.74309 121.491907 \r\nL 89.798342 121.316103 \r\nL 89.975938 121.228201 \r\nL 90.086443 121.110999 \r\nL 90.224573 121.023097 \r\nL 90.335077 120.935195 \r\nL 90.437688 120.847293 \r\nL 90.540299 120.583587 \r\nL 90.702109 120.524986 \r\nL 90.812613 120.319882 \r\nL 90.899437 120.23198 \r\nL 91.002049 120.085477 \r\nL 91.124392 119.997575 \r\nL 91.163858 119.880372 \r\nL 91.396706 119.79247 \r\nL 91.499317 119.587366 \r\nL 91.68086 119.499464 \r\nL 91.775578 119.32366 \r\nL 91.81899 119.265059 \r\nL 91.901868 119.089255 \r\nL 92.028159 119.001353 \r\nL 92.11893 118.913451 \r\nL 92.24522 118.825549 \r\nL 92.332045 118.532543 \r\nL 92.438603 118.444641 \r\nL 92.541214 118.239537 \r\nL 92.639878 118.151635 \r\nL 92.742489 117.91723 \r\nL 92.888513 117.829328 \r\nL 92.999017 117.741426 \r\nL 93.117414 117.653524 \r\nL 93.180559 117.536322 \r\nL 93.413407 117.47772 \r\nL 93.523911 117.331217 \r\nL 93.563377 117.243315 \r\nL 93.669935 116.833106 \r\nL 93.705454 116.745204 \r\nL 93.800172 116.569401 \r\nL 93.859371 116.481499 \r\nL 93.88305 116.393597 \r\nL 94.03302 116.305695 \r\nL 94.123791 116.159192 \r\nL 94.202723 116.07129 \r\nL 94.30928 115.924787 \r\nL 94.392158 115.895486 \r\nL 94.431624 115.778284 \r\nL 94.553968 115.690382 \r\nL 94.652633 115.485277 \r\nL 94.814442 115.397375 \r\nL 94.913107 115.250872 \r\nL 95.063077 115.16297 \r\nL 95.157795 114.987166 \r\nL 95.280138 114.899265 \r\nL 95.386696 114.664859 \r\nL 95.631384 114.576958 \r\nL 95.726102 114.459755 \r\nL 95.864232 114.371853 \r\nL 95.887911 114.313252 \r\nL 96.136546 114.22535 \r\nL 96.211531 114.078847 \r\nL 96.274676 113.990945 \r\nL 96.329928 113.932344 \r\nL 96.479898 113.844442 \r\nL 96.479898 113.78584 \r\nL 96.740372 113.697939 \r\nL 96.82325 113.492834 \r\nL 96.925861 113.404932 \r\nL 97.032419 113.258429 \r\nL 97.063991 113.170527 \r\nL 97.166602 113.024024 \r\nL 97.269213 112.936122 \r\nL 97.375771 112.81892 \r\nL 97.431023 112.731018 \r\nL 97.521794 112.584515 \r\nL 97.71123 112.496613 \r\nL 97.798055 112.320809 \r\nL 97.896719 112.291508 \r\nL 97.995384 112.086404 \r\nL 98.208499 111.998502 \r\nL 98.295324 111.822698 \r\nL 98.500546 111.734796 \r\nL 98.595263 111.617594 \r\nL 98.7255 111.529692 \r\nL 98.832058 111.47109 \r\nL 99.017547 111.383189 \r\nL 99.128051 111.060882 \r\nL 99.167517 110.97298 \r\nL 99.266182 110.621372 \r\nL 99.416152 110.562771 \r\nL 99.514816 110.474869 \r\nL 99.629267 110.386967 \r\nL 99.700305 110.269764 \r\nL 99.779237 110.211163 \r\nL 99.806863 110.123261 \r\nL 99.984459 110.035359 \r\nL 100.07523 109.830255 \r\nL 100.169948 109.742353 \r\nL 100.209414 109.683752 \r\nL 100.410689 109.59585 \r\nL 100.5133 109.420046 \r\nL 100.619858 109.332144 \r\nL 100.722469 109.185641 \r\nL 100.840866 109.097739 \r\nL 100.900065 109.039138 \r\nL 101.038195 108.951236 \r\nL 101.121073 108.746132 \r\nL 101.294722 108.65823 \r\nL 101.397333 108.335923 \r\nL 101.448639 108.248021 \r\nL 101.535464 108.101518 \r\nL 101.630181 108.013616 \r\nL 101.740686 107.808511 \r\nL 101.823564 107.720609 \r\nL 101.914335 107.544806 \r\nL 102.052465 107.456904 \r\nL 102.15113 107.251799 \r\nL 102.281367 107.163897 \r\nL 102.391871 106.900192 \r\nL 102.573413 106.81229 \r\nL 102.601039 106.753688 \r\nL 102.794422 106.665787 \r\nL 102.881246 106.519283 \r\nL 103.011484 106.431382 \r\nL 103.114095 106.34348 \r\nL 103.196973 106.255578 \r\nL 103.307477 106.079774 \r\nL 103.398248 105.991872 \r\nL 103.508752 105.786768 \r\nL 103.62715 105.698866 \r\nL 103.733707 105.493761 \r\nL 103.859998 105.405859 \r\nL 103.966555 105.288657 \r\nL 104.06522 105.230056 \r\nL 104.148098 104.937049 \r\nL 104.234923 104.849147 \r\nL 104.333587 104.673344 \r\nL 104.412518 104.585442 \r\nL 104.50329 104.49754 \r\nL 104.566435 104.438938 \r\nL 104.669046 104.263135 \r\nL 104.724298 104.175233 \r\nL 104.724298 104.145932 \r\nL 104.921627 104.05803 \r\nL 104.968986 103.911527 \r\nL 105.059757 103.823625 \r\nL 105.118956 103.735723 \r\nL 105.37943 103.647821 \r\nL 105.458362 103.559919 \r\nL 105.545186 103.472018 \r\nL 105.545186 103.413416 \r\nL 105.714889 103.325514 \r\nL 105.797767 103.237612 \r\nL 105.963523 103.149711 \r\nL 106.034562 103.061809 \r\nL 106.227944 102.973907 \r\nL 106.338448 102.856704 \r\nL 106.370021 102.768802 \r\nL 106.480525 102.622299 \r\nL 106.602869 102.534397 \r\nL 106.713373 102.417195 \r\nL 106.800198 102.329293 \r\nL 106.906755 102.124188 \r\nL 107.021206 102.036287 \r\nL 107.025153 101.977685 \r\nL 107.265894 101.889783 \r\nL 107.372452 101.772581 \r\nL 107.526368 101.684679 \r\nL 107.632926 101.538176 \r\nL 107.735537 101.450274 \r\nL 107.771056 101.333071 \r\nL 107.960492 101.245169 \r\nL 108.035477 101.069366 \r\nL 108.106515 101.010764 \r\nL 108.185446 100.717758 \r\nL 108.323577 100.629856 \r\nL 108.414348 100.424752 \r\nL 108.576158 100.33685 \r\nL 108.670875 100.102445 \r\nL 108.805059 100.014543 \r\nL 108.915563 99.926641 \r\nL 108.978709 99.838739 \r\nL 109.010281 99.692236 \r\nL 109.235236 99.604334 \r\nL 109.34574 99.516432 \r\nL 109.381259 99.457831 \r\nL 109.487817 99.311328 \r\nL 109.590428 99.252726 \r\nL 109.689092 99.076923 \r\nL 109.80749 98.989021 \r\nL 109.894314 98.871818 \r\nL 110.119269 98.813217 \r\nL 110.229774 98.666714 \r\nL 110.269239 98.578812 \r\nL 110.379744 98.432309 \r\nL 110.612592 98.373707 \r\nL 110.691523 98.227204 \r\nL 110.8336 98.139302 \r\nL 110.932264 97.758394 \r\nL 111.13354 97.670492 \r\nL 111.240097 97.523989 \r\nL 111.326922 97.436087 \r\nL 111.429533 97.289584 \r\nL 111.662381 97.201682 \r\nL 111.749206 97.084479 \r\nL 111.883389 96.996578 \r\nL 111.978107 96.820774 \r\nL 112.084665 96.732872 \r\nL 112.183329 96.527767 \r\nL 112.368819 96.469166 \r\nL 112.467483 96.234761 \r\nL 112.593773 96.146859 \r\nL 112.684545 95.883154 \r\nL 112.767423 95.795252 \r\nL 112.822675 95.70735 \r\nL 113.02395 95.619448 \r\nL 113.134455 95.531546 \r\nL 113.260745 95.443644 \r\nL 113.371249 95.23854 \r\nL 113.398875 95.179938 \r\nL 113.493593 95.004135 \r\nL 113.663296 94.916233 \r\nL 113.698815 94.857631 \r\nL 113.876411 94.769729 \r\nL 113.982969 94.593926 \r\nL 114.065847 94.506024 \r\nL 114.164511 94.359521 \r\nL 114.413146 94.271619 \r\nL 114.456558 94.183717 \r\nL 114.697299 94.095815 \r\nL 114.79991 93.949312 \r\nL 114.847269 93.86141 \r\nL 114.847269 93.832109 \r\nL 115.024865 93.773508 \r\nL 115.091957 93.539103 \r\nL 115.24982 93.451201 \r\nL 115.309019 93.304698 \r\nL 115.557653 93.216796 \r\nL 115.668157 93.128894 \r\nL 115.786555 93.040992 \r\nL 115.885219 92.865188 \r\nL 116.019403 92.777286 \r\nL 116.110174 92.542881 \r\nL 116.500885 92.454979 \r\nL 116.603496 92.308476 \r\nL 116.974474 92.220574 \r\nL 117.021833 92.161973 \r\nL 117.140231 92.103372 \r\nL 117.199429 91.839666 \r\nL 117.392812 91.751764 \r\nL 117.412544 91.663862 \r\nL 117.645392 91.57596 \r\nL 117.755897 91.370856 \r\nL 117.953226 91.282954 \r\nL 118.059783 91.048549 \r\nL 118.134768 90.989948 \r\nL 118.241326 90.843445 \r\nL 118.375509 90.755543 \r\nL 118.379456 90.696941 \r\nL 118.596518 90.60904 \r\nL 118.667556 90.550438 \r\nL 118.896458 90.462536 \r\nL 118.983282 90.286733 \r\nL 119.074054 90.198831 \r\nL 119.156932 90.081628 \r\nL 119.362154 89.993726 \r\nL 119.468711 89.876524 \r\nL 119.595002 89.788622 \r\nL 119.701559 89.70072 \r\nL 119.733132 89.612818 \r\nL 119.83969 89.466315 \r\nL 120.013339 89.378413 \r\nL 120.119897 89.290511 \r\nL 120.297492 89.202609 \r\nL 120.372477 89.114707 \r\nL 120.538234 89.026805 \r\nL 120.577699 88.968204 \r\nL 120.905265 88.880302 \r\nL 120.984197 88.704498 \r\nL 121.28019 88.616596 \r\nL 121.378855 88.470093 \r\nL 121.540664 88.382191 \r\nL 121.615649 88.264989 \r\nL 121.734047 88.177087 \r\nL 121.793245 88.030584 \r\nL 121.970841 87.942682 \r\nL 121.970841 87.884081 \r\nL 122.247102 87.796179 \r\nL 122.254995 87.737577 \r\nL 122.641759 87.649676 \r\nL 122.685172 87.591074 \r\nL 122.874608 87.503172 \r\nL 122.981165 87.268767 \r\nL 123.087723 87.210166 \r\nL 123.186387 87.092964 \r\nL 123.253479 87.005062 \r\nL 123.340304 86.770657 \r\nL 123.446861 86.712055 \r\nL 123.490274 86.624153 \r\nL 123.734961 86.536251 \r\nL 123.841519 86.419049 \r\nL 123.93229 86.331147 \r\nL 123.975703 86.272546 \r\nL 124.169085 86.184644 \r\nL 124.279589 86.00884 \r\nL 124.476918 85.920938 \r\nL 124.476918 85.891638 \r\nL 124.642674 85.833036 \r\nL 124.733445 85.715834 \r\nL 125.147836 85.627932 \r\nL 125.222821 85.510729 \r\nL 125.483295 85.422827 \r\nL 125.562227 85.276324 \r\nL 125.763502 85.188422 \r\nL 125.862167 85.012619 \r\nL 125.99635 84.924717 \r\nL 126.102908 84.778213 \r\nL 126.252878 84.690312 \r\nL 126.339702 84.60241 \r\nL 126.643589 84.514508 \r\nL 126.730414 84.338704 \r\nL 126.856704 84.250802 \r\nL 126.947475 84.074998 \r\nL 127.152697 83.987096 \r\nL 127.20795 83.811293 \r\nL 127.47237 83.723391 \r\nL 127.563141 83.576888 \r\nL 127.681539 83.488986 \r\nL 127.776257 83.283881 \r\nL 128.009105 83.195979 \r\nL 128.091983 83.108077 \r\nL 128.174861 83.049476 \r\nL 128.277472 82.815071 \r\nL 128.439282 82.727169 \r\nL 128.51032 82.639267 \r\nL 128.841833 82.551365 \r\nL 128.94839 82.346261 \r\nL 129.256223 82.258359 \r\nL 129.358834 82.082555 \r\nL 129.441712 81.994653 \r\nL 129.552216 81.936052 \r\nL 129.733759 81.84815 \r\nL 129.785065 81.730948 \r\nL 130.156043 81.643046 \r\nL 130.2626 81.467242 \r\nL 130.467822 81.37934 \r\nL 130.558594 81.262137 \r\nL 130.637525 81.203536 \r\nL 130.748029 81.086334 \r\nL 130.771709 81.057033 \r\nL 130.870373 80.91053 \r\nL 130.984824 80.822628 \r\nL 131.087435 80.588223 \r\nL 131.229512 80.500321 \r\nL 131.340016 80.353818 \r\nL 131.58865 80.265916 \r\nL 131.679422 80.207315 \r\nL 131.778086 80.119413 \r\nL 131.864911 80.00221 \r\nL 131.995148 79.914308 \r\nL 132.105652 79.797106 \r\nL 132.279301 79.709204 \r\nL 132.322714 79.592001 \r\nL 132.681852 79.504099 \r\nL 132.780517 79.416198 \r\nL 132.922594 79.328296 \r\nL 132.985739 79.211093 \r\nL 133.112029 79.123191 \r\nL 133.171228 79.06459 \r\nL 133.415916 78.976688 \r\nL 133.415916 78.947387 \r\nL 133.842146 78.859486 \r\nL 133.865826 78.800884 \r\nL 134.130246 78.712982 \r\nL 134.130246 78.683682 \r\nL 134.351255 78.59578 \r\nL 134.418346 78.390675 \r\nL 134.639355 78.302774 \r\nL 134.682767 78.185571 \r\nL 134.844577 78.097669 \r\nL 134.923508 77.980467 \r\nL 135.037959 77.892565 \r\nL 135.097158 77.804663 \r\nL 135.337899 77.716761 \r\nL 135.436563 77.599558 \r\nL 135.503655 77.511656 \r\nL 135.614159 77.394454 \r\nL 135.783862 77.306552 \r\nL 135.894366 77.247951 \r\nL 136.174573 77.160049 \r\nL 136.273238 77.042846 \r\nL 136.379795 76.954944 \r\nL 136.45478 76.867042 \r\nL 136.644216 76.779141 \r\nL 136.703415 76.661938 \r\nL 136.932316 76.574036 \r\nL 136.987568 76.486134 \r\nL 137.330921 76.398232 \r\nL 137.425638 76.31033 \r\nL 137.678219 76.222429 \r\nL 137.678219 76.193128 \r\nL 137.903174 76.134527 \r\nL 137.997892 76.017324 \r\nL 138.10445 75.929422 \r\nL 138.187328 75.753618 \r\nL 138.400443 75.665717 \r\nL 138.455695 75.548514 \r\nL 138.751688 75.460612 \r\nL 138.787208 75.402011 \r\nL 139.193705 75.314109 \r\nL 139.296316 75.226207 \r\nL 139.55679 75.138305 \r\nL 139.639668 75.079704 \r\nL 140.085632 74.991802 \r\nL 140.152723 74.9039 \r\nL 140.413197 74.815998 \r\nL 140.519755 74.669495 \r\nL 140.728924 74.581593 \r\nL 140.839428 74.317887 \r\nL 140.922306 74.229985 \r\nL 141.024917 73.96628 \r\nL 141.324857 73.878378 \r\nL 141.431414 73.643973 \r\nL 141.553758 73.556071 \r\nL 141.636636 73.468169 \r\nL 141.861591 73.380267 \r\nL 141.901057 73.233764 \r\nL 142.216783 73.204463 \r\nL 142.327288 73.05796 \r\nL 142.390433 72.970058 \r\nL 142.49699 72.882156 \r\nL 142.662747 72.794254 \r\nL 142.706159 72.735653 \r\nL 142.907434 72.647751 \r\nL 142.990313 72.530549 \r\nL 143.08503 72.442647 \r\nL 143.112656 72.354745 \r\nL 143.321825 72.266843 \r\nL 143.321825 72.237542 \r\nL 143.53494 72.149641 \r\nL 143.574406 72.061739 \r\nL 143.787521 71.973837 \r\nL 143.886186 71.739432 \r\nL 143.929598 71.68083 \r\nL 143.992743 71.505027 \r\nL 144.395294 71.417125 \r\nL 144.399241 71.358523 \r\nL 144.639982 71.270622 \r\nL 144.639982 71.241321 \r\nL 145.089892 71.153419 \r\nL 145.117518 71.065517 \r\nL 145.421404 70.977615 \r\nL 145.480603 70.919014 \r\nL 145.630573 70.860413 \r\nL 145.725291 70.655308 \r\nL 146.005498 70.567406 \r\nL 146.116002 70.391603 \r\nL 146.431728 70.303701 \r\nL 146.502766 70.186498 \r\nL 146.790867 70.127897 \r\nL 146.846119 69.981394 \r\nL 147.059234 69.893492 \r\nL 147.157898 69.717688 \r\nL 147.303922 69.659087 \r\nL 147.406533 69.512584 \r\nL 147.497304 69.424682 \r\nL 147.599915 69.36608 \r\nL 147.947214 69.278178 \r\nL 148.022199 69.190277 \r\nL 148.195848 69.102375 \r\nL 148.302406 68.985172 \r\nL 148.472109 68.89727 \r\nL 148.519467 68.809368 \r\nL 148.953591 68.721466 \r\nL 149.044362 68.545663 \r\nL 149.344302 68.457761 \r\nL 149.419287 68.399159 \r\nL 149.620563 68.311258 \r\nL 149.620563 68.281957 \r\nL 150.050739 68.194055 \r\nL 150.157297 68.047552 \r\nL 150.244122 67.95965 \r\nL 150.307267 67.842447 \r\nL 150.555901 67.754546 \r\nL 150.555901 67.695944 \r\nL 150.800589 67.608042 \r\nL 150.816375 67.549441 \r\nL 151.06501 67.461539 \r\nL 151.147888 67.315036 \r\nL 151.345217 67.227134 \r\nL 151.432042 67.109932 \r\nL 151.633317 67.02203 \r\nL 151.716195 66.904827 \r\nL 151.949043 66.816925 \r\nL 151.968776 66.729023 \r\nL 152.339754 66.641121 \r\nL 152.343701 66.58252 \r\nL 152.671267 66.494618 \r\nL 152.762038 66.406716 \r\nL 152.868596 66.318814 \r\nL 152.971207 66.260213 \r\nL 153.109337 66.201612 \r\nL 153.211948 66.11371 \r\nL 153.373758 66.025808 \r\nL 153.452689 65.908606 \r\nL 153.689484 65.850004 \r\nL 153.764469 65.703501 \r\nL 154.095981 65.615599 \r\nL 154.119661 65.556998 \r\nL 154.368295 65.469096 \r\nL 154.46696 65.410495 \r\nL 154.88135 65.322593 \r\nL 154.991854 65.20539 \r\nL 155.157611 65.117489 \r\nL 155.185237 65.058887 \r\nL 155.406245 64.970985 \r\nL 155.500963 64.795182 \r\nL 155.666719 64.70728 \r\nL 155.666719 64.677979 \r\nL 156.108736 64.590077 \r\nL 156.215293 64.472875 \r\nL 156.35737 64.414273 \r\nL 156.396836 64.326371 \r\nL 156.661257 64.23847 \r\nL 156.712562 64.150568 \r\nL 157.071701 64.062666 \r\nL 157.146686 63.974764 \r\nL 157.217724 63.886862 \r\nL 157.284816 63.769659 \r\nL 157.474252 63.681757 \r\nL 157.474252 63.652457 \r\nL 157.742619 63.564555 \r\nL 157.793924 63.505954 \r\nL 158.12149 63.418052 \r\nL 158.231994 63.300849 \r\nL 158.303033 63.242248 \r\nL 158.334605 63.125045 \r\nL 158.887126 63.037144 \r\nL 158.973951 62.978542 \r\nL 159.435701 62.89064 \r\nL 159.518579 62.832039 \r\nL 159.889557 62.744137 \r\nL 159.948756 62.685536 \r\nL 160.122405 62.597634 \r\nL 160.205283 62.509732 \r\nL 160.355253 62.42183 \r\nL 160.418398 62.275327 \r\nL 160.603887 62.187425 \r\nL 160.603887 62.158125 \r\nL 161.144569 62.070223 \r\nL 161.255073 61.894419 \r\nL 161.436615 61.806517 \r\nL 161.539226 61.660014 \r\nL 161.819433 61.572112 \r\nL 161.918098 61.396308 \r\nL 162.202251 61.308406 \r\nL 162.304862 61.103302 \r\nL 162.427206 61.0447 \r\nL 162.514031 60.927498 \r\nL 162.723199 60.839596 \r\nL 162.770558 60.722394 \r\nL 162.995513 60.634492 \r\nL 163.086285 60.54659 \r\nL 163.512515 60.458688 \r\nL 163.57566 60.400087 \r\nL 163.690111 60.312185 \r\nL 163.690111 60.282884 \r\nL 163.922959 60.194982 \r\nL 163.938745 60.136381 \r\nL 164.218952 60.048479 \r\nL 164.31367 59.931276 \r\nL 164.732007 59.843375 \r\nL 164.826725 59.755473 \r\nL 165.170077 59.667571 \r\nL 165.22533 59.550368 \r\nL 165.422658 59.462466 \r\nL 165.422658 59.433166 \r\nL 165.77785 59.374564 \r\nL 165.876515 59.228061 \r\nL 166.042271 59.140159 \r\nL 166.148829 59.022957 \r\nL 166.338264 58.935055 \r\nL 166.405356 58.847153 \r\nL 166.942091 58.759251 \r\nL 167.052595 58.642049 \r\nL 167.368321 58.554147 \r\nL 167.368321 58.524846 \r\nL 167.55381 58.466245 \r\nL 167.660368 58.319742 \r\nL 167.794551 58.23184 \r\nL 167.84191 58.143938 \r\nL 168.126064 58.056036 \r\nL 168.181316 57.938833 \r\nL 168.30366 57.850931 \r\nL 168.30366 57.821631 \r\nL 168.57992 57.733729 \r\nL 168.690424 57.557925 \r\nL 168.986418 57.470023 \r\nL 169.092975 57.352821 \r\nL 169.452114 57.264919 \r\nL 169.511313 57.177017 \r\nL 169.759947 57.089115 \r\nL 169.815199 57.030514 \r\nL 170.044101 56.942612 \r\nL 170.150658 56.825409 \r\nL 170.426919 56.737507 \r\nL 170.450598 56.678906 \r\nL 170.770271 56.591004 \r\nL 170.872882 56.473802 \r\nL 171.437242 56.3859 \r\nL 171.445136 56.327299 \r\nL 171.677984 56.239397 \r\nL 171.689823 56.180795 \r\nL 172.060802 56.092893 \r\nL 172.060802 56.063593 \r\nL 172.368635 55.975691 \r\nL 172.455459 55.858488 \r\nL 172.585696 55.770586 \r\nL 172.585696 55.741286 \r\nL 172.932995 55.653384 \r\nL 172.932995 55.624083 \r\nL 173.260561 55.536181 \r\nL 173.260561 55.506881 \r\nL 173.517089 55.418979 \r\nL 173.611807 55.272476 \r\nL 174.053823 55.184574 \r\nL 174.053823 55.155273 \r\nL 174.286671 55.067371 \r\nL 174.33403 54.979469 \r\nL 174.464267 54.891567 \r\nL 174.539252 54.774365 \r\nL 174.882604 54.686463 \r\nL 174.929963 54.627862 \r\nL 175.340407 54.53996 \r\nL 175.431179 54.364156 \r\nL 175.849516 54.276254 \r\nL 175.900821 54.188352 \r\nL 176.630938 54.10045 \r\nL 176.630938 54.07115 \r\nL 176.982184 53.983248 \r\nL 176.982184 53.953947 \r\nL 177.420254 53.866045 \r\nL 177.491292 53.778143 \r\nL 177.751766 53.690242 \r\nL 177.795179 53.60234 \r\nL 177.964881 53.514438 \r\nL 178.039866 53.426536 \r\nL 178.142477 53.338634 \r\nL 178.213516 53.16283 \r\nL 178.33586 53.074928 \r\nL 178.367432 53.016327 \r\nL 178.746304 52.928425 \r\nL 178.746304 52.899124 \r\nL 178.975205 52.811223 \r\nL 178.975205 52.781922 \r\nL 179.251466 52.723321 \r\nL 179.251466 52.664719 \r\nL 179.693482 52.576817 \r\nL 179.784254 52.488916 \r\nL 179.922384 52.401014 \r\nL 180.028941 52.313112 \r\nL 180.301255 52.22521 \r\nL 180.328881 52.137308 \r\nL 180.494637 52.049406 \r\nL 180.54989 51.990805 \r\nL 181.110304 51.902903 \r\nL 181.220808 51.7857 \r\nL 181.433923 51.697798 \r\nL 181.520748 51.639197 \r\nL 181.658878 51.551295 \r\nL 181.658878 51.521995 \r\nL 181.923298 51.434093 \r\nL 182.00223 51.375491 \r\nL 182.120627 51.28759 \r\nL 182.187719 51.199688 \r\nL 182.756026 51.111786 \r\nL 182.858637 50.994583 \r\nL 183.225669 50.906681 \r\nL 183.332227 50.760178 \r\nL 183.517716 50.701577 \r\nL 183.549288 50.613675 \r\nL 183.868961 50.525773 \r\nL 183.963679 50.408571 \r\nL 184.417535 50.320669 \r\nL 184.520146 50.262067 \r\nL 184.646437 50.174166 \r\nL 184.756941 50.115564 \r\nL 184.863499 50.027662 \r\nL 184.863499 49.998362 \r\nL 185.333141 49.91046 \r\nL 185.423913 49.793257 \r\nL 185.790944 49.705355 \r\nL 185.806731 49.617453 \r\nL 186.075098 49.529552 \r\nL 186.075098 49.500251 \r\nL 186.517115 49.412349 \r\nL 186.525008 49.353748 \r\nL 186.734176 49.265846 \r\nL 186.809161 49.177944 \r\nL 187.270911 49.090042 \r\nL 187.270911 49.060741 \r\nL 187.487973 49.00214 \r\nL 187.543225 48.914238 \r\nL 187.898417 48.826336 \r\nL 188.004974 48.738434 \r\nL 188.170731 48.650533 \r\nL 188.229929 48.591931 \r\nL 188.900847 48.504029 \r\nL 189.003458 48.386827 \r\nL 189.338917 48.298925 \r\nL 189.441528 48.211023 \r\nL 189.780934 48.123121 \r\nL 189.844079 48.005919 \r\nL 190.072981 47.918017 \r\nL 190.183485 47.771514 \r\nL 190.424226 47.683612 \r\nL 190.518944 47.62501 \r\nL 191.016213 47.537109 \r\nL 191.075411 47.478507 \r\nL 191.74633 47.390605 \r\nL 191.74633 47.361305 \r\nL 192.002857 47.273403 \r\nL 192.002857 47.244102 \r\nL 192.48434 47.1562 \r\nL 192.583004 47.097599 \r\nL 192.950036 47.009697 \r\nL 192.965822 46.951096 \r\nL 193.605168 46.863194 \r\nL 193.605168 46.833893 \r\nL 193.889321 46.745991 \r\nL 193.909054 46.68739 \r\nL 194.272139 46.599488 \r\nL 194.37475 46.482286 \r\nL 194.757568 46.394384 \r\nL 194.844393 46.335783 \r\nL 195.219318 46.247881 \r\nL 195.278516 46.159979 \r\nL 195.677121 46.072077 \r\nL 195.783678 46.013476 \r\nL 196.217802 45.925574 \r\nL 196.217802 45.896273 \r\nL 196.616406 45.808371 \r\nL 196.616406 45.779071 \r\nL 197.149194 45.691169 \r\nL 197.216286 45.632567 \r\nL 198.139785 45.544665 \r\nL 198.191091 45.486064 \r\nL 198.597588 45.398162 \r\nL 198.597588 45.368862 \r\nL 199.071177 45.28096 \r\nL 199.118536 45.222358 \r\nL 199.367171 45.134457 \r\nL 199.41453 45.075855 \r\nL 199.836813 44.987953 \r\nL 199.836813 44.958653 \r\nL 200.12886 44.870751 \r\nL 200.239364 44.81215 \r\nL 200.487999 44.724248 \r\nL 200.503785 44.636346 \r\nL 200.886603 44.548444 \r\nL 200.930015 44.431241 \r\nL 201.462803 44.343339 \r\nL 201.462803 44.284738 \r\nL 201.908766 44.196836 \r\nL 202.007431 44.021033 \r\nL 202.417875 43.933131 \r\nL 202.5047 43.815928 \r\nL 202.836212 43.728026 \r\nL 202.836212 43.698726 \r\nL 203.203244 43.610824 \r\nL 203.313748 43.493621 \r\nL 203.50713 43.405719 \r\nL 203.526863 43.347118 \r\nL 203.929414 43.259216 \r\nL 203.98072 43.200615 \r\nL 204.24514 43.112713 \r\nL 204.284606 43.054112 \r\nL 204.631905 42.96621 \r\nL 204.651638 42.907608 \r\nL 205.322556 42.819707 \r\nL 205.429113 42.673203 \r\nL 205.819825 42.585301 \r\nL 205.879023 42.468099 \r\nL 206.167123 42.380197 \r\nL 206.269734 42.204393 \r\nL 206.838042 42.116491 \r\nL 206.838042 42.087191 \r\nL 207.327417 41.999289 \r\nL 207.437921 41.882086 \r\nL 207.737861 41.794184 \r\nL 207.737861 41.764884 \r\nL 208.033855 41.676982 \r\nL 208.033855 41.647681 \r\nL 208.763971 41.559779 \r\nL 208.799491 41.471877 \r\nL 209.797975 41.383976 \r\nL 209.904532 41.266773 \r\nL 210.224205 41.178871 \r\nL 210.334709 41.090969 \r\nL 211.226636 41.003067 \r\nL 211.333193 40.944466 \r\nL 211.47527 40.856564 \r\nL 211.562095 40.768662 \r\nL 211.960699 40.68076 \r\nL 211.988325 40.592858 \r\nL 212.398769 40.504957 \r\nL 212.442181 40.446355 \r\nL 212.986809 40.358453 \r\nL 212.986809 40.329153 \r\nL 213.409093 40.241251 \r\nL 213.488024 40.153349 \r\nL 213.941881 40.065447 \r\nL 214.040545 40.006846 \r\nL 214.537814 39.918944 \r\nL 214.636479 39.831042 \r\nL 214.73909 39.74314 \r\nL 214.766716 39.684539 \r\nL 214.904846 39.596637 \r\nL 214.904846 39.567336 \r\nL 215.362649 39.479434 \r\nL 215.390275 39.362232 \r\nL 215.646802 39.27433 \r\nL 215.733627 39.157127 \r\nL 216.140124 39.069225 \r\nL 216.195377 39.010624 \r\nL 216.657126 38.922722 \r\nL 216.747897 38.83482 \r\nL 217.099143 38.746919 \r\nL 217.174128 38.688317 \r\nL 217.51748 38.600415 \r\nL 217.51748 38.571115 \r\nL 217.872672 38.483213 \r\nL 217.931871 38.395311 \r\nL 218.476498 38.307409 \r\nL 218.571216 38.248808 \r\nL 218.914568 38.160906 \r\nL 218.973767 38.043703 \r\nL 219.372371 37.955801 \r\nL 219.411837 37.8972 \r\nL 219.984091 37.809298 \r\nL 220.086702 37.750697 \r\nL 220.694475 37.662795 \r\nL 220.710261 37.604194 \r\nL 220.89575 37.516292 \r\nL 220.89575 37.486991 \r\nL 221.365393 37.399089 \r\nL 221.365393 37.369789 \r\nL 221.866608 37.281887 \r\nL 221.866608 37.252586 \r\nL 222.174441 37.164684 \r\nL 222.174441 37.135384 \r\nL 222.549366 37.047482 \r\nL 222.608565 36.988881 \r\nL 222.940077 36.900979 \r\nL 222.98349 36.813077 \r\nL 223.405773 36.725175 \r\nL 223.492598 36.607972 \r\nL 224.360845 36.52007 \r\nL 224.360845 36.49077 \r\nL 224.660785 36.402868 \r\nL 224.743663 36.314966 \r\nL 225.61191 36.227064 \r\nL 225.61191 36.197763 \r\nL 226.385439 36.109862 \r\nL 226.48805 36.02196 \r\nL 226.843242 35.934058 \r\nL 226.886655 35.875456 \r\nL 227.735169 35.787555 \r\nL 227.818047 35.728953 \r\nL 228.165346 35.641051 \r\nL 228.165346 35.611751 \r\nL 228.508698 35.553149 \r\nL 228.55211 35.465248 \r\nL 228.832317 35.377346 \r\nL 228.832317 35.348045 \r\nL 229.511129 35.260143 \r\nL 229.578221 35.172241 \r\nL 230.257032 35.084339 \r\nL 230.257032 35.055039 \r\nL 230.805606 34.967137 \r\nL 230.805606 34.937836 \r\nL 231.535723 34.879235 \r\nL 231.535723 34.820634 \r\nL 232.131656 34.732732 \r\nL 232.131656 34.703431 \r\nL 232.451329 34.615529 \r\nL 232.475008 34.556928 \r\nL 232.684177 34.469026 \r\nL 232.763108 34.381124 \r\nL 233.41824 34.293222 \r\nL 233.512958 34.234621 \r\nL 233.808951 34.146719 \r\nL 233.808951 34.117418 \r\nL 234.132571 34.029517 \r\nL 234.148357 33.970915 \r\nL 234.633786 33.883013 \r\nL 234.633786 33.824412 \r\nL 235.004764 33.73651 \r\nL 235.063963 33.590007 \r\nL 235.573072 33.502105 \r\nL 235.573072 33.472805 \r\nL 236.145325 33.384903 \r\nL 236.145325 33.355602 \r\nL 236.583395 33.2677 \r\nL 236.662327 33.209099 \r\nL 237.092504 33.121197 \r\nL 237.092504 33.091896 \r\nL 237.550307 33.003994 \r\nL 237.562147 32.945393 \r\nL 237.8463 32.857491 \r\nL 237.8463 32.828191 \r\nL 238.094935 32.740289 \r\nL 238.181759 32.681687 \r\nL 238.631669 32.593786 \r\nL 238.659295 32.535184 \r\nL 238.828998 32.447282 \r\nL 238.828998 32.417982 \r\nL 239.105258 32.33008 \r\nL 239.105258 32.300779 \r\nL 239.389412 32.212877 \r\nL 239.47229 32.124975 \r\nL 239.618313 32.037073 \r\nL 239.630153 31.978472 \r\nL 240.22214 31.89057 \r\nL 240.320804 31.831969 \r\nL 240.573385 31.744067 \r\nL 240.573385 31.714767 \r\nL 240.849646 31.626865 \r\nL 240.94831 31.509662 \r\nL 241.189051 31.42176 \r\nL 241.252196 31.333858 \r\nL 241.512671 31.245956 \r\nL 241.512671 31.216656 \r\nL 241.82445 31.128754 \r\nL 241.840236 31.070153 \r\nL 242.337505 30.982251 \r\nL 242.392757 30.865048 \r\nL 242.791362 30.777146 \r\nL 242.830827 30.718545 \r\nL 243.280737 30.630643 \r\nL 243.296524 30.572042 \r\nL 243.920083 30.48414 \r\nL 243.975335 30.425539 \r\nL 244.598894 30.337637 \r\nL 244.598894 30.308336 \r\nL 245.313225 30.220434 \r\nL 245.415836 30.132532 \r\nL 246.197258 30.04463 \r\nL 246.288029 29.986029 \r\nL 246.623488 29.898127 \r\nL 246.730046 29.810225 \r\nL 246.923428 29.722323 \r\nL 247.026039 29.663722 \r\nL 247.349659 29.57582 \r\nL 247.42859 29.517219 \r\nL 248.103455 29.429317 \r\nL 248.103455 29.400016 \r\nL 248.624403 29.312115 \r\nL 248.624403 29.282814 \r\nL 248.975648 29.194912 \r\nL 248.995381 29.136311 \r\nL 249.914934 29.048409 \r\nL 249.914934 29.019108 \r\nL 250.175408 28.960507 \r\nL 250.258286 28.872605 \r\nL 250.743715 28.784703 \r\nL 250.767395 28.696801 \r\nL 251.1068 28.608899 \r\nL 251.142319 28.550298 \r\nL 251.923742 28.462396 \r\nL 251.923742 28.433096 \r\nL 252.28288 28.345194 \r\nL 252.389438 28.257292 \r\nL 252.902493 28.16939 \r\nL 252.902493 28.140089 \r\nL 253.245845 28.052187 \r\nL 253.245845 28.022887 \r\nL 253.514212 27.934985 \r\nL 253.549732 27.876384 \r\nL 254.173291 27.788482 \r\nL 254.173291 27.759181 \r\nL 254.587681 27.671279 \r\nL 254.67056 27.612678 \r\nL 255.033645 27.524776 \r\nL 255.132309 27.436874 \r\nL 255.582219 27.348972 \r\nL 255.661151 27.26107 \r\nL 256.115007 27.173168 \r\nL 256.217618 27.085266 \r\nL 256.742513 26.997365 \r\nL 256.785925 26.938763 \r\nL 257.283194 26.850861 \r\nL 257.370019 26.79226 \r\nL 257.851501 26.704358 \r\nL 257.851501 26.675058 \r\nL 258.609244 26.587156 \r\nL 258.640816 26.528554 \r\nL 259.264376 26.440653 \r\nL 259.335414 26.352751 \r\nL 259.923454 26.264849 \r\nL 259.923454 26.235548 \r\nL 260.894312 26.147646 \r\nL 260.969297 26.089045 \r\nL 261.45078 26.001143 \r\nL 261.486299 25.88394 \r\nL 262.082232 25.796039 \r\nL 262.169057 25.737437 \r\nL 262.409798 25.649535 \r\nL 262.484783 25.532333 \r\nL 262.792616 25.444431 \r\nL 262.863654 25.356529 \r\nL 263.692436 25.268627 \r\nL 263.692436 25.239327 \r\nL 264.027895 25.151425 \r\nL 264.027895 25.122124 \r\nL 264.572522 25.034222 \r\nL 264.572522 25.004921 \r\nL 265.022432 24.91702 \r\nL 265.109257 24.829118 \r\nL 265.661778 24.741216 \r\nL 265.661778 24.711915 \r\nL 266.111687 24.624013 \r\nL 266.111687 24.594713 \r\nL 266.857591 24.506811 \r\nL 266.857591 24.47751 \r\nL 267.733731 24.389608 \r\nL 267.733731 24.360308 \r\nL 268.503313 24.272406 \r\nL 268.526993 24.213804 \r\nL 269.10714 24.125902 \r\nL 269.10714 24.096602 \r\nL 269.880669 24.0087 \r\nL 269.916188 23.950099 \r\nL 270.334525 23.862197 \r\nL 270.334525 23.832896 \r\nL 271.415887 23.744994 \r\nL 271.415887 23.715694 \r\nL 272.003927 23.627792 \r\nL 272.031554 23.56919 \r\nL 273.393123 23.481289 \r\nL 273.393123 23.451988 \r\nL 274.24953 23.364086 \r\nL 274.312675 23.305485 \r\nL 274.644188 23.217583 \r\nL 274.719173 23.10038 \r\nL 275.240121 23.012478 \r\nL 275.350625 22.924577 \r\nL 276.329376 22.836675 \r\nL 276.329376 22.807374 \r\nL 276.767446 22.719472 \r\nL 276.767446 22.690171 \r\nL 277.020027 22.60227 \r\nL 277.087119 22.543668 \r\nL 277.50151 22.455766 \r\nL 277.544922 22.397165 \r\nL 278.042191 22.309263 \r\nL 278.073764 22.192061 \r\nL 278.559193 22.104159 \r\nL 278.598658 22.045558 \r\nL 279.447172 21.957656 \r\nL 279.474799 21.899054 \r\nL 280.307526 21.811152 \r\nL 280.386458 21.752551 \r\nL 281.337583 21.664649 \r\nL 281.337583 21.635349 \r\nL 282.241349 21.547447 \r\nL 282.284762 21.459545 \r\nL 283.010932 21.371643 \r\nL 283.010932 21.342342 \r\nL 283.401643 21.25444 \r\nL 283.413483 21.195839 \r\nL 283.962057 21.107937 \r\nL 284.040989 21.020035 \r\nL 284.550097 20.932133 \r\nL 284.613242 20.844232 \r\nL 285.296 20.75633 \r\nL 285.363092 20.697728 \r\nL 286.199766 20.609826 \r\nL 286.199766 20.580526 \r\nL 287.000922 20.492624 \r\nL 287.000922 20.463323 \r\nL 287.802077 20.375421 \r\nL 287.829703 20.31682 \r\nL 288.12175 20.228918 \r\nL 288.145429 20.170317 \r\nL 289.384654 20.082415 \r\nL 289.43596 20.023814 \r\nL 290.963285 19.935912 \r\nL 291.034324 19.877311 \r\nL 291.503966 19.789409 \r\nL 291.503966 19.760108 \r\nL 292.427466 19.672206 \r\nL 292.427466 19.642906 \r\nL 292.81423 19.555004 \r\nL 292.81423 19.525703 \r\nL 293.481202 19.437801 \r\nL 293.481202 19.408501 \r\nL 294.558617 19.320599 \r\nL 294.558617 19.291298 \r\nL 295.119031 19.203396 \r\nL 295.16639 19.086194 \r\nL 296.04253 18.998292 \r\nL 296.121462 18.93969 \r\nL 297.198878 18.851788 \r\nL 297.305435 18.793187 \r\nL 298.288133 18.705285 \r\nL 298.311812 18.646684 \r\nL 299.164273 18.558782 \r\nL 299.255044 18.47088 \r\nL 301.476967 18.382978 \r\nL 301.476967 18.353678 \r\nL 302.222871 18.265776 \r\nL 302.222871 18.236475 \r\nL 303.008239 18.148573 \r\nL 303.031919 18.089972 \r\nL 303.975151 18.00207 \r\nL 304.006724 17.943469 \r\nL 304.646069 17.855567 \r\nL 304.646069 17.826266 \r\nL 305.407759 17.738364 \r\nL 305.514316 17.679763 \r\nL 305.782683 17.591861 \r\nL 305.782683 17.562561 \r\nL 306.102356 17.474659 \r\nL 306.102356 17.445358 \r\nL 307.061374 17.357456 \r\nL 307.061374 17.328156 \r\nL 307.594162 17.240254 \r\nL 307.594162 17.210953 \r\nL 308.040126 17.123051 \r\nL 308.040126 17.09375 \r\nL 308.853121 17.005849 \r\nL 308.951785 16.947247 \r\nL 309.30303 16.888646 \r\nL 309.30303 16.830045 \r\nL 309.887124 16.742143 \r\nL 309.993681 16.683542 \r\nL 311.055311 16.59564 \r\nL 311.07899 16.537038 \r\nL 312.014329 16.449137 \r\nL 312.014329 16.419836 \r\nL 312.310322 16.331934 \r\nL 312.40504 16.273333 \r\nL 313.73109 16.185431 \r\nL 313.73109 16.15613 \r\nL 314.386222 16.068228 \r\nL 314.496726 15.980326 \r\nL 315.637287 15.892425 \r\nL 315.720165 15.804523 \r\nL 317.930249 15.716621 \r\nL 317.930249 15.68732 \r\nL 319.453627 15.628719 \r\nL 319.453627 15.570118 \r\nL 320.302142 15.482216 \r\nL 320.302142 15.452915 \r\nL 321.032258 15.365013 \r\nL 321.032258 15.335712 \r\nL 322.058369 15.247811 \r\nL 322.121514 15.189209 \r\nL 322.626676 15.101307 \r\nL 322.626676 15.072007 \r\nL 324.540766 14.984105 \r\nL 324.540766 14.954804 \r\nL 325.326135 14.866902 \r\nL 325.326135 14.837602 \r\nL 326.624559 14.7497 \r\nL 326.636398 14.691099 \r\nL 328.976719 14.603197 \r\nL 328.976719 14.573896 \r\nL 330.247517 14.485994 \r\nL 330.350128 14.427393 \r\nL 331.490689 14.339491 \r\nL 331.490689 14.31019 \r\nL 332.433921 14.222288 \r\nL 332.433921 14.192988 \r\nL 333.136411 14.105086 \r\nL 333.136411 14.075785 \r\nL 334.213827 13.987883 \r\nL 334.249346 13.929282 \r\nL 335.472785 13.84138 \r\nL 335.472785 13.81208 \r\nL 336.58572 13.724178 \r\nL 336.696224 13.665576 \r\nL 337.521059 13.577674 \r\nL 337.521059 13.548374 \r\nL 338.416932 13.460472 \r\nL 338.416932 13.431171 \r\nL 339.632478 13.343269 \r\nL 339.742982 13.255368 \r\nL 341.041406 13.167466 \r\nL 341.041406 13.138165 \r\nL 341.732057 13.050263 \r\nL 341.732057 13.020962 \r\nL 343.36594 12.933061 \r\nL 343.401459 12.874459 \r\nL 344.885372 12.786557 \r\nL 344.885372 12.757257 \r\nL 346.065399 12.669355 \r\nL 346.065399 12.640054 \r\nL 347.734801 12.552152 \r\nL 347.734801 12.522852 \r\nL 349.068744 12.43495 \r\nL 349.127943 12.376349 \r\nL 350.698681 12.288447 \r\nL 350.730253 12.229845 \r\nL 352.802206 12.141943 \r\nL 352.802206 12.112643 \r\nL 354.404517 12.024741 \r\nL 354.42425 11.96614 \r\nL 355.817392 11.878238 \r\nL 355.817392 11.848937 \r\nL 359.12857 11.761035 \r\nL 359.12857 11.731735 \r\nL 361.208416 11.643833 \r\nL 361.208416 11.614532 \r\nL 362.759421 11.52663 \r\nL 362.759421 11.49733 \r\nL 363.880249 11.438728 \r\nL 363.880249 11.380127 \r\nL 366.49683 11.292225 \r\nL 366.49683 11.262924 \r\nL 371.595808 11.175023 \r\nL 371.595808 11.145722 \r\nL 374.737284 11.05782 \r\nL 374.737284 11.028519 \r\nL 378.58125 10.999219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p14b7ad9762)\" d=\"M 43.78125 228.439219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 43.78125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 228.439219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 378.58125 228.439219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 10.999219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 50.78125 48.355469 \r\nL 124.178125 48.355469 \r\nQ 126.178125 48.355469 126.178125 46.355469 \r\nL 126.178125 17.999219 \r\nQ 126.178125 15.999219 124.178125 15.999219 \r\nL 50.78125 15.999219 \r\nQ 48.78125 15.999219 48.78125 17.999219 \r\nL 48.78125 46.355469 \r\nQ 48.78125 48.355469 50.78125 48.355469 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 52.78125 24.097656 \r\nL 72.78125 24.097656 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_19\">\r\n     <!-- ROC -->\r\n     <g transform=\"translate(80.78125 27.597656)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 39.40625 66.21875 \r\nQ 28.65625 66.21875 22.328125 58.203125 \r\nQ 16.015625 50.203125 16.015625 36.375 \r\nQ 16.015625 22.609375 22.328125 14.59375 \r\nQ 28.65625 6.59375 39.40625 6.59375 \r\nQ 50.140625 6.59375 56.421875 14.59375 \r\nQ 62.703125 22.609375 62.703125 36.375 \r\nQ 62.703125 50.203125 56.421875 58.203125 \r\nQ 50.140625 66.21875 39.40625 66.21875 \r\nz\r\nM 39.40625 74.21875 \r\nQ 54.734375 74.21875 63.90625 63.9375 \r\nQ 73.09375 53.65625 73.09375 36.375 \r\nQ 73.09375 19.140625 63.90625 8.859375 \r\nQ 54.734375 -1.421875 39.40625 -1.421875 \r\nQ 24.03125 -1.421875 14.8125 8.828125 \r\nQ 5.609375 19.09375 5.609375 36.375 \r\nQ 5.609375 53.65625 14.8125 63.9375 \r\nQ 24.03125 74.21875 39.40625 74.21875 \r\nz\r\n\" id=\"DejaVuSans-79\"/>\r\n       <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-79\"/>\r\n      <use x=\"148.193359\" xlink:href=\"#DejaVuSans-67\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_21\">\r\n     <path d=\"M 52.78125 38.775781 \r\nL 72.78125 38.775781 \r\n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_22\"/>\r\n    <g id=\"text_20\">\r\n     <!-- Random -->\r\n     <g transform=\"translate(80.78125 42.275781)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"67.232422\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"128.511719\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"191.890625\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"255.367188\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"316.548828\" xlink:href=\"#DejaVuSans-109\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p14b7ad9762\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMaRlO1LCQbn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}