{
  
    
        "post0": {
            "title": "TEST",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gwonchankim.github.io/channee/ml/jupyter/2021/08/31/%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4.html",
            "relUrl": "/ml/jupyter/2021/08/31/%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4.html",
            "date": " • Aug 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Project2-4",
            "content": "&#44032;&#44277;&#46108; &#52572;&#51333; &#45936;&#51060;&#53552; &#49464;&#53944; &#49373;&#49457; . &#51648;&#44552;&#44620;&#51648; &#44284;&#51221;&#51012; &#54632;&#49688;&#54868; . &#51060;&#51204;&#50640; application &#45936;&#51060;&#53552; &#49464;&#53944;&#51032; feature engineering &#49688;&#54665; &#54980; &#49352;&#47213;&#44172; previous &#45936;&#51060;&#53552; &#49464;&#53944;&#47196; &#44032;&#44277;&#46108; &#45936;&#51060;&#53552;&#47484; &#51312;&#51064;. . import numpy as np import pandas as pd import gc import time import matplotlib.pyplot as plt import seaborn as sns import os %matplotlib inline pd.set_option(&#39;display.max_rows&#39;, 100) pd.set_option(&#39;display.max_columns&#39;, 200) . def get_dataset(): app_train = pd.read_csv(&#39;application_train.csv&#39;) app_test = pd.read_csv(&#39;application_test.csv&#39;) apps = pd.concat([app_train, app_test]) prev = pd.read_csv(&#39;previous_application.csv&#39;) return apps, prev apps, prev = get_dataset() . &#51060;&#51204; application &#45936;&#51060;&#53552;&#51032; feature engineering &#54632;&#49688; &#48373;&#49324; . def get_apps_processed(apps): # EXT_SOURCE_X FEATURE 가공 apps[&#39;APPS_EXT_SOURCE_MEAN&#39;] = apps[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].mean(axis=1) apps[&#39;APPS_EXT_SOURCE_STD&#39;] = apps[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].std(axis=1) apps[&#39;APPS_EXT_SOURCE_STD&#39;] = apps[&#39;APPS_EXT_SOURCE_STD&#39;].fillna(apps[&#39;APPS_EXT_SOURCE_STD&#39;].mean()) # AMT_CREDIT 비율로 Feature 가공 apps[&#39;APPS_ANNUITY_CREDIT_RATIO&#39;] = apps[&#39;AMT_ANNUITY&#39;]/apps[&#39;AMT_CREDIT&#39;] apps[&#39;APPS_GOODS_CREDIT_RATIO&#39;] = apps[&#39;AMT_GOODS_PRICE&#39;]/apps[&#39;AMT_CREDIT&#39;] # AMT_INCOME_TOTAL 비율로 Feature 가공 apps[&#39;APPS_ANNUITY_INCOME_RATIO&#39;] = apps[&#39;AMT_ANNUITY&#39;]/apps[&#39;AMT_INCOME_TOTAL&#39;] apps[&#39;APPS_CREDIT_INCOME_RATIO&#39;] = apps[&#39;AMT_CREDIT&#39;]/apps[&#39;AMT_INCOME_TOTAL&#39;] apps[&#39;APPS_GOODS_INCOME_RATIO&#39;] = apps[&#39;AMT_GOODS_PRICE&#39;]/apps[&#39;AMT_INCOME_TOTAL&#39;] apps[&#39;APPS_CNT_FAM_INCOME_RATIO&#39;] = apps[&#39;AMT_INCOME_TOTAL&#39;]/apps[&#39;CNT_FAM_MEMBERS&#39;] # DAYS_BIRTH, DAYS_EMPLOYED 비율로 Feature 가공 apps[&#39;APPS_EMPLOYED_BIRTH_RATIO&#39;] = apps[&#39;DAYS_EMPLOYED&#39;]/apps[&#39;DAYS_BIRTH&#39;] apps[&#39;APPS_INCOME_EMPLOYED_RATIO&#39;] = apps[&#39;AMT_INCOME_TOTAL&#39;]/apps[&#39;DAYS_EMPLOYED&#39;] apps[&#39;APPS_INCOME_BIRTH_RATIO&#39;] = apps[&#39;AMT_INCOME_TOTAL&#39;]/apps[&#39;DAYS_BIRTH&#39;] apps[&#39;APPS_CAR_BIRTH_RATIO&#39;] = apps[&#39;OWN_CAR_AGE&#39;] / apps[&#39;DAYS_BIRTH&#39;] apps[&#39;APPS_CAR_EMPLOYED_RATIO&#39;] = apps[&#39;OWN_CAR_AGE&#39;] / apps[&#39;DAYS_EMPLOYED&#39;] return apps . previous &#45936;&#51060;&#53552; &#44032;&#44277;&#54980; &#51064;&#53076;&#46377; &#48143; &#52572;&#51333; &#45936;&#51060;&#53552; &#51665;&#54633; &#49373;&#49457;&#54616;&#45716; &#54632;&#49688; &#49440;&#50616; . from sklearn.model_selection import train_test_split from lightgbm import LGBMClassifier def get_prev_processed(prev): # 대출 신청 금액과 실제 대출액/대출 상품금액 차이 및 비율 prev[&#39;PREV_CREDIT_DIFF&#39;] = prev[&#39;AMT_APPLICATION&#39;] - prev[&#39;AMT_CREDIT&#39;] prev[&#39;PREV_GOODS_DIFF&#39;] = prev[&#39;AMT_APPLICATION&#39;] - prev[&#39;AMT_GOODS_PRICE&#39;] prev[&#39;PREV_CREDIT_APPL_RATIO&#39;] = prev[&#39;AMT_CREDIT&#39;]/prev[&#39;AMT_APPLICATION&#39;] # prev[&#39;PREV_ANNUITY_APPL_RATIO&#39;] = prev[&#39;AMT_ANNUITY&#39;]/prev[&#39;AMT_APPLICATION&#39;] prev[&#39;PREV_GOODS_APPL_RATIO&#39;] = prev[&#39;AMT_GOODS_PRICE&#39;]/prev[&#39;AMT_APPLICATION&#39;] prev[&#39;DAYS_FIRST_DRAWING&#39;].replace(365243, np.nan, inplace= True) prev[&#39;DAYS_FIRST_DUE&#39;].replace(365243, np.nan, inplace= True) prev[&#39;DAYS_LAST_DUE_1ST_VERSION&#39;].replace(365243, np.nan, inplace= True) prev[&#39;DAYS_LAST_DUE&#39;].replace(365243, np.nan, inplace= True) prev[&#39;DAYS_TERMINATION&#39;].replace(365243, np.nan, inplace= True) # 첫번째 만기일과 마지막 만기일까지의 기간 prev[&#39;PREV_DAYS_LAST_DUE_DIFF&#39;] = prev[&#39;DAYS_LAST_DUE_1ST_VERSION&#39;] - prev[&#39;DAYS_LAST_DUE&#39;] # 매월 납부 금액과 납부 횟수 곱해서 전체 납부 금액 구함. all_pay = prev[&#39;AMT_ANNUITY&#39;] * prev[&#39;CNT_PAYMENT&#39;] # 전체 납부 금액 대비 AMT_CREDIT 비율을 구하고 여기에 다시 납부횟수로 나누어서 이자율 계산. prev[&#39;PREV_INTERESTS_RATE&#39;] = (all_pay/prev[&#39;AMT_CREDIT&#39;] - 1)/prev[&#39;CNT_PAYMENT&#39;] return prev def get_prev_amt_agg(prev): # 새롭게 생성된 대출 신청액 대비 다른 금액 차이 및 비율로 aggregation 수행. agg_dict = { # 기존 컬럼. &#39;SK_ID_CURR&#39;:[&#39;count&#39;], &#39;AMT_CREDIT&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_ANNUITY&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_APPLICATION&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_DOWN_PAYMENT&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_GOODS_PRICE&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;RATE_DOWN_PAYMENT&#39;: [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;], &#39;DAYS_DECISION&#39;: [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;], &#39;CNT_PAYMENT&#39;: [&#39;mean&#39;, &#39;sum&#39;], # 가공 컬럼 &#39;PREV_CREDIT_DIFF&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;PREV_CREDIT_APPL_RATIO&#39;:[&#39;mean&#39;, &#39;max&#39;], &#39;PREV_GOODS_DIFF&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;PREV_GOODS_APPL_RATIO&#39;:[&#39;mean&#39;, &#39;max&#39;], &#39;PREV_DAYS_LAST_DUE_DIFF&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;PREV_INTERESTS_RATE&#39;:[&#39;mean&#39;, &#39;max&#39;] } prev_group = prev.groupby(&#39;SK_ID_CURR&#39;) prev_amt_agg = prev_group.agg(agg_dict) # multi index 컬럼을 &#39;_&#39;로 연결하여 컬럼명 변경 prev_amt_agg.columns = [&quot;PREV_&quot;+ &quot;_&quot;.join(x).upper() for x in prev_amt_agg.columns.ravel()] return prev_amt_agg def get_prev_refused_appr_agg(prev): # 원래 groupby 컬럼 + 세부 기준 컬럼으로 groupby 수행. 세분화된 레벨로 aggregation 수행 한 뒤에 unstack()으로 컬럼레벨로 변형. prev_refused_appr_group = prev[prev[&#39;NAME_CONTRACT_STATUS&#39;].isin([&#39;Approved&#39;, &#39;Refused&#39;])].groupby([ &#39;SK_ID_CURR&#39;, &#39;NAME_CONTRACT_STATUS&#39;]) prev_refused_appr_agg = prev_refused_appr_group[&#39;SK_ID_CURR&#39;].count().unstack() # 컬럼명 변경. prev_refused_appr_agg.columns = [&#39;PREV_APPROVED_COUNT&#39;, &#39;PREV_REFUSED_COUNT&#39; ] # NaN값은 모두 0으로 변경. prev_refused_appr_agg = prev_refused_appr_agg.fillna(0) return prev_refused_appr_agg def get_prev_agg(prev): prev = get_prev_processed(prev) prev_amt_agg = get_prev_amt_agg(prev) prev_refused_appr_agg = get_prev_refused_appr_agg(prev) # prev_amt_agg와 조인. prev_agg = prev_amt_agg.merge(prev_refused_appr_agg, on=&#39;SK_ID_CURR&#39;, how=&#39;left&#39;) # SK_ID_CURR별 과거 대출건수 대비 APPROVED_COUNT 및 REFUSED_COUNT 비율 생성. prev_agg[&#39;PREV_REFUSED_RATIO&#39;] = prev_agg[&#39;PREV_REFUSED_COUNT&#39;]/prev_agg[&#39;PREV_SK_ID_CURR_COUNT&#39;] prev_agg[&#39;PREV_APPROVED_RATIO&#39;] = prev_agg[&#39;PREV_APPROVED_COUNT&#39;]/prev_agg[&#39;PREV_SK_ID_CURR_COUNT&#39;] # &#39;PREV_REFUSED_COUNT&#39;, &#39;PREV_APPROVED_COUNT&#39; 컬럼 drop prev_agg = prev_agg.drop([&#39;PREV_REFUSED_COUNT&#39;, &#39;PREV_APPROVED_COUNT&#39;], axis=1) return prev_agg def get_apps_all_with_prev_agg(apps, prev): apps_all = get_apps_processed(apps) prev_agg = get_prev_agg(prev) print(&#39;prev_agg shape:&#39;, prev_agg.shape) print(&#39;apps_all before merge shape:&#39;, apps_all.shape) apps_all = apps_all.merge(prev_agg, on=&#39;SK_ID_CURR&#39;, how=&#39;left&#39;) print(&#39;apps_all after merge with prev_agg shape:&#39;, apps_all.shape) return apps_all def get_apps_all_encoded(apps_all): object_columns = apps_all.dtypes[apps_all.dtypes == &#39;object&#39;].index.tolist() for column in object_columns: apps_all[column] = pd.factorize(apps_all[column])[0] return apps_all def get_apps_all_train_test(apps_all): apps_all_train = apps_all[~apps_all[&#39;TARGET&#39;].isnull()] apps_all_test = apps_all[apps_all[&#39;TARGET&#39;].isnull()] apps_all_test = apps_all_test.drop(&#39;TARGET&#39;, axis=1) return apps_all_train, apps_all_test def train_apps_all(apps_all_train): ftr_app = apps_all_train.drop([&#39;SK_ID_CURR&#39;, &#39;TARGET&#39;], axis=1) target_app = apps_all_train[&#39;TARGET&#39;] train_x, valid_x, train_y, valid_y = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020) print(&#39;train shape:&#39;, train_x.shape, &#39;valid shape:&#39;, valid_x.shape) clf = LGBMClassifier( nthread=4, n_estimators=2000, learning_rate=0.01, num_leaves=32, colsample_bytree=0.8, subsample=0.8, max_depth=8, reg_alpha=0.04, reg_lambda=0.07, min_child_weight=40, silent=-1, verbose=-1, ) clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= &#39;auc&#39;, verbose= 100, early_stopping_rounds= 100) return clf . &#52572;&#51333; &#51665;&#54633; &#49373;&#49457; &#48143; &#51064;&#53076;&#46377;, &#54617;&#49845;/&#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#48516;&#47532;, &#54617;&#49845;/&#44160;&#51613; &#54588;&#52376;&#50752; &#53440;&#44191; &#45936;&#51060;&#53552; &#48516;&#47532; . apps_all = get_apps_all_with_prev_agg(apps, prev) apps_all = get_apps_all_encoded(apps_all) apps_all_train, apps_all_test = get_apps_all_train_test(apps_all) ftr_app = apps_all_train.drop([&#39;SK_ID_CURR&#39;, &#39;TARGET&#39;], axis=1) target_app = apps_all_train[&#39;TARGET&#39;] X_train, X_test, y_train, y_test = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020) . &lt;ipython-input-4-cfb8ca96aa23&gt;:53: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self. prev_amt_agg.columns = [&#34;PREV_&#34;+ &#34;_&#34;.join(x).upper() for x in prev_amt_agg.columns.ravel()] . prev_agg shape: (338857, 41) apps_all before merge shape: (356255, 135) apps_all after merge with prev_agg shape: (356255, 176) . from bayes_opt import BayesianOptimization from sklearn.metrics import roc_auc_score from lightgbm import LGBMClassifier . 필요한 부분을 최대한 처리하여 Pipeline은 필요 없을 듯 하다. | randomforest와 LightGBM, XGboost를 수행해보고자 한다 | . !pip install bayesian-optimization . Requirement already satisfied: bayesian-optimization in c: users channee anaconda3 lib site-packages (1.2.0) Requirement already satisfied: scikit-learn&gt;=0.18.0 in c: users channee anaconda3 lib site-packages (from bayesian-optimization) (0.24.1) Requirement already satisfied: numpy&gt;=1.9.0 in c: users channee anaconda3 lib site-packages (from bayesian-optimization) (1.20.1) Requirement already satisfied: scipy&gt;=0.14.0 in c: users channee anaconda3 lib site-packages (from bayesian-optimization) (1.6.2) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in c: users channee anaconda3 lib site-packages (from scikit-learn&gt;=0.18.0-&gt;bayesian-optimization) (2.1.0) Requirement already satisfied: joblib&gt;=0.11 in c: users channee anaconda3 lib site-packages (from scikit-learn&gt;=0.18.0-&gt;bayesian-optimization) (1.0.1) . from bayes_opt import BayesianOptimization from sklearn.metrics import roc_auc_score from lightgbm import LGBMClassifier . bayesian_params = { &#39;max_depth&#39;: (6, 16), &#39;num_leaves&#39;: (24, 64), &#39;min_child_samples&#39;: (10, 200), &#39;min_child_weight&#39;:(1, 50), &#39;subsample&#39;:(0.5, 1.0), &#39;colsample_bytree&#39;: (0.5, 1.0), &#39;max_bin&#39;:(10, 500), &#39;reg_lambda&#39;:(0.001, 10), &#39;reg_alpha&#39;: (0.01, 50) } . def lgb_roc_eval(max_depth, num_leaves, min_child_samples, min_child_weight, subsample, colsample_bytree,max_bin, reg_lambda, reg_alpha): params = { &quot;n_estimators&quot;:500, &quot;learning_rate&quot;:0.02, &#39;max_depth&#39;: int(round(max_depth)), # 호출 시 실수형 값이 들어오므로 정수형 하이퍼 파라미터는 정수형으로 변경 &#39;num_leaves&#39;: int(round(num_leaves)), &#39;min_child_samples&#39;: int(round(min_child_samples)), &#39;min_child_weight&#39;: int(round(min_child_weight)), &#39;subsample&#39;: max(min(subsample, 1), 0), &#39;colsample_bytree&#39;: max(min(colsample_bytree, 1), 0), &#39;max_bin&#39;: max(int(round(max_bin)),10), &#39;reg_lambda&#39;: max(reg_lambda,0), &#39;reg_alpha&#39;: max(reg_alpha, 0) } lgb_model = LGBMClassifier(**params) lgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric= &#39;auc&#39;, verbose= 100, early_stopping_rounds= 100) proba = lgb_model.predict_proba(X_test)[:, 1] roc_auc = roc_auc_score(valid_y, valid_proba) return roc_auc . lgbBO = BayesianOptimization(lgb_roc_eval,bayesian_params , random_state=0) # 함수 반환값이 최대가 되는 입력값 유추를 위한 iteration 수행. lgbBO.maximize(init_points=5, n_iter=25) . | iter | target | colsam... | max_bin | max_depth | min_ch... | min_ch... | num_le... | reg_alpha | reg_la... | subsample | - Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.769662 training&#39;s binary_logloss: 0.246032 valid_1&#39;s auc: 0.755483 valid_1&#39;s binary_logloss: 0.248917 [200] training&#39;s auc: 0.787253 training&#39;s binary_logloss: 0.238487 valid_1&#39;s auc: 0.766224 valid_1&#39;s binary_logloss: 0.244243 [300] training&#39;s auc: 0.798898 training&#39;s binary_logloss: 0.23398 valid_1&#39;s auc: 0.771467 valid_1&#39;s binary_logloss: 0.242344 [400] training&#39;s auc: 0.807869 training&#39;s binary_logloss: 0.23065 valid_1&#39;s auc: 0.773972 valid_1&#39;s binary_logloss: 0.241458 [500] training&#39;s auc: 0.815952 training&#39;s binary_logloss: 0.227669 valid_1&#39;s auc: 0.775806 valid_1&#39;s binary_logloss: 0.240831 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.815952 training&#39;s binary_logloss: 0.227669 valid_1&#39;s auc: 0.775806 valid_1&#39;s binary_logloss: 0.240831 | 1 | 0.7758 | 0.7744 | 360.4 | 12.03 | 113.5 | 21.76 | 49.84 | 21.88 | 8.918 | 0.9818 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.76256 training&#39;s binary_logloss: 0.247457 valid_1&#39;s auc: 0.7537 valid_1&#39;s binary_logloss: 0.249099 [200] training&#39;s auc: 0.780205 training&#39;s binary_logloss: 0.24047 valid_1&#39;s auc: 0.765756 valid_1&#39;s binary_logloss: 0.244226 [300] training&#39;s auc: 0.79096 training&#39;s binary_logloss: 0.236494 valid_1&#39;s auc: 0.771277 valid_1&#39;s binary_logloss: 0.242257 [400] training&#39;s auc: 0.799189 training&#39;s binary_logloss: 0.233539 valid_1&#39;s auc: 0.774083 valid_1&#39;s binary_logloss: 0.241244 [500] training&#39;s auc: 0.806102 training&#39;s binary_logloss: 0.231049 valid_1&#39;s auc: 0.775791 valid_1&#39;s binary_logloss: 0.240642 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.806102 training&#39;s binary_logloss: 0.231049 valid_1&#39;s auc: 0.775791 valid_1&#39;s binary_logloss: 0.240642 | 2 | 0.7758 | 0.6917 | 397.9 | 11.29 | 117.9 | 46.35 | 26.84 | 4.366 | 0.2032 | 0.9163 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.775644 training&#39;s binary_logloss: 0.243872 valid_1&#39;s auc: 0.757262 valid_1&#39;s binary_logloss: 0.247984 [200] training&#39;s auc: 0.796903 training&#39;s binary_logloss: 0.235046 valid_1&#39;s auc: 0.768757 valid_1&#39;s binary_logloss: 0.243207 [300] training&#39;s auc: 0.812041 training&#39;s binary_logloss: 0.229261 valid_1&#39;s auc: 0.773366 valid_1&#39;s binary_logloss: 0.24156 [400] training&#39;s auc: 0.824984 training&#39;s binary_logloss: 0.224423 valid_1&#39;s auc: 0.776251 valid_1&#39;s binary_logloss: 0.240565 [500] training&#39;s auc: 0.835704 training&#39;s binary_logloss: 0.220372 valid_1&#39;s auc: 0.777178 valid_1&#39;s binary_logloss: 0.24019 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.835704 training&#39;s binary_logloss: 0.220372 valid_1&#39;s auc: 0.777178 valid_1&#39;s binary_logloss: 0.24019 | 3 | 0.7772 | 0.8891 | 436.3 | 15.79 | 161.8 | 23.61 | 55.22 | 5.923 | 6.4 | 0.5717 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.765842 training&#39;s binary_logloss: 0.246884 valid_1&#39;s auc: 0.753808 valid_1&#39;s binary_logloss: 0.249258 [200] training&#39;s auc: 0.783137 training&#39;s binary_logloss: 0.239739 valid_1&#39;s auc: 0.765526 valid_1&#39;s binary_logloss: 0.244489 [300] training&#39;s auc: 0.793648 training&#39;s binary_logloss: 0.23577 valid_1&#39;s auc: 0.770579 valid_1&#39;s binary_logloss: 0.242643 [400] training&#39;s auc: 0.801582 training&#39;s binary_logloss: 0.232812 valid_1&#39;s auc: 0.773253 valid_1&#39;s binary_logloss: 0.241685 [500] training&#39;s auc: 0.808241 training&#39;s binary_logloss: 0.23036 valid_1&#39;s auc: 0.774833 valid_1&#39;s binary_logloss: 0.241128 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.808241 training&#39;s binary_logloss: 0.23036 valid_1&#39;s auc: 0.774833 valid_1&#39;s binary_logloss: 0.241128 | 4 | 0.7748 | 0.9723 | 265.7 | 10.15 | 60.27 | 38.94 | 42.25 | 28.43 | 0.1889 | 0.8088 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.7659 training&#39;s binary_logloss: 0.247276 valid_1&#39;s auc: 0.753968 valid_1&#39;s binary_logloss: 0.24956 [200] training&#39;s auc: 0.781741 training&#39;s binary_logloss: 0.240327 valid_1&#39;s auc: 0.764668 valid_1&#39;s binary_logloss: 0.244831 [300] training&#39;s auc: 0.791758 training&#39;s binary_logloss: 0.236503 valid_1&#39;s auc: 0.769722 valid_1&#39;s binary_logloss: 0.242955 [400] training&#39;s auc: 0.799313 training&#39;s binary_logloss: 0.233722 valid_1&#39;s auc: 0.772473 valid_1&#39;s binary_logloss: 0.241976 [500] training&#39;s auc: 0.805605 training&#39;s binary_logloss: 0.231421 valid_1&#39;s auc: 0.774035 valid_1&#39;s binary_logloss: 0.241428 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.805605 training&#39;s binary_logloss: 0.231421 valid_1&#39;s auc: 0.774035 valid_1&#39;s binary_logloss: 0.241428 | 5 | 0.774 | 0.806 | 312.3 | 15.44 | 139.5 | 18.62 | 41.48 | 34.88 | 0.6032 | 0.8334 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.778416 training&#39;s binary_logloss: 0.243866 valid_1&#39;s auc: 0.758906 valid_1&#39;s binary_logloss: 0.248045 [200] training&#39;s auc: 0.797547 training&#39;s binary_logloss: 0.235078 valid_1&#39;s auc: 0.768769 valid_1&#39;s binary_logloss: 0.243239 [300] training&#39;s auc: 0.812388 training&#39;s binary_logloss: 0.229334 valid_1&#39;s auc: 0.773393 valid_1&#39;s binary_logloss: 0.241516 [400] training&#39;s auc: 0.824856 training&#39;s binary_logloss: 0.224602 valid_1&#39;s auc: 0.77606 valid_1&#39;s binary_logloss: 0.240581 [500] training&#39;s auc: 0.835757 training&#39;s binary_logloss: 0.220492 valid_1&#39;s auc: 0.777391 valid_1&#39;s binary_logloss: 0.240112 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.835757 training&#39;s binary_logloss: 0.220492 valid_1&#39;s auc: 0.777391 valid_1&#39;s binary_logloss: 0.240112 | 6 | 0.7774 | 0.6405 | 435.0 | 13.5 | 169.3 | 26.92 | 57.69 | 5.768 | 9.196 | 0.613 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.772281 training&#39;s binary_logloss: 0.246014 valid_1&#39;s auc: 0.756518 valid_1&#39;s binary_logloss: 0.249054 [200] training&#39;s auc: 0.789124 training&#39;s binary_logloss: 0.238224 valid_1&#39;s auc: 0.766602 valid_1&#39;s binary_logloss: 0.244218 [300] training&#39;s auc: 0.800463 training&#39;s binary_logloss: 0.233735 valid_1&#39;s auc: 0.771115 valid_1&#39;s binary_logloss: 0.242475 [400] training&#39;s auc: 0.809924 training&#39;s binary_logloss: 0.230144 valid_1&#39;s auc: 0.774036 valid_1&#39;s binary_logloss: 0.241453 [500] training&#39;s auc: 0.81804 training&#39;s binary_logloss: 0.227105 valid_1&#39;s auc: 0.775536 valid_1&#39;s binary_logloss: 0.240903 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.81804 training&#39;s binary_logloss: 0.227105 valid_1&#39;s auc: 0.775536 valid_1&#39;s binary_logloss: 0.240903 | 7 | 0.7755 | 0.6778 | 477.2 | 11.75 | 194.9 | 46.85 | 59.47 | 26.32 | 8.388 | 0.7753 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.777583 training&#39;s binary_logloss: 0.243725 valid_1&#39;s auc: 0.758288 valid_1&#39;s binary_logloss: 0.247968 [200] training&#39;s auc: 0.797949 training&#39;s binary_logloss: 0.234844 valid_1&#39;s auc: 0.768779 valid_1&#39;s binary_logloss: 0.24323 [300] training&#39;s auc: 0.812857 training&#39;s binary_logloss: 0.229109 valid_1&#39;s auc: 0.773238 valid_1&#39;s binary_logloss: 0.241582 [400] training&#39;s auc: 0.825851 training&#39;s binary_logloss: 0.22425 valid_1&#39;s auc: 0.776055 valid_1&#39;s binary_logloss: 0.240619 [500] training&#39;s auc: 0.836858 training&#39;s binary_logloss: 0.220087 valid_1&#39;s auc: 0.777214 valid_1&#39;s binary_logloss: 0.240157 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.836858 training&#39;s binary_logloss: 0.220087 valid_1&#39;s auc: 0.777214 valid_1&#39;s binary_logloss: 0.240157 | 8 | 0.7772 | 0.7374 | 408.1 | 12.84 | 197.6 | 5.693 | 57.73 | 6.171 | 8.007 | 0.6427 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.776884 training&#39;s binary_logloss: 0.244363 valid_1&#39;s auc: 0.757931 valid_1&#39;s binary_logloss: 0.248329 [200] training&#39;s auc: 0.795476 training&#39;s binary_logloss: 0.235896 valid_1&#39;s auc: 0.767942 valid_1&#39;s binary_logloss: 0.243577 [300] training&#39;s auc: 0.809275 training&#39;s binary_logloss: 0.230547 valid_1&#39;s auc: 0.77254 valid_1&#39;s binary_logloss: 0.241869 [400] training&#39;s auc: 0.82066 training&#39;s binary_logloss: 0.226218 valid_1&#39;s auc: 0.774854 valid_1&#39;s binary_logloss: 0.241044 [500] training&#39;s auc: 0.830938 training&#39;s binary_logloss: 0.222269 valid_1&#39;s auc: 0.776395 valid_1&#39;s binary_logloss: 0.240484 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.830938 training&#39;s binary_logloss: 0.222269 valid_1&#39;s auc: 0.776395 valid_1&#39;s binary_logloss: 0.240484 | 9 | 0.7764 | 0.7749 | 388.9 | 14.3 | 193.9 | 49.91 | 62.85 | 13.15 | 9.906 | 0.6812 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.764203 training&#39;s binary_logloss: 0.247826 valid_1&#39;s auc: 0.752311 valid_1&#39;s binary_logloss: 0.249919 [200] training&#39;s auc: 0.779974 training&#39;s binary_logloss: 0.241103 valid_1&#39;s auc: 0.763794 valid_1&#39;s binary_logloss: 0.245126 [300] training&#39;s auc: 0.789033 training&#39;s binary_logloss: 0.237594 valid_1&#39;s auc: 0.768866 valid_1&#39;s binary_logloss: 0.243267 [400] training&#39;s auc: 0.795627 training&#39;s binary_logloss: 0.235115 valid_1&#39;s auc: 0.771779 valid_1&#39;s binary_logloss: 0.242221 [500] training&#39;s auc: 0.800884 training&#39;s binary_logloss: 0.233158 valid_1&#39;s auc: 0.773414 valid_1&#39;s binary_logloss: 0.241632 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.800884 training&#39;s binary_logloss: 0.233158 valid_1&#39;s auc: 0.773414 valid_1&#39;s binary_logloss: 0.241632 | 10 | 0.7734 | 0.9461 | 421.7 | 6.672 | 189.9 | 10.04 | 47.84 | 46.75 | 9.148 | 0.7801 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.764174 training&#39;s binary_logloss: 0.248029 valid_1&#39;s auc: 0.755162 valid_1&#39;s binary_logloss: 0.249577 [200] training&#39;s auc: 0.778767 training&#39;s binary_logloss: 0.241173 valid_1&#39;s auc: 0.765754 valid_1&#39;s binary_logloss: 0.244443 [300] training&#39;s auc: 0.788759 training&#39;s binary_logloss: 0.237386 valid_1&#39;s auc: 0.771036 valid_1&#39;s binary_logloss: 0.242441 [400] training&#39;s auc: 0.795987 training&#39;s binary_logloss: 0.234694 valid_1&#39;s auc: 0.773675 valid_1&#39;s binary_logloss: 0.241458 [500] training&#39;s auc: 0.80232 training&#39;s binary_logloss: 0.232393 valid_1&#39;s auc: 0.775601 valid_1&#39;s binary_logloss: 0.240772 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.80232 training&#39;s binary_logloss: 0.232393 valid_1&#39;s auc: 0.775601 valid_1&#39;s binary_logloss: 0.240772 | 11 | 0.7756 | 0.5017 | 393.7 | 10.71 | 119.0 | 45.57 | 28.2 | 10.95 | 4.662 | 0.9552 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.778162 training&#39;s binary_logloss: 0.243432 valid_1&#39;s auc: 0.759238 valid_1&#39;s binary_logloss: 0.247741 [200] training&#39;s auc: 0.799162 training&#39;s binary_logloss: 0.234309 valid_1&#39;s auc: 0.769473 valid_1&#39;s binary_logloss: 0.24293 [300] training&#39;s auc: 0.814812 training&#39;s binary_logloss: 0.228328 valid_1&#39;s auc: 0.773682 valid_1&#39;s binary_logloss: 0.241344 [400] training&#39;s auc: 0.827839 training&#39;s binary_logloss: 0.223397 valid_1&#39;s auc: 0.776065 valid_1&#39;s binary_logloss: 0.240505 [500] training&#39;s auc: 0.838855 training&#39;s binary_logloss: 0.219213 valid_1&#39;s auc: 0.777105 valid_1&#39;s binary_logloss: 0.240137 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.838855 training&#39;s binary_logloss: 0.219213 valid_1&#39;s auc: 0.777105 valid_1&#39;s binary_logloss: 0.240137 | 12 | 0.7771 | 0.6423 | 400.8 | 13.13 | 174.5 | 22.2 | 53.42 | 0.8135 | 3.857 | 0.603 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.777272 training&#39;s binary_logloss: 0.243367 valid_1&#39;s auc: 0.759082 valid_1&#39;s binary_logloss: 0.247643 [200] training&#39;s auc: 0.7988 training&#39;s binary_logloss: 0.234288 valid_1&#39;s auc: 0.769727 valid_1&#39;s binary_logloss: 0.24285 [300] training&#39;s auc: 0.814442 training&#39;s binary_logloss: 0.228368 valid_1&#39;s auc: 0.773843 valid_1&#39;s binary_logloss: 0.241303 [400] training&#39;s auc: 0.827084 training&#39;s binary_logloss: 0.223525 valid_1&#39;s auc: 0.776152 valid_1&#39;s binary_logloss: 0.240502 [500] training&#39;s auc: 0.838169 training&#39;s binary_logloss: 0.219393 valid_1&#39;s auc: 0.777237 valid_1&#39;s binary_logloss: 0.240146 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.838169 training&#39;s binary_logloss: 0.219393 valid_1&#39;s auc: 0.777237 valid_1&#39;s binary_logloss: 0.240146 | 13 | 0.7772 | 0.6969 | 388.8 | 13.69 | 178.8 | 26.23 | 50.31 | 0.1285 | 1.032 | 0.8563 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.777861 training&#39;s binary_logloss: 0.242936 valid_1&#39;s auc: 0.757958 valid_1&#39;s binary_logloss: 0.247557 [200] training&#39;s auc: 0.801461 training&#39;s binary_logloss: 0.233442 valid_1&#39;s auc: 0.769359 valid_1&#39;s binary_logloss: 0.242945 [300] training&#39;s auc: 0.818486 training&#39;s binary_logloss: 0.226978 valid_1&#39;s auc: 0.77388 valid_1&#39;s binary_logloss: 0.241346 [400] training&#39;s auc: 0.832854 training&#39;s binary_logloss: 0.221651 valid_1&#39;s auc: 0.776378 valid_1&#39;s binary_logloss: 0.240465 [500] training&#39;s auc: 0.844619 training&#39;s binary_logloss: 0.217191 valid_1&#39;s auc: 0.777355 valid_1&#39;s binary_logloss: 0.240144 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.844619 training&#39;s binary_logloss: 0.217191 valid_1&#39;s auc: 0.777355 valid_1&#39;s binary_logloss: 0.240144 | 14 | 0.7774 | 0.9789 | 363.4 | 15.32 | 190.7 | 2.003 | 57.05 | 1.982 | 4.416 | 0.9291 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.779098 training&#39;s binary_logloss: 0.242597 valid_1&#39;s auc: 0.758713 valid_1&#39;s binary_logloss: 0.247432 [200] training&#39;s auc: 0.802 training&#39;s binary_logloss: 0.233149 valid_1&#39;s auc: 0.769621 valid_1&#39;s binary_logloss: 0.242884 [300] training&#39;s auc: 0.81865 training&#39;s binary_logloss: 0.22679 valid_1&#39;s auc: 0.773862 valid_1&#39;s binary_logloss: 0.241339 [400] training&#39;s auc: 0.832546 training&#39;s binary_logloss: 0.221521 valid_1&#39;s auc: 0.776112 valid_1&#39;s binary_logloss: 0.24053 [500] training&#39;s auc: 0.844492 training&#39;s binary_logloss: 0.216971 valid_1&#39;s auc: 0.776749 valid_1&#39;s binary_logloss: 0.240308 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.844492 training&#39;s binary_logloss: 0.216971 valid_1&#39;s auc: 0.776749 valid_1&#39;s binary_logloss: 0.240308 | 15 | 0.7767 | 0.8747 | 398.4 | 15.32 | 177.1 | 23.05 | 56.52 | 2.533 | 1.005 | 0.9827 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.76187 training&#39;s binary_logloss: 0.247879 valid_1&#39;s auc: 0.753913 valid_1&#39;s binary_logloss: 0.249302 [200] training&#39;s auc: 0.778662 training&#39;s binary_logloss: 0.241031 valid_1&#39;s auc: 0.765519 valid_1&#39;s binary_logloss: 0.2444 [300] training&#39;s auc: 0.78922 training&#39;s binary_logloss: 0.23712 valid_1&#39;s auc: 0.771074 valid_1&#39;s binary_logloss: 0.242375 [400] training&#39;s auc: 0.797187 training&#39;s binary_logloss: 0.234247 valid_1&#39;s auc: 0.773953 valid_1&#39;s binary_logloss: 0.241344 [500] training&#39;s auc: 0.804056 training&#39;s binary_logloss: 0.231805 valid_1&#39;s auc: 0.775929 valid_1&#39;s binary_logloss: 0.24065 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.804056 training&#39;s binary_logloss: 0.231805 valid_1&#39;s auc: 0.775929 valid_1&#39;s binary_logloss: 0.24065 | 16 | 0.7759 | 0.6846 | 368.2 | 14.93 | 197.8 | 25.54 | 25.02 | 4.245 | 2.664 | 0.8288 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.76672 training&#39;s binary_logloss: 0.247028 valid_1&#39;s auc: 0.756541 valid_1&#39;s binary_logloss: 0.248962 [200] training&#39;s auc: 0.783669 training&#39;s binary_logloss: 0.239569 valid_1&#39;s auc: 0.767434 valid_1&#39;s binary_logloss: 0.24386 [300] training&#39;s auc: 0.79523 training&#39;s binary_logloss: 0.235244 valid_1&#39;s auc: 0.772526 valid_1&#39;s binary_logloss: 0.241924 [400] training&#39;s auc: 0.804427 training&#39;s binary_logloss: 0.231925 valid_1&#39;s auc: 0.775267 valid_1&#39;s binary_logloss: 0.240915 [500] training&#39;s auc: 0.811987 training&#39;s binary_logloss: 0.229176 valid_1&#39;s auc: 0.776778 valid_1&#39;s binary_logloss: 0.240381 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.811987 training&#39;s binary_logloss: 0.229176 valid_1&#39;s auc: 0.776778 valid_1&#39;s binary_logloss: 0.240381 | 17 | 0.7768 | 0.5385 | 375.6 | 15.71 | 162.2 | 1.157 | 31.29 | 0.3692 | 9.23 | 0.5688 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.763487 training&#39;s binary_logloss: 0.247559 valid_1&#39;s auc: 0.753988 valid_1&#39;s binary_logloss: 0.249255 [200] training&#39;s auc: 0.780223 training&#39;s binary_logloss: 0.240626 valid_1&#39;s auc: 0.76534 valid_1&#39;s binary_logloss: 0.244429 [300] training&#39;s auc: 0.790465 training&#39;s binary_logloss: 0.236701 valid_1&#39;s auc: 0.770206 valid_1&#39;s binary_logloss: 0.242582 [400] training&#39;s auc: 0.797595 training&#39;s binary_logloss: 0.234049 valid_1&#39;s auc: 0.772583 valid_1&#39;s binary_logloss: 0.24171 [500] training&#39;s auc: 0.80371 training&#39;s binary_logloss: 0.231846 valid_1&#39;s auc: 0.774369 valid_1&#39;s binary_logloss: 0.241097 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.80371 training&#39;s binary_logloss: 0.231846 valid_1&#39;s auc: 0.774369 valid_1&#39;s binary_logloss: 0.241097 | 18 | 0.7744 | 0.7394 | 417.5 | 6.129 | 174.7 | 31.59 | 28.05 | 1.518 | 6.418 | 0.8383 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.776986 training&#39;s binary_logloss: 0.24363 valid_1&#39;s auc: 0.757374 valid_1&#39;s binary_logloss: 0.248025 [200] training&#39;s auc: 0.797886 training&#39;s binary_logloss: 0.234776 valid_1&#39;s auc: 0.768483 valid_1&#39;s binary_logloss: 0.243304 [300] training&#39;s auc: 0.813076 training&#39;s binary_logloss: 0.228954 valid_1&#39;s auc: 0.77327 valid_1&#39;s binary_logloss: 0.241566 [400] training&#39;s auc: 0.825935 training&#39;s binary_logloss: 0.224135 valid_1&#39;s auc: 0.775806 valid_1&#39;s binary_logloss: 0.240681 [500] training&#39;s auc: 0.836838 training&#39;s binary_logloss: 0.219956 valid_1&#39;s auc: 0.776992 valid_1&#39;s binary_logloss: 0.240235 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.836838 training&#39;s binary_logloss: 0.219956 valid_1&#39;s auc: 0.776992 valid_1&#39;s binary_logloss: 0.240235 | 19 | 0.777 | 0.8929 | 381.0 | 12.24 | 162.5 | 1.124 | 62.24 | 9.051 | 8.559 | 0.5757 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.774485 training&#39;s binary_logloss: 0.244852 valid_1&#39;s auc: 0.757004 valid_1&#39;s binary_logloss: 0.248424 [200] training&#39;s auc: 0.792537 training&#39;s binary_logloss: 0.236941 valid_1&#39;s auc: 0.767318 valid_1&#39;s binary_logloss: 0.243762 [300] training&#39;s auc: 0.804149 training&#39;s binary_logloss: 0.232344 valid_1&#39;s auc: 0.771777 valid_1&#39;s binary_logloss: 0.242089 [400] training&#39;s auc: 0.8131 training&#39;s binary_logloss: 0.228878 valid_1&#39;s auc: 0.774161 valid_1&#39;s binary_logloss: 0.241232 [500] training&#39;s auc: 0.821163 training&#39;s binary_logloss: 0.225782 valid_1&#39;s auc: 0.775404 valid_1&#39;s binary_logloss: 0.240783 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.821163 training&#39;s binary_logloss: 0.225782 valid_1&#39;s auc: 0.775404 valid_1&#39;s binary_logloss: 0.240783 | 20 | 0.7754 | 0.8395 | 369.5 | 7.312 | 179.4 | 17.39 | 58.63 | 13.71 | 4.133 | 0.8568 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.780021 training&#39;s binary_logloss: 0.243308 valid_1&#39;s auc: 0.759823 valid_1&#39;s binary_logloss: 0.24777 [200] training&#39;s auc: 0.800958 training&#39;s binary_logloss: 0.233946 valid_1&#39;s auc: 0.769898 valid_1&#39;s binary_logloss: 0.24286 [300] training&#39;s auc: 0.81698 training&#39;s binary_logloss: 0.22778 valid_1&#39;s auc: 0.774314 valid_1&#39;s binary_logloss: 0.241218 [400] training&#39;s auc: 0.830722 training&#39;s binary_logloss: 0.222612 valid_1&#39;s auc: 0.776942 valid_1&#39;s binary_logloss: 0.240308 [500] training&#39;s auc: 0.842042 training&#39;s binary_logloss: 0.218291 valid_1&#39;s auc: 0.778057 valid_1&#39;s binary_logloss: 0.239921 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.842042 training&#39;s binary_logloss: 0.218291 valid_1&#39;s auc: 0.778057 valid_1&#39;s binary_logloss: 0.239921 | 21 | 0.7781 | 0.5967 | 403.2 | 12.91 | 158.8 | 7.338 | 58.12 | 0.6682 | 9.355 | 0.801 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.77898 training&#39;s binary_logloss: 0.243342 valid_1&#39;s auc: 0.758555 valid_1&#39;s binary_logloss: 0.247992 [200] training&#39;s auc: 0.798959 training&#39;s binary_logloss: 0.234514 valid_1&#39;s auc: 0.76841 valid_1&#39;s binary_logloss: 0.243369 [300] training&#39;s auc: 0.814164 training&#39;s binary_logloss: 0.228666 valid_1&#39;s auc: 0.773321 valid_1&#39;s binary_logloss: 0.241591 [400] training&#39;s auc: 0.827085 training&#39;s binary_logloss: 0.223771 valid_1&#39;s auc: 0.776039 valid_1&#39;s binary_logloss: 0.240644 [500] training&#39;s auc: 0.838285 training&#39;s binary_logloss: 0.219513 valid_1&#39;s auc: 0.777508 valid_1&#39;s binary_logloss: 0.240124 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.838285 training&#39;s binary_logloss: 0.219513 valid_1&#39;s auc: 0.777508 valid_1&#39;s binary_logloss: 0.240124 | 22 | 0.7775 | 0.6984 | 408.4 | 15.75 | 137.0 | 1.697 | 63.85 | 11.6 | 0.1763 | 0.864 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.777057 training&#39;s binary_logloss: 0.243549 valid_1&#39;s auc: 0.757727 valid_1&#39;s binary_logloss: 0.247866 [200] training&#39;s auc: 0.798708 training&#39;s binary_logloss: 0.234457 valid_1&#39;s auc: 0.769064 valid_1&#39;s binary_logloss: 0.243119 [300] training&#39;s auc: 0.814158 training&#39;s binary_logloss: 0.228529 valid_1&#39;s auc: 0.773437 valid_1&#39;s binary_logloss: 0.241564 [400] training&#39;s auc: 0.827436 training&#39;s binary_logloss: 0.223518 valid_1&#39;s auc: 0.776108 valid_1&#39;s binary_logloss: 0.240607 [500] training&#39;s auc: 0.83867 training&#39;s binary_logloss: 0.219253 valid_1&#39;s auc: 0.777169 valid_1&#39;s binary_logloss: 0.240208 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.83867 training&#39;s binary_logloss: 0.219253 valid_1&#39;s auc: 0.777169 valid_1&#39;s binary_logloss: 0.240208 | 23 | 0.7772 | 0.9089 | 407.4 | 15.54 | 180.7 | 24.17 | 58.56 | 5.618 | 9.113 | 0.788 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.7743 training&#39;s binary_logloss: 0.244663 valid_1&#39;s auc: 0.757346 valid_1&#39;s binary_logloss: 0.248295 [200] training&#39;s auc: 0.7934 training&#39;s binary_logloss: 0.236415 valid_1&#39;s auc: 0.768102 valid_1&#39;s binary_logloss: 0.243544 [300] training&#39;s auc: 0.806554 training&#39;s binary_logloss: 0.231308 valid_1&#39;s auc: 0.772786 valid_1&#39;s binary_logloss: 0.241797 [400] training&#39;s auc: 0.817607 training&#39;s binary_logloss: 0.22716 valid_1&#39;s auc: 0.775328 valid_1&#39;s binary_logloss: 0.240878 [500] training&#39;s auc: 0.827256 training&#39;s binary_logloss: 0.223529 valid_1&#39;s auc: 0.776919 valid_1&#39;s binary_logloss: 0.240307 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.827256 training&#39;s binary_logloss: 0.223529 valid_1&#39;s auc: 0.776919 valid_1&#39;s binary_logloss: 0.240307 | 24 | 0.7769 | 0.8291 | 408.9 | 10.8 | 198.5 | 10.39 | 54.8 | 12.48 | 6.565 | 0.5806 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.780987 training&#39;s binary_logloss: 0.242281 valid_1&#39;s auc: 0.759117 valid_1&#39;s binary_logloss: 0.247485 [200] training&#39;s auc: 0.803399 training&#39;s binary_logloss: 0.232707 valid_1&#39;s auc: 0.769436 valid_1&#39;s binary_logloss: 0.242907 [300] training&#39;s auc: 0.820217 training&#39;s binary_logloss: 0.226213 valid_1&#39;s auc: 0.773667 valid_1&#39;s binary_logloss: 0.241358 [400] training&#39;s auc: 0.834774 training&#39;s binary_logloss: 0.220692 valid_1&#39;s auc: 0.776268 valid_1&#39;s binary_logloss: 0.240465 [500] training&#39;s auc: 0.846897 training&#39;s binary_logloss: 0.215994 valid_1&#39;s auc: 0.777223 valid_1&#39;s binary_logloss: 0.240096 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.846897 training&#39;s binary_logloss: 0.215994 valid_1&#39;s auc: 0.777223 valid_1&#39;s binary_logloss: 0.240096 | 25 | 0.7772 | 0.8242 | 414.0 | 12.84 | 139.6 | 23.64 | 62.52 | 4.5 | 1.024 | 0.9178 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.781337 training&#39;s binary_logloss: 0.242246 valid_1&#39;s auc: 0.759542 valid_1&#39;s binary_logloss: 0.247353 [200] training&#39;s auc: 0.80494 training&#39;s binary_logloss: 0.232417 valid_1&#39;s auc: 0.770308 valid_1&#39;s binary_logloss: 0.242697 [300] training&#39;s auc: 0.822298 training&#39;s binary_logloss: 0.22572 valid_1&#39;s auc: 0.774614 valid_1&#39;s binary_logloss: 0.241111 [400] training&#39;s auc: 0.836837 training&#39;s binary_logloss: 0.220186 valid_1&#39;s auc: 0.777086 valid_1&#39;s binary_logloss: 0.240274 [500] training&#39;s auc: 0.848534 training&#39;s binary_logloss: 0.21562 valid_1&#39;s auc: 0.777809 valid_1&#39;s binary_logloss: 0.23998 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.848534 training&#39;s binary_logloss: 0.21562 valid_1&#39;s auc: 0.777809 valid_1&#39;s binary_logloss: 0.23998 | 26 | 0.7778 | 0.8364 | 438.5 | 12.59 | 126.3 | 7.767 | 63.26 | 0.3294 | 9.232 | 0.6026 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.777309 training&#39;s binary_logloss: 0.243448 valid_1&#39;s auc: 0.75775 valid_1&#39;s binary_logloss: 0.247903 [200] training&#39;s auc: 0.799142 training&#39;s binary_logloss: 0.234375 valid_1&#39;s auc: 0.768792 valid_1&#39;s binary_logloss: 0.243165 [300] training&#39;s auc: 0.814917 training&#39;s binary_logloss: 0.22837 valid_1&#39;s auc: 0.773507 valid_1&#39;s binary_logloss: 0.241467 [400] training&#39;s auc: 0.828058 training&#39;s binary_logloss: 0.223386 valid_1&#39;s auc: 0.775906 valid_1&#39;s binary_logloss: 0.240582 [500] training&#39;s auc: 0.839395 training&#39;s binary_logloss: 0.219123 valid_1&#39;s auc: 0.777117 valid_1&#39;s binary_logloss: 0.240155 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.839395 training&#39;s binary_logloss: 0.219123 valid_1&#39;s auc: 0.777117 valid_1&#39;s binary_logloss: 0.240155 | 27 | 0.7771 | 0.7062 | 438.3 | 13.16 | 93.45 | 3.168 | 56.62 | 4.489 | 5.242 | 0.5061 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.778145 training&#39;s binary_logloss: 0.243305 valid_1&#39;s auc: 0.757936 valid_1&#39;s binary_logloss: 0.2478 [200] training&#39;s auc: 0.799265 training&#39;s binary_logloss: 0.234379 valid_1&#39;s auc: 0.768543 valid_1&#39;s binary_logloss: 0.243239 [300] training&#39;s auc: 0.81363 training&#39;s binary_logloss: 0.228749 valid_1&#39;s auc: 0.772821 valid_1&#39;s binary_logloss: 0.241685 [400] training&#39;s auc: 0.825123 training&#39;s binary_logloss: 0.224326 valid_1&#39;s auc: 0.774995 valid_1&#39;s binary_logloss: 0.240928 [500] training&#39;s auc: 0.835617 training&#39;s binary_logloss: 0.220326 valid_1&#39;s auc: 0.776429 valid_1&#39;s binary_logloss: 0.240431 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.835617 training&#39;s binary_logloss: 0.220326 valid_1&#39;s auc: 0.776429 valid_1&#39;s binary_logloss: 0.240431 | 28 | 0.7764 | 0.9458 | 465.0 | 7.9 | 121.9 | 2.539 | 62.43 | 4.784 | 9.659 | 0.5056 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.768712 training&#39;s binary_logloss: 0.245667 valid_1&#39;s auc: 0.755179 valid_1&#39;s binary_logloss: 0.248495 [200] training&#39;s auc: 0.788298 training&#39;s binary_logloss: 0.237819 valid_1&#39;s auc: 0.767231 valid_1&#39;s binary_logloss: 0.243688 [300] training&#39;s auc: 0.801509 training&#39;s binary_logloss: 0.232922 valid_1&#39;s auc: 0.772532 valid_1&#39;s binary_logloss: 0.241848 [400] training&#39;s auc: 0.811986 training&#39;s binary_logloss: 0.229057 valid_1&#39;s auc: 0.775444 valid_1&#39;s binary_logloss: 0.24083 [500] training&#39;s auc: 0.820471 training&#39;s binary_logloss: 0.225952 valid_1&#39;s auc: 0.776356 valid_1&#39;s binary_logloss: 0.24049 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.820471 training&#39;s binary_logloss: 0.225952 valid_1&#39;s auc: 0.776356 valid_1&#39;s binary_logloss: 0.24049 | 29 | 0.7764 | 0.9001 | 432.8 | 12.59 | 121.5 | 4.114 | 38.13 | 2.886 | 7.886 | 0.9864 | Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.78142 training&#39;s binary_logloss: 0.242343 valid_1&#39;s auc: 0.760102 valid_1&#39;s binary_logloss: 0.247216 [200] training&#39;s auc: 0.803636 training&#39;s binary_logloss: 0.232854 valid_1&#39;s auc: 0.770109 valid_1&#39;s binary_logloss: 0.242694 [300] training&#39;s auc: 0.819047 training&#39;s binary_logloss: 0.226767 valid_1&#39;s auc: 0.77406 valid_1&#39;s binary_logloss: 0.241196 [400] training&#39;s auc: 0.831064 training&#39;s binary_logloss: 0.22211 valid_1&#39;s auc: 0.775997 valid_1&#39;s binary_logloss: 0.240495 [500] training&#39;s auc: 0.841456 training&#39;s binary_logloss: 0.218102 valid_1&#39;s auc: 0.777091 valid_1&#39;s binary_logloss: 0.240091 Did not meet early stopping. Best iteration is: [500] training&#39;s auc: 0.841456 training&#39;s binary_logloss: 0.218102 valid_1&#39;s auc: 0.777091 valid_1&#39;s binary_logloss: 0.240091 | 30 | 0.7771 | 0.7396 | 442.3 | 9.243 | 126.5 | 31.85 | 63.82 | 0.6593 | 6.771 | 0.8922 | ===================================================================================================================================== . lgbBO.res . [{&#39;target&#39;: 0.7758055093230539, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.7744067519636624, &#39;max_bin&#39;: 360.44278952248555, &#39;max_depth&#39;: 12.027633760716439, &#39;min_child_samples&#39;: 113.52780476941041, &#39;min_child_weight&#39;: 21.75908516760633, &#39;num_leaves&#39;: 49.835764522666246, &#39;reg_alpha&#39;: 21.884984691022, &#39;reg_lambda&#39;: 8.917838234820016, &#39;subsample&#39;: 0.9818313802505146}}, {&#39;target&#39;: 0.7757909289675659, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.6917207594128889, &#39;max_bin&#39;: 397.94526866050563, &#39;max_depth&#39;: 11.288949197529044, &#39;min_child_samples&#39;: 117.92846660784714, &#39;min_child_weight&#39;: 46.35423527634039, &#39;num_leaves&#39;: 26.841442327915477, &#39;reg_alpha&#39;: 4.36559369208002, &#39;reg_lambda&#39;: 0.20316375600581688, &#39;subsample&#39;: 0.916309922773969}}, {&#39;target&#39;: 0.7771779879367707, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.8890783754749252, &#39;max_bin&#39;: 436.30595264094137, &#39;max_depth&#39;: 15.78618342232764, &#39;min_child_samples&#39;: 161.8401272011775, &#39;min_child_weight&#39;: 23.61248875039366, &#39;num_leaves&#39;: 55.22116705145822, &#39;reg_alpha&#39;: 5.922538549187972, &#39;reg_lambda&#39;: 6.3995702922539115, &#39;subsample&#39;: 0.5716766437045232}}, {&#39;target&#39;: 0.7748333416046418, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.972334458524792, &#39;max_bin&#39;: 265.70567765753515, &#39;max_depth&#39;: 10.146619399905235, &#39;min_child_samples&#39;: 60.265566299879126, &#39;min_child_weight&#39;: 38.93745078227661, &#39;num_leaves&#39;: 42.24601328866194, &#39;reg_alpha&#39;: 28.426013103943742, &#39;reg_lambda&#39;: 0.18887921456311507, &#39;subsample&#39;: 0.8088177485379385}}, {&#39;target&#39;: 0.774035094542375, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.8060478613612108, &#39;max_bin&#39;: 312.2976584686309, &#39;max_depth&#39;: 15.437480785146242, &#39;min_child_samples&#39;: 139.54585682966186, &#39;min_child_weight&#39;: 18.615887128115514, &#39;num_leaves&#39;: 41.481278151973655, &#39;reg_alpha&#39;: 34.88458348440397, &#39;reg_lambda&#39;: 0.6031944908210691, &#39;subsample&#39;: 0.8333833577228338}}, {&#39;target&#39;: 0.7773908017189786, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.640481830861817, &#39;max_bin&#39;: 435.0379450370509, &#39;max_depth&#39;: 13.497244758196743, &#39;min_child_samples&#39;: 169.30259380663517, &#39;min_child_weight&#39;: 26.924368410534857, &#39;num_leaves&#39;: 57.69153705029583, &#39;reg_alpha&#39;: 5.7675960060342195, &#39;reg_lambda&#39;: 9.196441703351635, &#39;subsample&#39;: 0.6129607288812317}}, {&#39;target&#39;: 0.7755362516633085, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.6778196691000836, &#39;max_bin&#39;: 477.176698811766, &#39;max_depth&#39;: 11.752369565537183, &#39;min_child_samples&#39;: 194.8929524178145, &#39;min_child_weight&#39;: 46.84607882647774, &#39;num_leaves&#39;: 59.472175385640206, &#39;reg_alpha&#39;: 26.316833854656593, &#39;reg_lambda&#39;: 8.387658534981561, &#39;subsample&#39;: 0.7753185059732159}}, {&#39;target&#39;: 0.7772142029411041, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.7373732904804776, &#39;max_bin&#39;: 408.1358313138957, &#39;max_depth&#39;: 12.843992667819151, &#39;min_child_samples&#39;: 197.5558677155646, &#39;min_child_weight&#39;: 5.693485177437429, &#39;num_leaves&#39;: 57.72941400376497, &#39;reg_alpha&#39;: 6.171344107340323, &#39;reg_lambda&#39;: 8.007058832279368, &#39;subsample&#39;: 0.6427405880739969}}, {&#39;target&#39;: 0.7763952615906466, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.7748893532024008, &#39;max_bin&#39;: 388.9169674423393, &#39;max_depth&#39;: 14.298324669842952, &#39;min_child_samples&#39;: 193.91589835191607, &#39;min_child_weight&#39;: 49.909444261894315, &#39;num_leaves&#39;: 62.84933349372062, &#39;reg_alpha&#39;: 13.150761788903786, &#39;reg_lambda&#39;: 9.906371801724482, &#39;subsample&#39;: 0.6811573472682452}}, {&#39;target&#39;: 0.7734141467631326, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.9461483552083645, &#39;max_bin&#39;: 421.69292633292594, &#39;max_depth&#39;: 6.671600015870614, &#39;min_child_samples&#39;: 189.89049916570497, &#39;min_child_weight&#39;: 10.044424315906738, &#39;num_leaves&#39;: 47.83526701748679, &#39;reg_alpha&#39;: 46.74823035052019, &#39;reg_lambda&#39;: 9.147970169890396, &#39;subsample&#39;: 0.7801135437391282}}, {&#39;target&#39;: 0.7756008426857747, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.5017469237656516, &#39;max_bin&#39;: 393.72190024490175, &#39;max_depth&#39;: 10.707724066578827, &#39;min_child_samples&#39;: 118.96813016814396, &#39;min_child_weight&#39;: 45.56868876896979, &#39;num_leaves&#39;: 28.201502549830405, &#39;reg_alpha&#39;: 10.952704243299513, &#39;reg_lambda&#39;: 4.662235191571802, &#39;subsample&#39;: 0.9551699402948195}}, {&#39;target&#39;: 0.7771050305636831, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.6422839773084814, &#39;max_bin&#39;: 400.82150970541625, &#39;max_depth&#39;: 13.131175782431662, &#39;min_child_samples&#39;: 174.45170668076625, &#39;min_child_weight&#39;: 22.20273089632871, &#39;num_leaves&#39;: 53.41657085105421, &#39;reg_alpha&#39;: 0.8135160827380638, &#39;reg_lambda&#39;: 3.856905787752113, &#39;subsample&#39;: 0.6029851974682363}}, {&#39;target&#39;: 0.7772365047377109, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.6968897778533021, &#39;max_bin&#39;: 388.82764448356886, &#39;max_depth&#39;: 13.691810834514651, &#39;min_child_samples&#39;: 178.80475184834663, &#39;min_child_weight&#39;: 26.23145438502705, &#39;num_leaves&#39;: 50.312746331501124, &#39;reg_alpha&#39;: 0.1285436498176259, &#39;reg_lambda&#39;: 1.032293662925894, &#39;subsample&#39;: 0.8562506227206372}}, {&#39;target&#39;: 0.7773549981224318, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.9789353469971445, &#39;max_bin&#39;: 363.4354295856673, &#39;max_depth&#39;: 15.315211746017521, &#39;min_child_samples&#39;: 190.73741827012313, &#39;min_child_weight&#39;: 2.003360059766269, &#39;num_leaves&#39;: 57.046953584464674, &#39;reg_alpha&#39;: 1.9824621842535364, &#39;reg_lambda&#39;: 4.416491356182225, &#39;subsample&#39;: 0.9290899949750824}}, {&#39;target&#39;: 0.7767492867273098, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.8747411340191682, &#39;max_bin&#39;: 398.36524681235744, &#39;max_depth&#39;: 15.320951372373688, &#39;min_child_samples&#39;: 177.08379767055555, &#39;min_child_weight&#39;: 23.052918269012956, &#39;num_leaves&#39;: 56.51625901289448, &#39;reg_alpha&#39;: 2.5331760761204847, &#39;reg_lambda&#39;: 1.0049109599081048, &#39;subsample&#39;: 0.9826775193057701}}, {&#39;target&#39;: 0.775929160395352, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.6845927664319438, &#39;max_bin&#39;: 368.221991524003, &#39;max_depth&#39;: 14.927073385197513, &#39;min_child_samples&#39;: 197.8030464042996, &#39;min_child_weight&#39;: 25.535853809991778, &#39;num_leaves&#39;: 25.021887939272, &#39;reg_alpha&#39;: 4.2450984307780235, &#39;reg_lambda&#39;: 2.664497197105028, &#39;subsample&#39;: 0.8288176887623219}}, {&#39;target&#39;: 0.7767777231064307, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.538512544314991, &#39;max_bin&#39;: 375.62078344659466, &#39;max_depth&#39;: 15.712329472869794, &#39;min_child_samples&#39;: 162.22115952435223, &#39;min_child_weight&#39;: 1.156941735685324, &#39;num_leaves&#39;: 31.294569230544013, &#39;reg_alpha&#39;: 0.36916877863456377, &#39;reg_lambda&#39;: 9.229532425566951, &#39;subsample&#39;: 0.5688093468774877}}, {&#39;target&#39;: 0.7743690019971274, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.7394308873506115, &#39;max_bin&#39;: 417.4989608386593, &#39;max_depth&#39;: 6.128913120786649, &#39;min_child_samples&#39;: 174.74416919268225, &#39;min_child_weight&#39;: 31.59245109049789, &#39;num_leaves&#39;: 28.05211885684618, &#39;reg_alpha&#39;: 1.5180061549611963, &#39;reg_lambda&#39;: 6.418396503524244, &#39;subsample&#39;: 0.8382939851041469}}, {&#39;target&#39;: 0.7769920411479967, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.8929451865708424, &#39;max_bin&#39;: 380.96408472669606, &#39;max_depth&#39;: 12.24296875089054, &#39;min_child_samples&#39;: 162.4681623947883, &#39;min_child_weight&#39;: 1.124323100936236, &#39;num_leaves&#39;: 62.237143754650376, &#39;reg_alpha&#39;: 9.051348381820413, &#39;reg_lambda&#39;: 8.55903813259641, &#39;subsample&#39;: 0.5756763184847222}}, {&#39;target&#39;: 0.7754041675256128, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.8395378766641771, &#39;max_bin&#39;: 369.45992318249404, &#39;max_depth&#39;: 7.311653298584847, &#39;min_child_samples&#39;: 179.448902083174, &#39;min_child_weight&#39;: 17.38672526468209, &#39;num_leaves&#39;: 58.6349743249024, &#39;reg_alpha&#39;: 13.707733769542015, &#39;reg_lambda&#39;: 4.132666603445228, &#39;subsample&#39;: 0.8567763982467431}}, {&#39;target&#39;: 0.7780565770624691, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.596684514717646, &#39;max_bin&#39;: 403.18943474881877, &#39;max_depth&#39;: 12.907058961217114, &#39;min_child_samples&#39;: 158.84286216603994, &#39;min_child_weight&#39;: 7.338287048901049, &#39;num_leaves&#39;: 58.11656509027165, &#39;reg_alpha&#39;: 0.6681705096965275, &#39;reg_lambda&#39;: 9.355145759543333, &#39;subsample&#39;: 0.8010238277727275}}, {&#39;target&#39;: 0.7775081101221989, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.6983666820490788, &#39;max_bin&#39;: 408.3845530220334, &#39;max_depth&#39;: 15.750233779055964, &#39;min_child_samples&#39;: 136.95205244907152, &#39;min_child_weight&#39;: 1.6965708103918615, &#39;num_leaves&#39;: 63.8506437238174, &#39;reg_alpha&#39;: 11.603475539657392, &#39;reg_lambda&#39;: 0.17633246697484675, &#39;subsample&#39;: 0.8640107135672386}}, {&#39;target&#39;: 0.7771685255576835, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.9089270943590277, &#39;max_bin&#39;: 407.4077569955801, &#39;max_depth&#39;: 15.54111069785531, &#39;min_child_samples&#39;: 180.7441063643676, &#39;min_child_weight&#39;: 24.16966471165873, &#39;num_leaves&#39;: 58.55774861930196, &#39;reg_alpha&#39;: 5.617770487707293, &#39;reg_lambda&#39;: 9.112524233329468, &#39;subsample&#39;: 0.7879625370264551}}, {&#39;target&#39;: 0.7769187946775454, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.8291181953449183, &#39;max_bin&#39;: 408.90814062859624, &#39;max_depth&#39;: 10.799903523797198, &#39;min_child_samples&#39;: 198.46670214232182, &#39;min_child_weight&#39;: 10.393808863704972, &#39;num_leaves&#39;: 54.79520213741949, &#39;reg_alpha&#39;: 12.481921001697238, &#39;reg_lambda&#39;: 6.564787886354399, &#39;subsample&#39;: 0.580612232524619}}, {&#39;target&#39;: 0.7772230140569003, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.8242099119497381, &#39;max_bin&#39;: 414.0195379290514, &#39;max_depth&#39;: 12.838272481918226, &#39;min_child_samples&#39;: 139.5861484641032, &#39;min_child_weight&#39;: 23.638609111440616, &#39;num_leaves&#39;: 62.52080831472535, &#39;reg_alpha&#39;: 4.499765686038743, &#39;reg_lambda&#39;: 1.0239192579133045, &#39;subsample&#39;: 0.9178127659622564}}, {&#39;target&#39;: 0.777808998528086, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.8364260787813464, &#39;max_bin&#39;: 438.5344980810344, &#39;max_depth&#39;: 12.59368901950138, &#39;min_child_samples&#39;: 126.31456889978566, &#39;min_child_weight&#39;: 7.767143783785291, &#39;num_leaves&#39;: 63.260425772040506, &#39;reg_alpha&#39;: 0.32940336713078977, &#39;reg_lambda&#39;: 9.23233466904062, &#39;subsample&#39;: 0.602582344997781}}, {&#39;target&#39;: 0.7771167374184544, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.7062460990955068, &#39;max_bin&#39;: 438.3043198858672, &#39;max_depth&#39;: 13.162643244440723, &#39;min_child_samples&#39;: 93.44971252277496, &#39;min_child_weight&#39;: 3.1677333380244557, &#39;num_leaves&#39;: 56.6248859454379, &#39;reg_alpha&#39;: 4.488505501877722, &#39;reg_lambda&#39;: 5.24181749541699, &#39;subsample&#39;: 0.5061422738685454}}, {&#39;target&#39;: 0.7764292464153193, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.9458015786718881, &#39;max_bin&#39;: 465.00800252186986, &#39;max_depth&#39;: 7.899527803307271, &#39;min_child_samples&#39;: 121.9201901620783, &#39;min_child_weight&#39;: 2.5392614386800236, &#39;num_leaves&#39;: 62.427676297468345, &#39;reg_alpha&#39;: 4.784274264326323, &#39;reg_lambda&#39;: 9.658699354226123, &#39;subsample&#39;: 0.505629085733924}}, {&#39;target&#39;: 0.7763562620386317, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.900086638538324, &#39;max_bin&#39;: 432.8385783874056, &#39;max_depth&#39;: 12.593459218233793, &#39;min_child_samples&#39;: 121.50703399935037, &#39;min_child_weight&#39;: 4.114137212124302, &#39;num_leaves&#39;: 38.12688146861727, &#39;reg_alpha&#39;: 2.885555306126921, &#39;reg_lambda&#39;: 7.88597353383862, &#39;subsample&#39;: 0.9863664224720974}}, {&#39;target&#39;: 0.777091004576216, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.7395613472763644, &#39;max_bin&#39;: 442.26433974553163, &#39;max_depth&#39;: 9.243101646454075, &#39;min_child_samples&#39;: 126.50175957072607, &#39;min_child_weight&#39;: 31.84651381717126, &#39;num_leaves&#39;: 63.82367461756897, &#39;reg_alpha&#39;: 0.6593173679660734, &#39;reg_lambda&#39;: 6.770945353052257, &#39;subsample&#39;: 0.8921800827540989}}] . Iteration &#44208;&#44284; Dictionary&#50640;&#49436; &#52572;&#45824; target&#44050;&#51012; &#44032;&#51648;&#45716; index &#52628;&#52636;&#54616;&#44256; &#44536;&#46412;&#51032; parameter &#44050;&#51012; &#52628;&#52636;. . target_list = [] for result in lgbBO.res: target = result[&#39;target&#39;] target_list.append(target) print(target_list) # 가장 큰 target 값을 가지는 순번(index)를 추출 print(&#39;maximum target index:&#39;, np.argmax(np.array(target_list))) . [0.7758055093230539, 0.7757909289675659, 0.7771779879367707, 0.7748333416046418, 0.774035094542375, 0.7773908017189786, 0.7755362516633085, 0.7772142029411041, 0.7763952615906466, 0.7734141467631326, 0.7756008426857747, 0.7771050305636831, 0.7772365047377109, 0.7773549981224318, 0.7767492867273098, 0.775929160395352, 0.7767777231064307, 0.7743690019971274, 0.7769920411479967, 0.7754041675256128, 0.7780565770624691, 0.7775081101221989, 0.7771685255576835, 0.7769187946775454, 0.7772230140569003, 0.777808998528086, 0.7771167374184544, 0.7764292464153193, 0.7763562620386317, 0.777091004576216] maximum target index: 20 . max_dict = lgbBO.res[np.argmax(np.array(target_list))] print(max_dict) . {&#39;target&#39;: 0.7780565770624691, &#39;params&#39;: {&#39;colsample_bytree&#39;: 0.596684514717646, &#39;max_bin&#39;: 403.18943474881877, &#39;max_depth&#39;: 12.907058961217114, &#39;min_child_samples&#39;: 158.84286216603994, &#39;min_child_weight&#39;: 7.338287048901049, &#39;num_leaves&#39;: 58.11656509027165, &#39;reg_alpha&#39;: 0.6681705096965275, &#39;reg_lambda&#39;: 9.355145759543333, &#39;subsample&#39;: 0.8010238277727275}} . &#52572;&#51201;&#54868;&#46108; &#54616;&#51060;&#54140; &#54028;&#46972;&#48120;&#53552;&#47484; &#44592;&#48152;&#51004;&#47196; &#51116; &#53580;&#49828;&#53944; . ftr_app = apps_all_train.drop([&#39;SK_ID_CURR&#39;, &#39;TARGET&#39;], axis=1) target_app = apps_all_train[&#39;TARGET&#39;] X_train, X_test, y_train, y_test = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020) print(&#39;train shape:&#39;, train_x.shape, &#39;valid shape:&#39;, valid_x.shape) lgbm_wrapper = LGBMClassifier( n_estimators=1000, learning_rate=0.02, max_depth = 13, num_leaves=58, colsample_bytree=0.597, subsample=0.801, max_bin=403, reg_alpha=0.668, reg_lambda=9.355, min_child_weight=7, min_child_samples=159, silent=-1, verbose=-1, ) evals = [(X_test, y_test)] lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=evals, verbose=100) preds = lgbm_wrapper.predict(X_test) pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1] . train shape: (215257, 174) valid shape: (92254, 174) Training until validation scores don&#39;t improve for 100 rounds [100] valid_0&#39;s auc: 0.759823 valid_0&#39;s binary_logloss: 0.24777 [200] valid_0&#39;s auc: 0.769883 valid_0&#39;s binary_logloss: 0.242862 [300] valid_0&#39;s auc: 0.773949 valid_0&#39;s binary_logloss: 0.241297 [400] valid_0&#39;s auc: 0.776637 valid_0&#39;s binary_logloss: 0.240371 [500] valid_0&#39;s auc: 0.77781 valid_0&#39;s binary_logloss: 0.239972 [600] valid_0&#39;s auc: 0.778251 valid_0&#39;s binary_logloss: 0.239807 [700] valid_0&#39;s auc: 0.778768 valid_0&#39;s binary_logloss: 0.239639 [800] valid_0&#39;s auc: 0.778859 valid_0&#39;s binary_logloss: 0.239603 [900] valid_0&#39;s auc: 0.779087 valid_0&#39;s binary_logloss: 0.239534 [1000] valid_0&#39;s auc: 0.77916 valid_0&#39;s binary_logloss: 0.239504 Did not meet early stopping. Best iteration is: [987] valid_0&#39;s auc: 0.779182 valid_0&#39;s binary_logloss: 0.239501 . from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix from sklearn.metrics import precision_recall_curve from sklearn.metrics import f1_score from sklearn.metrics import roc_curve from sklearn.metrics import roc_auc_score def get_clf_eval(y_test, pred=None, pred_proba=None): confusion = confusion_matrix( y_test, pred) accuracy = accuracy_score(y_test , pred) precision = precision_score(y_test , pred) recall = recall_score(y_test , pred) f1 = f1_score(y_test,pred) # ROC-AUC 추가 roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) # ROC-AUC print 추가 print(&#39;정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}&#39;.format(accuracy, precision, recall, f1, roc_auc)) get_clf_eval(y_test, preds, pred_proba) . 오차 행렬 [[84653 180] [ 7201 220]] 정확도: 0.9200, 정밀도: 0.5500, 재현율: 0.0296, F1: 0.0563, AUC:0.7792 . def roc_curve_plot(y_test , pred_proba_c1): # 임곗값에 따른 FPR, TPR 값을 반환 받음. fprs , tprs , thresholds = roc_curve(y_test ,pred_proba_c1) # ROC Curve를 plot 곡선으로 그림. plt.plot(fprs , tprs, label=&#39;ROC&#39;) # 가운데 대각선 직선을 그림. plt.plot([0, 1], [0, 1], &#39;k--&#39;, label=&#39;Random&#39;) # FPR X 축의 Scale을 0.1 단위로 변경, X,Y 축명 설정등 start, end = plt.xlim() plt.xticks(np.round(np.arange(start, end, 0.1),2)) plt.xlim(0,1); plt.ylim(0,1) plt.xlabel(&#39;FPR( 1 - Sensitivity )&#39;); plt.ylabel(&#39;TPR( Recall )&#39;) plt.legend() plt.show() roc_curve_plot(y_test, pred_proba) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-09-01T09:46:50.009734 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ cross validation &#51004;&#47196; hyper parameter &#51116; tuning . bayesian_params = { &#39;max_depth&#39;: (6, 16), &#39;num_leaves&#39;: (24, 64), &#39;min_data_in_leaf&#39;: (10, 200), # min_child_samples &#39;min_child_weight&#39;:(1, 50), &#39;bagging_fraction&#39;:(0.5, 1.0), # subsample &#39;feature_fraction&#39;: (0.5, 1.0), # colsample_bytree &#39;max_bin&#39;:(10, 500), &#39;lambda_l2&#39;:(0.001, 10), # reg_lambda &#39;lambda_l1&#39;: (0.01, 50) # reg_alpha } . import lightgbm as lgb train_data = lgb.Dataset(data=ftr_app, label=target_app, free_raw_data=False) def lgb_roc_eval_cv(max_depth, num_leaves, min_data_in_leaf, min_child_weight, bagging_fraction, feature_fraction, max_bin, lambda_l2, lambda_l1): params = { &quot;num_iterations&quot;:500, &quot;learning_rate&quot;:0.02, &#39;early_stopping_rounds&#39;:100, &#39;metric&#39;:&#39;auc&#39;, &#39;max_depth&#39;: int(round(max_depth)), # 호출 시 실수형 값이 들어오므로 실수형 하이퍼 파라미터는 정수형으로 변경 &#39;num_leaves&#39;: int(round(num_leaves)), &#39;min_data_in_leaf&#39;: int(round(min_data_in_leaf)), &#39;min_child_weight&#39;: int(round(min_child_weight)), &#39;bagging_fraction&#39;: max(min(bagging_fraction, 1), 0), &#39;feature_fraction&#39;: max(min(feature_fraction, 1), 0), &#39;max_bin&#39;: max(int(round(max_bin)),10), &#39;lambda_l2&#39;: max(lambda_l2,0), &#39;lambda_l1&#39;: max(lambda_l1, 0) } # 파이썬 lightgbm의 cv 메소드를 사용. cv_result = lgb.cv(params, train_data, nfold=3, seed=0, verbose_eval =100, early_stopping_rounds=50, metrics=[&#39;auc&#39;]) return max(cv_result[&#39;auc-mean&#39;]) . lgbBO = BayesianOptimization(lgb_roc_eval_cv,bayesian_params , random_state=0) # 함수 반환값이 최대가 되는 입력값 유추를 위한 iteration 수행. lgbBO.maximize(init_points=5, n_iter=25) . | iter | target | baggin... | featur... | lambda_l1 | lambda_l2 | max_bin | max_depth | min_ch... | min_da... | num_le... | - . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163181 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 20217 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229890 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 20217 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170975 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 20217 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.752413 + 0.00209345 [200] cv_agg&#39;s auc: 0.761932 + 0.00172974 [300] cv_agg&#39;s auc: 0.766578 + 0.00170894 [400] cv_agg&#39;s auc: 0.768997 + 0.00156106 [500] cv_agg&#39;s auc: 0.770413 + 0.0014655 | 1 | 0.7704 | 0.7744 | 0.8576 | 30.14 | 5.449 | 217.6 | 12.46 | 22.44 | 179.4 | 62.55 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150128 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 39937 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269771 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 39937 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220449 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 39937 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.752486 + 0.00223274 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [200] cv_agg&#39;s auc: 0.762232 + 0.00215819 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [300] cv_agg&#39;s auc: 0.767065 + 0.00202812 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [400] cv_agg&#39;s auc: 0.76932 + 0.0018439 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [500] cv_agg&#39;s auc: 0.770732 + 0.00167646 | 2 | 0.7707 | 0.6917 | 0.8959 | 26.45 | 5.681 | 463.5 | 6.71 | 5.269 | 13.84 | 57.3 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031859 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 21674 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145476 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 21674 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217018 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 21674 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.747033 + 0.00241826 [200] cv_agg&#39;s auc: 0.757622 + 0.00238554 [300] cv_agg&#39;s auc: 0.763026 + 0.00240661 [400] cv_agg&#39;s auc: 0.765727 + 0.00246971 [500] cv_agg&#39;s auc: 0.767406 + 0.00235341 | 3 | 0.7674 | 0.8891 | 0.935 | 48.93 | 7.992 | 236.1 | 13.81 | 6.795 | 131.6 | 29.73 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216285 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 34163 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147815 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 34163 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213175 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 34163 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.751849 + 0.00212158 [200] cv_agg&#39;s auc: 0.762101 + 0.00196294 [300] cv_agg&#39;s auc: 0.766926 + 0.00203444 [400] cv_agg&#39;s auc: 0.769328 + 0.00189603 [500] cv_agg&#39;s auc: 0.770617 + 0.00193616 | 4 | 0.7706 | 0.9723 | 0.7609 | 20.74 | 2.646 | 389.4 | 10.56 | 28.85 | 13.57 | 48.71 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223108 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 17490 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163106 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 17490 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019565 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 17490 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.750094 + 0.00231875 [200] cv_agg&#39;s auc: 0.760001 + 0.00216013 [300] cv_agg&#39;s auc: 0.765002 + 0.00214597 [400] cv_agg&#39;s auc: 0.767619 + 0.00204746 [500] cv_agg&#39;s auc: 0.769264 + 0.00198502 | 5 | 0.7693 | 0.806 | 0.8085 | 47.19 | 6.819 | 186.2 | 10.37 | 35.18 | 21.44 | 50.67 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.231705 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 20299 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018600 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 20299 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017201 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 20299 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.75385 + 0.00200506 [200] cv_agg&#39;s auc: 0.762312 + 0.00169187 [300] cv_agg&#39;s auc: 0.766827 + 0.00187033 [400] cv_agg&#39;s auc: 0.769265 + 0.00182896 [500] cv_agg&#39;s auc: 0.770744 + 0.00172514 | 6 | 0.7707 | 0.8999 | 0.5994 | 31.07 | 4.228 | 219.1 | 11.43 | 15.67 | 183.4 | 63.65 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292215 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 37203 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 169 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169085 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 37203 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 169 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222382 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 37203 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 169 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.751501 + 0.00221409 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [200] cv_agg&#39;s auc: 0.761105 + 0.0020891 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [300] cv_agg&#39;s auc: 0.766057 + 0.00201994 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [400] cv_agg&#39;s auc: 0.768684 + 0.00192914 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [500] cv_agg&#39;s auc: 0.77023 + 0.00185669 | 7 | 0.7702 | 0.7404 | 0.7668 | 40.5 | 6.329 | 428.5 | 7.581 | 18.72 | 36.92 | 49.96 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166676 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 17390 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228066 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 17390 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020179 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 17390 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.753772 + 0.00271324 [200] cv_agg&#39;s auc: 0.764642 + 0.00241158 [300] cv_agg&#39;s auc: 0.768816 + 0.00221752 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [400] cv_agg&#39;s auc: 0.770854 + 0.00201828 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [500] cv_agg&#39;s auc: 0.77191 + 0.0020515 | 8 | 0.7719 | 0.8143 | 0.8885 | 3.55 | 4.612 | 184.9 | 9.342 | 4.441 | 198.5 | 57.95 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021471 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 12896 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024637 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 12896 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021677 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 12896 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.750032 + 0.00214251 [200] cv_agg&#39;s auc: 0.760098 + 0.00198386 [300] cv_agg&#39;s auc: 0.765089 + 0.00206596 [400] cv_agg&#39;s auc: 0.76748 + 0.00205654 [500] cv_agg&#39;s auc: 0.769124 + 0.00185619 | 9 | 0.7691 | 0.8068 | 0.9537 | 32.45 | 0.03474 | 133.3 | 14.63 | 4.227 | 196.7 | 46.35 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017705 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 19499 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019306 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 19499 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258748 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 19499 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.752892 + 0.00239651 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [200] cv_agg&#39;s auc: 0.762432 + 0.00238515 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [300] cv_agg&#39;s auc: 0.766805 + 0.0024799 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [400] cv_agg&#39;s auc: 0.769205 + 0.00226882 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [500] cv_agg&#39;s auc: 0.770527 + 0.00217943 | 10 | 0.7705 | 0.5315 | 0.6123 | 0.7358 | 7.801 | 209.7 | 6.136 | 16.85 | 187.7 | 39.45 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021861 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 19007 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025572 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 19007 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023354 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 19007 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.751214 + 0.002638 [200] cv_agg&#39;s auc: 0.762887 + 0.00256294 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [300] cv_agg&#39;s auc: 0.76752 + 0.00242286 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [400] cv_agg&#39;s auc: 0.769755 + 0.00229775 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [500] cv_agg&#39;s auc: 0.771039 + 0.00218057 | 11 | 0.771 | 0.5941 | 0.8808 | 3.598 | 6.108 | 203.7 | 7.426 | 11.11 | 183.6 | 38.16 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264825 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 16402 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224450 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 16402 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222576 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 16402 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 165 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.753336 + 0.00242392 [200] cv_agg&#39;s auc: 0.764042 + 0.0019804 [300] cv_agg&#39;s auc: 0.768487 + 0.00181753 [400] cv_agg&#39;s auc: 0.770666 + 0.00166117 [500] cv_agg&#39;s auc: 0.771877 + 0.00164313 | 12 | 0.7719 | 0.6576 | 0.6812 | 4.784 | 0.958 | 173.0 | 9.47 | 8.141 | 167.8 | 49.13 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220884 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 19579 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166716 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 19579 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153352 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 19579 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.752887 + 0.0024333 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [200] cv_agg&#39;s auc: 0.762859 + 0.00250737 [300] cv_agg&#39;s auc: 0.767329 + 0.00227619 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [400] cv_agg&#39;s auc: 0.7697 + 0.00213193 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [500] cv_agg&#39;s auc: 0.77114 + 0.00205365 | 13 | 0.7711 | 0.8786 | 0.6958 | 7.338 | 2.273 | 211.3 | 7.188 | 17.38 | 192.5 | 39.91 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170049 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 16894 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144410 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 16894 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235024 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 16894 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.749206 + 0.00268119 [200] cv_agg&#39;s auc: 0.760618 + 0.00261398 [300] cv_agg&#39;s auc: 0.765772 + 0.00265961 [400] cv_agg&#39;s auc: 0.76807 + 0.00249413 [500] cv_agg&#39;s auc: 0.769569 + 0.0023889 | 14 | 0.7696 | 0.6999 | 0.9419 | 16.83 | 4.11 | 179.0 | 12.26 | 10.37 | 197.7 | 31.16 | [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019978 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 16816 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022800 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 16816 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221621 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 16816 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 165 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.755504 + 0.00217257 [200] cv_agg&#39;s auc: 0.764391 + 0.00204597 [300] cv_agg&#39;s auc: 0.768847 + 0.00201129 [400] cv_agg&#39;s auc: 0.771043 + 0.00189954 [500] cv_agg&#39;s auc: 0.772446 + 0.00191014 | 15 | 0.7724 | 0.9299 | 0.5747 | 7.543 | 3.292 | 177.9 | 10.62 | 1.398 | 167.0 | 54.19 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161626 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 34319 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036407 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 34319 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275589 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 34319 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.751498 + 0.00213142 [200] cv_agg&#39;s auc: 0.76134 + 0.00190147 [300] cv_agg&#39;s auc: 0.766449 + 0.00197641 [400] cv_agg&#39;s auc: 0.768736 + 0.00198658 [500] cv_agg&#39;s auc: 0.770273 + 0.00196995 | 16 | 0.7703 | 0.8845 | 0.727 | 23.44 | 0.6011 | 390.9 | 13.53 | 29.49 | 16.74 | 42.66 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222346 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 36283 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226372 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 36283 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266766 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 36283 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.750135 + 0.00234214 [200] cv_agg&#39;s auc: 0.760181 + 0.00219461 [300] cv_agg&#39;s auc: 0.765124 + 0.00222087 [400] cv_agg&#39;s auc: 0.767708 + 0.00229372 [500] cv_agg&#39;s auc: 0.769155 + 0.0020001 | 17 | 0.7692 | 0.5286 | 0.9555 | 37.34 | 4.565 | 415.6 | 15.38 | 30.31 | 77.81 | 51.73 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020385 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 17800 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017535 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 17800 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014516 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 17800 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.756489 + 0.00222134 [200] cv_agg&#39;s auc: 0.764369 + 0.00182719 [300] cv_agg&#39;s auc: 0.768363 + 0.00176986 [400] cv_agg&#39;s auc: 0.770592 + 0.00173656 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [500] cv_agg&#39;s auc: 0.771856 + 0.00183132 | 18 | 0.7719 | 0.6943 | 0.5035 | 14.91 | 3.789 | 190.4 | 9.341 | 7.834 | 183.6 | 61.8 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221313 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 27374 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219466 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 27374 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153893 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 27374 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.751297 + 0.00232016 [200] cv_agg&#39;s auc: 0.761509 + 0.00220102 [300] cv_agg&#39;s auc: 0.76659 + 0.0021112 [400] cv_agg&#39;s auc: 0.769188 + 0.00192386 [500] cv_agg&#39;s auc: 0.77086 + 0.00163407 | 19 | 0.7709 | 0.9644 | 0.8266 | 23.37 | 0.6431 | 304.5 | 10.6 | 41.3 | 144.9 | 44.4 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028651 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 16818 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018467 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 16818 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017772 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 16818 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.755812 + 0.00217874 [200] cv_agg&#39;s auc: 0.764044 + 0.0019529 [300] cv_agg&#39;s auc: 0.768441 + 0.00186193 [400] cv_agg&#39;s auc: 0.770725 + 0.00169801 [500] cv_agg&#39;s auc: 0.77197 + 0.00163797 | 20 | 0.772 | 0.959 | 0.5257 | 10.8 | 1.577 | 177.5 | 15.64 | 3.17 | 144.9 | 56.8 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169408 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 41759 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215377 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 41759 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221710 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 41759 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.751072 + 0.00218472 [200] cv_agg&#39;s auc: 0.761439 + 0.00212142 [300] cv_agg&#39;s auc: 0.766515 + 0.00204473 [400] cv_agg&#39;s auc: 0.769194 + 0.00183068 [500] cv_agg&#39;s auc: 0.77054 + 0.00173042 | 21 | 0.7705 | 0.6587 | 0.7454 | 19.94 | 4.78 | 487.5 | 9.144 | 41.32 | 192.7 | 33.83 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021007 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 5318 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020654 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 5318 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020216 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 5318 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.750244 + 0.00221396 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [200] cv_agg&#39;s auc: 0.759543 + 0.0021569 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [300] cv_agg&#39;s auc: 0.764436 + 0.00205619 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [400] cv_agg&#39;s auc: 0.766954 + 0.0020125 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [500] cv_agg&#39;s auc: 0.768461 + 0.00193603 | 22 | 0.7685 | 0.9983 | 0.749 | 48.25 | 8.54 | 47.5 | 9.0 | 34.54 | 164.5 | 49.58 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163681 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 33376 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270704 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 33376 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225304 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 33376 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.748988 + 0.00258915 [200] cv_agg&#39;s auc: 0.758788 + 0.00222831 [300] cv_agg&#39;s auc: 0.764116 + 0.00232042 [400] cv_agg&#39;s auc: 0.76702 + 0.00218378 [500] cv_agg&#39;s auc: 0.768711 + 0.00216329 | 23 | 0.7687 | 0.9707 | 0.6446 | 36.9 | 1.905 | 378.9 | 9.577 | 47.94 | 108.4 | 25.81 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261242 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 17478 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221486 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 17478 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022389 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 17478 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.754393 + 0.00249219 [200] cv_agg&#39;s auc: 0.764248 + 0.00225796 [300] cv_agg&#39;s auc: 0.768461 + 0.00200145 [400] cv_agg&#39;s auc: 0.77067 + 0.00182478 [500] cv_agg&#39;s auc: 0.771811 + 0.00171913 | 24 | 0.7718 | 0.5754 | 0.7482 | 6.943 | 9.527 | 185.9 | 11.16 | 2.699 | 195.7 | 54.58 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264848 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 14077 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217247 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 14077 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223460 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 14077 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.754153 + 0.00222357 [200] cv_agg&#39;s auc: 0.764278 + 0.00232934 [300] cv_agg&#39;s auc: 0.768641 + 0.00214161 [400] cv_agg&#39;s auc: 0.770555 + 0.00209644 [500] cv_agg&#39;s auc: 0.771691 + 0.00197203 | 25 | 0.7717 | 0.7917 | 0.6605 | 3.41 | 8.116 | 146.5 | 10.42 | 1.418 | 102.0 | 56.14 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.267075 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 11051 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221536 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 11051 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169461 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 11051 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.750838 + 0.00256294 [200] cv_agg&#39;s auc: 0.761322 + 0.00237721 [300] cv_agg&#39;s auc: 0.766301 + 0.00235791 [400] cv_agg&#39;s auc: 0.768735 + 0.00221622 [500] cv_agg&#39;s auc: 0.77019 + 0.00218553 | 26 | 0.7702 | 0.5176 | 0.6145 | 4.366 | 2.571 | 111.4 | 6.679 | 6.432 | 86.05 | 29.8 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023574 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 18842 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024873 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 18842 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024455 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 18842 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.752761 + 0.00253328 [200] cv_agg&#39;s auc: 0.764295 + 0.00214627 [300] cv_agg&#39;s auc: 0.768268 + 0.00157287 [400] cv_agg&#39;s auc: 0.769987 + 0.00150729 [500] cv_agg&#39;s auc: 0.770816 + 0.00144921 | 27 | 0.7708 | 1.0 | 1.0 | 0.01 | 0.001 | 201.9 | 16.0 | 1.0 | 189.9 | 64.0 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022653 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 14825 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024379 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 14825 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022400 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 14825 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.753842 + 0.00219322 [200] cv_agg&#39;s auc: 0.764597 + 0.00227203 [300] cv_agg&#39;s auc: 0.768912 + 0.00188056 [400] cv_agg&#39;s auc: 0.770975 + 0.00181401 [500] cv_agg&#39;s auc: 0.772 + 0.001822 | 28 | 0.772 | 0.6115 | 0.9252 | 6.227 | 9.844 | 153.7 | 9.391 | 4.001 | 131.8 | 61.4 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018247 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 15329 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018926 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 15329 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021225 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 15329 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.755952 + 0.00217505 [200] cv_agg&#39;s auc: 0.764695 + 0.00210075 [300] cv_agg&#39;s auc: 0.769001 + 0.00207877 [400] cv_agg&#39;s auc: 0.770975 + 0.00221316 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [500] cv_agg&#39;s auc: 0.772213 + 0.00214666 | 29 | 0.7722 | 0.6475 | 0.5026 | 0.8679 | 2.108 | 159.8 | 9.651 | 45.07 | 121.8 | 60.14 | . C: Users channee anaconda3 lib site-packages lightgbm engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) C: Users channee anaconda3 lib site-packages lightgbm engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument _log_warning(&#34;Found `{}` in params. Will use it instead of argument&#34;.format(alias)) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020594 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 15248 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020154 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 15248 [LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016801 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 15248 [LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [LightGBM] [Info] Start training from score 0.080729 [100] cv_agg&#39;s auc: 0.755049 + 0.00200925 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [200] cv_agg&#39;s auc: 0.762787 + 0.00190172 [300] cv_agg&#39;s auc: 0.767045 + 0.00186358 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [400] cv_agg&#39;s auc: 0.769573 + 0.00165805 [500] cv_agg&#39;s auc: 0.770932 + 0.00161966 | 30 | 0.7709 | 0.9025 | 0.5039 | 26.8 | 6.035 | 159.5 | 9.297 | 36.25 | 103.7 | 61.16 | ===================================================================================================================================== . lgbBO.res . [{&#39;target&#39;: 0.7704130128940919, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.7744067519636624, &#39;feature_fraction&#39;: 0.8575946831862098, &#39;lambda_l1&#39;: 30.142141169821482, &#39;lambda_l2&#39;: 5.449286946785972, &#39;max_bin&#39;: 217.5908516760633, &#39;max_depth&#39;: 12.458941130666561, &#39;min_child_weight&#39;: 22.441773351871934, &#39;min_data_in_leaf&#39;: 179.43687014859515, &#39;num_leaves&#39;: 62.54651042004117}}, {&#39;target&#39;: 0.7707320615580647, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.6917207594128889, &#39;feature_fraction&#39;: 0.8958625190413323, &#39;lambda_l1&#39;: 26.449457038447697, &#39;lambda_l2&#39;: 5.680877566378229, &#39;max_bin&#39;: 463.5423527634039, &#39;max_depth&#39;: 6.710360581978869, &#39;min_child_weight&#39;: 5.269335685375495, &#39;min_data_in_leaf&#39;: 13.841495513661886, &#39;num_leaves&#39;: 57.30479382191752}}, {&#39;target&#39;: 0.7674060125927115, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.8890783754749252, &#39;feature_fraction&#39;: 0.9350060741234096, &#39;lambda_l1&#39;: 48.93113092821587, &#39;lambda_l2&#39;: 7.99178648360302, &#39;max_bin&#39;: 236.1248875039366, &#39;max_depth&#39;: 13.805291762864554, &#39;min_child_weight&#39;: 6.795446867577728, &#39;min_data_in_leaf&#39;: 131.58499405222955, &#39;num_leaves&#39;: 29.734131496361854}}, {&#39;target&#39;: 0.7706174479634317, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.972334458524792, &#39;feature_fraction&#39;: 0.7609241608750359, &#39;lambda_l1&#39;: 20.738950380126276, &#39;lambda_l2&#39;: 2.646291565434165, &#39;max_bin&#39;: 389.3745078227662, &#39;max_depth&#39;: 10.561503322165485, &#39;min_child_weight&#39;: 28.853263494563777, &#39;min_data_in_leaf&#39;: 13.570062082907477, &#39;num_leaves&#39;: 48.705419883035084}}, {&#39;target&#39;: 0.7692640265604144, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.8060478613612108, &#39;feature_fraction&#39;: 0.8084669984373785, &#39;lambda_l1&#39;: 47.18796644494606, &#39;lambda_l2&#39;: 6.818521170735731, &#39;max_bin&#39;: 186.15887128115514, &#39;max_depth&#39;: 10.370319537993414, &#39;min_child_weight&#39;: 35.183928600435976, &#39;min_data_in_leaf&#39;: 21.44283960956127, &#39;num_leaves&#39;: 50.670668617826706}}, {&#39;target&#39;: 0.7707441174483378, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.8999235532397036, &#39;feature_fraction&#39;: 0.5994369290340391, &#39;lambda_l1&#39;: 31.074749286460722, &#39;lambda_l2&#39;: 4.228074498468137, &#39;max_bin&#39;: 219.10229236574725, &#39;max_depth&#39;: 11.427010587833749, &#39;min_child_weight&#39;: 15.667080789571736, &#39;min_data_in_leaf&#39;: 183.41711356263576, &#39;num_leaves&#39;: 63.64711067026588}}, {&#39;target&#39;: 0.770229710649176, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.7404321772522139, &#39;feature_fraction&#39;: 0.7668315626022122, &#39;lambda_l1&#39;: 40.496397840921915, &#39;lambda_l2&#39;: 6.329038251811815, &#39;max_bin&#39;: 428.47974892655657, &#39;max_depth&#39;: 7.581108392237857, &#39;min_child_weight&#39;: 18.721900794608175, &#39;min_data_in_leaf&#39;: 36.91555073964587, &#39;num_leaves&#39;: 49.96227208627961}}, {&#39;target&#39;: 0.7719096310120473, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.8143033572205088, &#39;feature_fraction&#39;: 0.8884974616228736, &#39;lambda_l1&#39;: 3.5504085208024487, &#39;lambda_l2&#39;: 4.611830458102663, &#39;max_bin&#39;: 184.9398381966357, &#39;max_depth&#39;: 9.34172001478695, &#39;min_child_weight&#39;: 4.440733035446478, &#39;min_data_in_leaf&#39;: 198.5155850624426, &#39;num_leaves&#39;: 57.94763994734239}}, {&#39;target&#39;: 0.7691251381694691, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.8067814387656114, &#39;feature_fraction&#39;: 0.9536571212037808, &#39;lambda_l1&#39;: 32.4521675349507, &#39;lambda_l2&#39;: 0.034735067421668264, &#39;max_bin&#39;: 133.30349225834965, &#39;max_depth&#39;: 14.62576290350622, &#39;min_child_weight&#39;: 4.227256072865901, &#39;min_data_in_leaf&#39;: 196.70462703040053, &#39;num_leaves&#39;: 46.351397650073146}}, {&#39;target&#39;: 0.7705267701934897, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.5315401804588187, &#39;feature_fraction&#39;: 0.612303641148811, &#39;lambda_l1&#39;: 0.7358255076011369, &#39;lambda_l2&#39;: 7.800929367645439, &#39;max_bin&#39;: 209.6908259343996, &#39;max_depth&#39;: 6.135772933697576, &#39;min_child_weight&#39;: 16.849896162166722, &#39;min_data_in_leaf&#39;: 187.67343516011584, &#39;num_leaves&#39;: 39.44552475790631}}, {&#39;target&#39;: 0.7710389366063817, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.5940850954373089, &#39;feature_fraction&#39;: 0.8807926141911452, &#39;lambda_l1&#39;: 3.5979735041387433, &#39;lambda_l2&#39;: 6.108496637444428, &#39;max_bin&#39;: 203.72096015715022, &#39;max_depth&#39;: 7.425928691480058, &#39;min_child_weight&#39;: 11.111565381111847, &#39;min_data_in_leaf&#39;: 183.58886753504038, &#39;num_leaves&#39;: 38.16261823863786}}, {&#39;target&#39;: 0.7718773672753726, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.657614492893178, &#39;feature_fraction&#39;: 0.6811813427672369, &#39;lambda_l1&#39;: 4.783749362922549, &#39;lambda_l2&#39;: 0.957997706400652, &#39;max_bin&#39;: 173.03662669577741, &#39;max_depth&#39;: 9.470274418940324, &#39;min_child_weight&#39;: 8.141437429408178, &#39;min_data_in_leaf&#39;: 167.75970713753094, &#39;num_leaves&#39;: 49.13015644124545}}, {&#39;target&#39;: 0.7711454074315656, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.8786355335852942, &#39;feature_fraction&#39;: 0.6958187117248162, &#39;lambda_l1&#39;: 7.338172390732913, &#39;lambda_l2&#39;: 2.272677319027393, &#39;max_bin&#39;: 211.27555567684178, &#39;max_depth&#39;: 7.187955460578591, &#39;min_child_weight&#39;: 17.382076324484828, &#39;min_data_in_leaf&#39;: 192.54729357889843, &#39;num_leaves&#39;: 39.9125263869441}}, {&#39;target&#39;: 0.7695691149218623, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.6998973669239097, &#39;feature_fraction&#39;: 0.9419379137870297, &#39;lambda_l1&#39;: 16.828686125736517, &#39;lambda_l2&#39;: 4.109532131663239, &#39;max_bin&#39;: 178.95633418021208, &#39;max_depth&#39;: 12.259221410586013, &#39;min_child_weight&#39;: 10.36621015246892, &#39;min_data_in_leaf&#39;: 197.6663134909162, &#39;num_leaves&#39;: 31.1594353080567}}, {&#39;target&#39;: 0.7724457191365559, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.9299169469599182, &#39;feature_fraction&#39;: 0.5746809011453164, &#39;lambda_l1&#39;: 7.543441207233135, &#39;lambda_l2&#39;: 3.292195078359306, &#39;max_bin&#39;: 177.94521484058205, &#39;max_depth&#39;: 10.61966768561966, &#39;min_child_weight&#39;: 1.397663097770165, &#39;min_data_in_leaf&#39;: 166.95618212603793, &#39;num_leaves&#39;: 54.18985368611588}}, {&#39;target&#39;: 0.7702728801817355, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.8844562637903797, &#39;feature_fraction&#39;: 0.7269989688983634, &#39;lambda_l1&#39;: 23.43573646623273, &#39;lambda_l2&#39;: 0.601050164341131, &#39;max_bin&#39;: 390.8777598211938, &#39;max_depth&#39;: 13.531193778598972, &#39;min_child_weight&#39;: 29.486064774397775, &#39;min_data_in_leaf&#39;: 16.738983494464513, &#39;num_leaves&#39;: 42.66429176206504}}, {&#39;target&#39;: 0.7691546772195966, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.5285839690801153, &#39;feature_fraction&#39;: 0.9554784176155467, &#39;lambda_l1&#39;: 37.33730516793895, &#39;lambda_l2&#39;: 4.564719396185809, &#39;max_bin&#39;: 415.6458449914903, &#39;max_depth&#39;: 15.378391843299484, &#39;min_child_weight&#39;: 30.31421249555068, &#39;min_data_in_leaf&#39;: 77.81456840599282, &#39;num_leaves&#39;: 51.72987878906692}}, {&#39;target&#39;: 0.7718560657970831, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.6943325913373878, &#39;feature_fraction&#39;: 0.5034545166040366, &#39;lambda_l1&#39;: 14.911628058528853, &#39;lambda_l2&#39;: 3.7888209125712415, &#39;max_bin&#39;: 190.37970682243528, &#39;max_depth&#39;: 9.340892785972903, &#39;min_child_weight&#39;: 7.833820755151955, &#39;min_data_in_leaf&#39;: 183.59628053956956, &#39;num_leaves&#39;: 61.80346524240569}}, {&#39;target&#39;: 0.7708603583870698, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.9644030062523238, &#39;feature_fraction&#39;: 0.8266253858261036, &#39;lambda_l1&#39;: 23.3655535588359, &#39;lambda_l2&#39;: 0.6430770731307863, &#39;max_bin&#39;: 304.49321845606374, &#39;max_depth&#39;: 10.603479817877357, &#39;min_child_weight&#39;: 41.299919645601506, &#39;min_data_in_leaf&#39;: 144.92832924801024, &#39;num_leaves&#39;: 44.39976983311924}}, {&#39;target&#39;: 0.7719703571205899, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.958982625395157, &#39;feature_fraction&#39;: 0.5256535956136048, &#39;lambda_l1&#39;: 10.796315561196417, &#39;lambda_l2&#39;: 1.576964383196764, &#39;max_bin&#39;: 177.52508011858072, &#39;max_depth&#39;: 15.636191927411867, &#39;min_child_weight&#39;: 3.1704301617341315, &#39;min_data_in_leaf&#39;: 144.90957631393655, &#39;num_leaves&#39;: 56.801621269683345}}, {&#39;target&#39;: 0.77054013792762, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.6587283396471605, &#39;feature_fraction&#39;: 0.7454373863647559, &#39;lambda_l1&#39;: 19.94077731253563, &#39;lambda_l2&#39;: 4.780423949040658, &#39;max_bin&#39;: 487.5445679344283, &#39;max_depth&#39;: 9.144439579692584, &#39;min_child_weight&#39;: 41.323096983850455, &#39;min_data_in_leaf&#39;: 192.74672138313088, &#39;num_leaves&#39;: 33.83493453930372}}, {&#39;target&#39;: 0.7684607098023677, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.9982660890256111, &#39;feature_fraction&#39;: 0.7490195259582262, &#39;lambda_l1&#39;: 48.254390571730376, &#39;lambda_l2&#39;: 8.539953109394675, &#39;max_bin&#39;: 47.49826121182029, &#39;max_depth&#39;: 8.999627090062027, &#39;min_child_weight&#39;: 34.5372274617776, &#39;min_data_in_leaf&#39;: 164.45701046555462, &#39;num_leaves&#39;: 49.57867373971419}}, {&#39;target&#39;: 0.7687114634910718, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.9707101482347569, &#39;feature_fraction&#39;: 0.6446304084751359, &#39;lambda_l1&#39;: 36.895675572499385, &#39;lambda_l2&#39;: 1.9051562879071906, &#39;max_bin&#39;: 378.88812663146865, &#39;max_depth&#39;: 9.576771523517493, &#39;min_child_weight&#39;: 47.93924951072747, &#39;min_data_in_leaf&#39;: 108.42458883918025, &#39;num_leaves&#39;: 25.81430654629507}}, {&#39;target&#39;: 0.7718111662970083, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.5754069027269129, &#39;feature_fraction&#39;: 0.7482186706912217, &#39;lambda_l1&#39;: 6.943057799845263, &#39;lambda_l2&#39;: 9.52747316232688, &#39;max_bin&#39;: 185.8683083359433, &#39;max_depth&#39;: 11.15539259574, &#39;min_child_weight&#39;: 2.698545324998308, &#39;min_data_in_leaf&#39;: 195.68310244680038, &#39;num_leaves&#39;: 54.58265366779938}}, {&#39;target&#39;: 0.7716905284261153, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.791661982638599, &#39;feature_fraction&#39;: 0.6605051402067922, &#39;lambda_l1&#39;: 3.4104212745018185, &#39;lambda_l2&#39;: 8.115945070358096, &#39;max_bin&#39;: 146.54012416035053, &#39;max_depth&#39;: 10.417648175230251, &#39;min_child_weight&#39;: 1.4180074553193076, &#39;min_data_in_leaf&#39;: 102.01504347366142, &#39;num_leaves&#39;: 56.140917586018034}}, {&#39;target&#39;: 0.7701900669922322, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.5176386777761051, &#39;feature_fraction&#39;: 0.6145074911288368, &#39;lambda_l1&#39;: 4.366000163341913, &#39;lambda_l2&#39;: 2.5707335917583762, &#39;max_bin&#39;: 111.39585114915086, &#39;max_depth&#39;: 6.678620073549007, &#39;min_child_weight&#39;: 6.431766588419492, &#39;min_data_in_leaf&#39;: 86.05285708510192, &#39;num_leaves&#39;: 29.799311383594773}}, {&#39;target&#39;: 0.7708307863985207, &#39;params&#39;: {&#39;bagging_fraction&#39;: 1.0, &#39;feature_fraction&#39;: 1.0, &#39;lambda_l1&#39;: 0.01, &#39;lambda_l2&#39;: 0.001, &#39;max_bin&#39;: 201.87175711924215, &#39;max_depth&#39;: 16.0, &#39;min_child_weight&#39;: 1.0, &#39;min_data_in_leaf&#39;: 189.8553861436057, &#39;num_leaves&#39;: 64.0}}, {&#39;target&#39;: 0.7719997542250262, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.6114748246676402, &#39;feature_fraction&#39;: 0.9252070326413618, &#39;lambda_l1&#39;: 6.227021904304498, &#39;lambda_l2&#39;: 9.844477214259657, &#39;max_bin&#39;: 153.69685844257907, &#39;max_depth&#39;: 9.391343257415311, &#39;min_child_weight&#39;: 4.001203263458588, &#39;min_data_in_leaf&#39;: 131.7722947221485, &#39;num_leaves&#39;: 61.40198317680489}}, {&#39;target&#39;: 0.7722128809560068, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.6475157210105189, &#39;feature_fraction&#39;: 0.5026014053686318, &#39;lambda_l1&#39;: 0.8678952276657708, &#39;lambda_l2&#39;: 2.108427218491435, &#39;max_bin&#39;: 159.7514426083027, &#39;max_depth&#39;: 9.651405177349304, &#39;min_child_weight&#39;: 45.072924694401834, &#39;min_data_in_leaf&#39;: 121.77348343784082, &#39;num_leaves&#39;: 60.13997800642714}}, {&#39;target&#39;: 0.7709315329107215, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.9024808966462303, &#39;feature_fraction&#39;: 0.5038753407648007, &#39;lambda_l1&#39;: 26.801718133436975, &#39;lambda_l2&#39;: 6.034542815576746, &#39;max_bin&#39;: 159.45233398204377, &#39;max_depth&#39;: 9.296591640023367, &#39;min_child_weight&#39;: 36.24576576521278, &#39;min_data_in_leaf&#39;: 103.70752753815785, &#39;num_leaves&#39;: 61.159466432090326}}] . target_list = [] for result in lgbBO.res: target = result[&#39;target&#39;] target_list.append(target) print(target_list) # 가장 큰 target 값을 가지는 순번(index)를 추출 print(&#39;maximum target index:&#39;, np.argmax(np.array(target_list))) . [0.7704130128940919, 0.7707320615580647, 0.7674060125927115, 0.7706174479634317, 0.7692640265604144, 0.7707441174483378, 0.770229710649176, 0.7719096310120473, 0.7691251381694691, 0.7705267701934897, 0.7710389366063817, 0.7718773672753726, 0.7711454074315656, 0.7695691149218623, 0.7724457191365559, 0.7702728801817355, 0.7691546772195966, 0.7718560657970831, 0.7708603583870698, 0.7719703571205899, 0.77054013792762, 0.7684607098023677, 0.7687114634910718, 0.7718111662970083, 0.7716905284261153, 0.7701900669922322, 0.7708307863985207, 0.7719997542250262, 0.7722128809560068, 0.7709315329107215] maximum target index: 14 . max_dict = lgbBO.res[np.argmax(np.array(target_list))] print(max_dict) . {&#39;target&#39;: 0.7724457191365559, &#39;params&#39;: {&#39;bagging_fraction&#39;: 0.9299169469599182, &#39;feature_fraction&#39;: 0.5746809011453164, &#39;lambda_l1&#39;: 7.543441207233135, &#39;lambda_l2&#39;: 3.292195078359306, &#39;max_bin&#39;: 177.94521484058205, &#39;max_depth&#39;: 10.61966768561966, &#39;min_child_weight&#39;: 1.397663097770165, &#39;min_data_in_leaf&#39;: 166.95618212603793, &#39;num_leaves&#39;: 54.18985368611588}} . ftr_app = apps_all_train.drop([&#39;SK_ID_CURR&#39;, &#39;TARGET&#39;], axis=1) target_app = apps_all_train[&#39;TARGET&#39;] X_train, X_test, y_train, y_test = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020) print(&#39;train shape:&#39;, train_x.shape, &#39;valid shape:&#39;, valid_x.shape) lgbm_wrapper = LGBMClassifier( n_jobs=-1, n_estimators=1000, learning_rate=0.02, max_depth = 11, num_leaves=54, colsample_bytree=0.574, subsample=0.930, max_bin=403, reg_alpha=7.543, reg_lambda=3.292, min_child_weight=1, min_child_samples=167, silent=-1, verbose=-1, ) evals = [(X_test, y_test)] lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=evals, verbose=100) preds = lgbm_wrapper.predict(X_test) pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1] . train shape: (215257, 174) valid shape: (92254, 174) Training until validation scores don&#39;t improve for 100 rounds [100] valid_0&#39;s auc: 0.759222 valid_0&#39;s binary_logloss: 0.248113 [200] valid_0&#39;s auc: 0.769117 valid_0&#39;s binary_logloss: 0.24319 [300] valid_0&#39;s auc: 0.773816 valid_0&#39;s binary_logloss: 0.241396 [400] valid_0&#39;s auc: 0.77635 valid_0&#39;s binary_logloss: 0.24053 [500] valid_0&#39;s auc: 0.777994 valid_0&#39;s binary_logloss: 0.239917 [600] valid_0&#39;s auc: 0.77869 valid_0&#39;s binary_logloss: 0.239651 [700] valid_0&#39;s auc: 0.779292 valid_0&#39;s binary_logloss: 0.239452 [800] valid_0&#39;s auc: 0.779592 valid_0&#39;s binary_logloss: 0.239369 [900] valid_0&#39;s auc: 0.77968 valid_0&#39;s binary_logloss: 0.239334 [1000] valid_0&#39;s auc: 0.779873 valid_0&#39;s binary_logloss: 0.239287 Did not meet early stopping. Best iteration is: [998] valid_0&#39;s auc: 0.779896 valid_0&#39;s binary_logloss: 0.239281 . get_clf_eval(y_test, preds, pred_proba) . 오차 행렬 [[84646 187] [ 7200 221]] 정확도: 0.9199, 정밀도: 0.5417, 재현율: 0.0298, F1: 0.0565, AUC:0.7799 . roc_curve_plot(y_test, pred_proba) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-09-01T10:27:44.518246 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/",
            "url": "https://gwonchankim.github.io/channee/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_4.html",
            "relUrl": "/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_4.html",
            "date": " • Aug 31, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Project2-3",
            "content": "pandas groupby &#45800;&#51068; aggregation &#54632;&#49688; &#49324;&#50857; . groupby SK_ID_CURR | SK_ID_CURR별 건수, AMT_CREDIT에 대한 평균, 최대 값 | . prev.groupby(&#39;SK_ID_CURR&#39;) . &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000027305E02EB0&gt; . prev_group= prev.groupby(&#39;SK_ID_CURR&#39;) # DataFrameGroupby 객체에 aggregation함수 수행 결과를 저장한 DataFrame 생성 및 aggregation값 저장. prev_agg = pd.DataFrame() prev_agg[&#39;CNT&#39;] = prev_group[&#39;SK_ID_CURR&#39;].count() prev_agg[&#39;AVG_CREDIT&#39;] = prev_group[&#39;AMT_CREDIT&#39;].mean() prev_agg[&#39;MAX_CREDIT&#39;] = prev_group[&#39;AMT_CREDIT&#39;].max() # groupby 컬럼값이 DataFrame의 Index가 됨. 컬럼으로 변환하려면 reset_index()로 변환 필요. prev_agg.head(10) . CNT AVG_CREDIT MAX_CREDIT . SK_ID_CURR . 100001 1 | 23787.000000 | 23787.0 | . 100002 1 | 179055.000000 | 179055.0 | . 100003 3 | 484191.000000 | 1035882.0 | . 100004 1 | 20106.000000 | 20106.0 | . 100005 2 | 20076.750000 | 40153.5 | . 100006 9 | 291695.500000 | 906615.0 | . 100007 6 | 166638.750000 | 284400.0 | . 100008 5 | 162767.700000 | 501975.0 | . 100009 7 | 70137.642857 | 98239.5 | . 100010 1 | 260811.000000 | 260811.0 | . groupby agg()&#54632;&#49688;&#47484; &#51060;&#50857;&#54616;&#50668; &#50668;&#47084;&#44060;&#51032; aggregation &#54632;&#49688; &#51201;&#50857; . prev_group = prev.groupby(&#39;SK_ID_CURR&#39;) # DataFrameGroupby의 agg() 함수를 이용하여 여러개의 aggregation 함수 적용 prev_agg1 = prev_group[&#39;AMT_CREDIT&#39;].agg([&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;]) prev_agg2 = prev_group[&#39;AMT_ANNUITY&#39;].agg([&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;]) # merge를 이용하여 두개의 DataFrame 결합. prev_agg = prev_agg1.merge(prev_agg2, on=&#39;SK_ID_CURR&#39;, how=&#39;inner&#39;) prev_agg.head(10) . mean_x max_x sum_x mean_y max_y sum_y . SK_ID_CURR . 100001 23787.000000 | 23787.0 | 23787.0 | 3951.000000 | 3951.000 | 3951.000 | . 100002 179055.000000 | 179055.0 | 179055.0 | 9251.775000 | 9251.775 | 9251.775 | . 100003 484191.000000 | 1035882.0 | 1452573.0 | 56553.990000 | 98356.995 | 169661.970 | . 100004 20106.000000 | 20106.0 | 20106.0 | 5357.250000 | 5357.250 | 5357.250 | . 100005 20076.750000 | 40153.5 | 40153.5 | 4813.200000 | 4813.200 | 4813.200 | . 100006 291695.500000 | 906615.0 | 2625259.5 | 23651.175000 | 39954.510 | 141907.050 | . 100007 166638.750000 | 284400.0 | 999832.5 | 12278.805000 | 22678.785 | 73672.830 | . 100008 162767.700000 | 501975.0 | 813838.5 | 15839.696250 | 25309.575 | 63358.785 | . 100009 70137.642857 | 98239.5 | 490963.5 | 10051.412143 | 17341.605 | 70359.885 | . 100010 260811.000000 | 260811.0 | 260811.0 | 27463.410000 | 27463.410 | 27463.410 | . agg()&#50640; dictionary&#47484; &#51060;&#50857;&#54616;&#50668; groupby &#51201;&#50857; . agg_dict = { &#39;SK_ID_CURR&#39;:[&#39;count&#39;], &#39;AMT_CREDIT&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_ANNUITY&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_APPLICATION&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_DOWN_PAYMENT&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_GOODS_PRICE&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;] } prev_group = prev.groupby(&#39;SK_ID_CURR&#39;) prev_amt_agg = prev_group.agg(agg_dict) prev_amt_agg.head() . SK_ID_CURR AMT_CREDIT AMT_ANNUITY AMT_APPLICATION AMT_DOWN_PAYMENT AMT_GOODS_PRICE . count mean max sum mean max sum mean max sum mean max sum mean max sum . SK_ID_CURR . 100001 1 | 23787.00 | 23787.0 | 23787.0 | 3951.000 | 3951.000 | 3951.000 | 24835.50 | 24835.5 | 24835.5 | 2520.0 | 2520.0 | 2520.0 | 24835.5 | 24835.5 | 24835.5 | . 100002 1 | 179055.00 | 179055.0 | 179055.0 | 9251.775 | 9251.775 | 9251.775 | 179055.00 | 179055.0 | 179055.0 | 0.0 | 0.0 | 0.0 | 179055.0 | 179055.0 | 179055.0 | . 100003 3 | 484191.00 | 1035882.0 | 1452573.0 | 56553.990 | 98356.995 | 169661.970 | 435436.50 | 900000.0 | 1306309.5 | 3442.5 | 6885.0 | 6885.0 | 435436.5 | 900000.0 | 1306309.5 | . 100004 1 | 20106.00 | 20106.0 | 20106.0 | 5357.250 | 5357.250 | 5357.250 | 24282.00 | 24282.0 | 24282.0 | 4860.0 | 4860.0 | 4860.0 | 24282.0 | 24282.0 | 24282.0 | . 100005 2 | 20076.75 | 40153.5 | 40153.5 | 4813.200 | 4813.200 | 4813.200 | 22308.75 | 44617.5 | 44617.5 | 4464.0 | 4464.0 | 4464.0 | 44617.5 | 44617.5 | 44617.5 | . grouby agg &#47196; &#47564;&#46308;&#50612;&#51652; Multi index &#52972;&#47100; &#48320;&#44221;. . MultiIndex로 되어 있는 컬럼명 확인 | MultiIndex 컬럼명을 _로 연결하여 컬럼명 변경. | . prev_amt_agg.columns . MultiIndex([( &#39;SK_ID_CURR&#39;, &#39;count&#39;), ( &#39;AMT_CREDIT&#39;, &#39;mean&#39;), ( &#39;AMT_CREDIT&#39;, &#39;max&#39;), ( &#39;AMT_CREDIT&#39;, &#39;sum&#39;), ( &#39;AMT_ANNUITY&#39;, &#39;mean&#39;), ( &#39;AMT_ANNUITY&#39;, &#39;max&#39;), ( &#39;AMT_ANNUITY&#39;, &#39;sum&#39;), ( &#39;AMT_APPLICATION&#39;, &#39;mean&#39;), ( &#39;AMT_APPLICATION&#39;, &#39;max&#39;), ( &#39;AMT_APPLICATION&#39;, &#39;sum&#39;), (&#39;AMT_DOWN_PAYMENT&#39;, &#39;mean&#39;), (&#39;AMT_DOWN_PAYMENT&#39;, &#39;max&#39;), (&#39;AMT_DOWN_PAYMENT&#39;, &#39;sum&#39;), ( &#39;AMT_GOODS_PRICE&#39;, &#39;mean&#39;), ( &#39;AMT_GOODS_PRICE&#39;, &#39;max&#39;), ( &#39;AMT_GOODS_PRICE&#39;, &#39;sum&#39;)], ) . prev_amt_agg.columns = [&quot;PREV_&quot;+&quot;_&quot;.join(x).upper() for x in prev_amt_agg.columns.ravel()] . prev_amt_agg.head() . PREV_SK_ID_CURR_COUNT PREV_AMT_CREDIT_MEAN PREV_AMT_CREDIT_MAX PREV_AMT_CREDIT_SUM PREV_AMT_ANNUITY_MEAN PREV_AMT_ANNUITY_MAX PREV_AMT_ANNUITY_SUM PREV_AMT_APPLICATION_MEAN PREV_AMT_APPLICATION_MAX PREV_AMT_APPLICATION_SUM PREV_AMT_DOWN_PAYMENT_MEAN PREV_AMT_DOWN_PAYMENT_MAX PREV_AMT_DOWN_PAYMENT_SUM PREV_AMT_GOODS_PRICE_MEAN PREV_AMT_GOODS_PRICE_MAX PREV_AMT_GOODS_PRICE_SUM . SK_ID_CURR . 100001 1 | 23787.00 | 23787.0 | 23787.0 | 3951.000 | 3951.000 | 3951.000 | 24835.50 | 24835.5 | 24835.5 | 2520.0 | 2520.0 | 2520.0 | 24835.5 | 24835.5 | 24835.5 | . 100002 1 | 179055.00 | 179055.0 | 179055.0 | 9251.775 | 9251.775 | 9251.775 | 179055.00 | 179055.0 | 179055.0 | 0.0 | 0.0 | 0.0 | 179055.0 | 179055.0 | 179055.0 | . 100003 3 | 484191.00 | 1035882.0 | 1452573.0 | 56553.990 | 98356.995 | 169661.970 | 435436.50 | 900000.0 | 1306309.5 | 3442.5 | 6885.0 | 6885.0 | 435436.5 | 900000.0 | 1306309.5 | . 100004 1 | 20106.00 | 20106.0 | 20106.0 | 5357.250 | 5357.250 | 5357.250 | 24282.00 | 24282.0 | 24282.0 | 4860.0 | 4860.0 | 4860.0 | 24282.0 | 24282.0 | 24282.0 | . 100005 2 | 20076.75 | 40153.5 | 40153.5 | 4813.200 | 4813.200 | 4813.200 | 22308.75 | 44617.5 | 44617.5 | 4464.0 | 4464.0 | 4464.0 | 44617.5 | 44617.5 | 44617.5 | . prev &#54588;&#52376; &#44032;&#44277;. &#45824;&#52636; &#49888;&#52397;&#50529; &#45824;&#48708; &#45796;&#47480; &#44552;&#50529; &#52264;&#51060; &#48143; &#48708;&#50984; &#49373;&#49457;. . prev[&#39;PREV_CREDIT_DIFF&#39;] = prev[&#39;AMT_APPLICATION&#39;] - prev[&#39;AMT_CREDIT&#39;] prev[&#39;PREV_GOODS_DIFF&#39;] = prev[&#39;AMT_APPLICATION&#39;] - prev[&#39;AMT_GOODS_PRICE&#39;] prev[&#39;PREV_CREDIT_APPL_RATIO&#39;] = prev[&#39;AMT_CREDIT&#39;]/prev[&#39;AMT_APPLICATION&#39;] prev[&#39;PREV_ANNUITY_APPL_RATIO&#39;] = prev[&#39;AMT_ANNUITY&#39;]/prev[&#39;AMT_APPLICATION&#39;] prev[&#39;PREV_GOODS_APPL_RATIO&#39;] = prev[&#39;AMT_GOODS_PRICE&#39;]/prev[&#39;AMT_APPLICATION&#39;] . DAYS_XXX &#54588;&#52376;&#51032; 365243 &#51012; NULL&#47196; &#48320;&#54872;&#54616;&#44256;, &#52395;&#48264;&#51704; &#47564;&#44592;&#51068;&#44284; &#47560;&#51648;&#47561; &#47564;&#44592;&#51068;&#44620;&#51648;&#51032; &#44592;&#44036; &#44032;&#44277; . prev[&#39;DAYS_FIRST_DRAWING&#39;].replace(365243, np.nan, inplace= True) prev[&#39;DAYS_FIRST_DUE&#39;].replace(365243, np.nan, inplace= True) prev[&#39;DAYS_LAST_DUE_1ST_VERSION&#39;].replace(365243, np.nan, inplace= True) prev[&#39;DAYS_LAST_DUE&#39;].replace(365243, np.nan, inplace= True) prev[&#39;DAYS_TERMINATION&#39;].replace(365243, np.nan, inplace= True) # 첫번째 만기일과 마지막 만기일까지의 기간 prev[&#39;PREV_DAYS_LAST_DUE_DIFF&#39;] = prev[&#39;DAYS_LAST_DUE_1ST_VERSION&#39;] - prev[&#39;DAYS_LAST_DUE&#39;] . &#44592;&#51316; &#51060;&#51088;&#50984; &#44288;&#47144; &#52972;&#47100;&#51060; null&#51060; &#47566;&#50500;&#49436; &#49352;&#47213;&#44172; &#44036;&#45800;&#54620; &#51060;&#51088;&#50984;&#51012; &#45824;&#52636; &#44552;&#50529;&#44284; &#45824;&#52636; &#44552;&#50529; &#45225;&#48512; &#54943;&#49688;&#47484; &#44592;&#48152;&#51004;&#47196; &#44228;&#49328; . all_pay = prev[&#39;AMT_ANNUITY&#39;] * prev[&#39;CNT_PAYMENT&#39;] # 전체 납부 금액 대비 AMT_CREDIT 비율을 구하고 여기에 다시 납부횟수로 나누어서 이자율 계산. prev[&#39;PREV_INTERESTS_RATE&#39;] = (all_pay/prev[&#39;AMT_CREDIT&#39;] - 1)/prev[&#39;CNT_PAYMENT&#39;] . prev.iloc[:, -7:].head(10) . PREV_CREDIT_DIFF PREV_GOODS_DIFF PREV_CREDIT_APPL_RATIO PREV_ANNUITY_APPL_RATIO PREV_GOODS_APPL_RATIO PREV_DAYS_LAST_DUE_DIFF PREV_INTERESTS_RATE . 0 0.0 | 0.0 | 1.000000 | 0.100929 | 1.0 | 342.0 | 0.017596 | . 1 -72171.0 | 0.0 | 1.118800 | 0.041463 | 1.0 | NaN | 0.009282 | . 2 -23944.5 | 0.0 | 1.212840 | 0.133873 | 1.0 | NaN | 0.027047 | . 3 -20790.0 | 0.0 | 1.046200 | 0.104536 | 1.0 | 30.0 | 0.016587 | . 4 -66555.0 | 0.0 | 1.197200 | 0.094591 | 1.0 | NaN | 0.037343 | . 5 -25573.5 | 0.0 | 1.081186 | 0.075251 | 1.0 | 0.0 | 0.014044 | . 6 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 7 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 8 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 9 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | . &#44592;&#51316; &#45824;&#52636; &#44552;&#50529;, &#45824;&#52636; &#49345;&#53468; &#44288;&#47144; &#54588;&#52376;&#46308;&#44284; &#51060;&#46308;&#51012; &#44032;&#44277;&#54616;&#50668; &#47564;&#46308;&#50612;&#51652; &#49352;&#47196;&#50868; &#52972;&#47100;&#46308;&#47196; aggregation &#49688;&#54665;. . agg_dict = { # 기존 컬럼. &#39;SK_ID_CURR&#39;:[&#39;count&#39;], &#39;AMT_CREDIT&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_ANNUITY&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_APPLICATION&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_DOWN_PAYMENT&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;AMT_GOODS_PRICE&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;RATE_DOWN_PAYMENT&#39;: [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;], &#39;DAYS_DECISION&#39;: [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;], &#39;CNT_PAYMENT&#39;: [&#39;mean&#39;, &#39;sum&#39;], # 가공 컬럼 &#39;PREV_CREDIT_DIFF&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;PREV_CREDIT_APPL_RATIO&#39;:[&#39;mean&#39;, &#39;max&#39;], &#39;PREV_GOODS_DIFF&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;PREV_GOODS_APPL_RATIO&#39;:[&#39;mean&#39;, &#39;max&#39;], &#39;PREV_DAYS_LAST_DUE_DIFF&#39;:[&#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;], &#39;PREV_INTERESTS_RATE&#39;:[&#39;mean&#39;, &#39;max&#39;] } prev_group = prev.groupby(&#39;SK_ID_CURR&#39;) prev_amt_agg = prev_group.agg(agg_dict) # multi index 컬럼을 &#39;_&#39;로 연결하여 컬럼명 변경 prev_amt_agg.columns = [&quot;PREV_&quot;+ &quot;_&quot;.join(x).upper() for x in prev_amt_agg.columns.ravel()] . prev_amt_agg.head() . PREV_SK_ID_CURR_COUNT PREV_AMT_CREDIT_MEAN PREV_AMT_CREDIT_MAX PREV_AMT_CREDIT_SUM PREV_AMT_ANNUITY_MEAN PREV_AMT_ANNUITY_MAX PREV_AMT_ANNUITY_SUM PREV_AMT_APPLICATION_MEAN PREV_AMT_APPLICATION_MAX PREV_AMT_APPLICATION_SUM PREV_AMT_DOWN_PAYMENT_MEAN PREV_AMT_DOWN_PAYMENT_MAX PREV_AMT_DOWN_PAYMENT_SUM PREV_AMT_GOODS_PRICE_MEAN PREV_AMT_GOODS_PRICE_MAX PREV_AMT_GOODS_PRICE_SUM PREV_RATE_DOWN_PAYMENT_MIN PREV_RATE_DOWN_PAYMENT_MAX PREV_RATE_DOWN_PAYMENT_MEAN PREV_DAYS_DECISION_MIN PREV_DAYS_DECISION_MAX PREV_DAYS_DECISION_MEAN PREV_CNT_PAYMENT_MEAN PREV_CNT_PAYMENT_SUM PREV_PREV_CREDIT_DIFF_MEAN PREV_PREV_CREDIT_DIFF_MAX PREV_PREV_CREDIT_DIFF_SUM PREV_PREV_CREDIT_APPL_RATIO_MEAN PREV_PREV_CREDIT_APPL_RATIO_MAX PREV_PREV_GOODS_DIFF_MEAN PREV_PREV_GOODS_DIFF_MAX PREV_PREV_GOODS_DIFF_SUM PREV_PREV_GOODS_APPL_RATIO_MEAN PREV_PREV_GOODS_APPL_RATIO_MAX PREV_PREV_DAYS_LAST_DUE_DIFF_MEAN PREV_PREV_DAYS_LAST_DUE_DIFF_MAX PREV_PREV_DAYS_LAST_DUE_DIFF_SUM PREV_PREV_INTERESTS_RATE_MEAN PREV_PREV_INTERESTS_RATE_MAX . SK_ID_CURR . 100001 1 | 23787.00 | 23787.0 | 23787.0 | 3951.000 | 3951.000 | 3951.000 | 24835.50 | 24835.5 | 24835.5 | 2520.0 | 2520.0 | 2520.0 | 24835.5 | 24835.5 | 24835.5 | 0.104326 | 0.104326 | 0.104326 | -1740 | -1740 | -1740.0 | 8.0 | 8.0 | 1048.5 | 1048.5 | 1048.5 | 0.957782 | 0.957782 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 120.0 | 120.0 | 120.0 | 0.041099 | 0.041099 | . 100002 1 | 179055.00 | 179055.0 | 179055.0 | 9251.775 | 9251.775 | 9251.775 | 179055.00 | 179055.0 | 179055.0 | 0.0 | 0.0 | 0.0 | 179055.0 | 179055.0 | 179055.0 | 0.000000 | 0.000000 | 0.000000 | -606 | -606 | -606.0 | 24.0 | 24.0 | 0.0 | 0.0 | 0.0 | 1.000000 | 1.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 150.0 | 150.0 | 150.0 | 0.010003 | 0.010003 | . 100003 3 | 484191.00 | 1035882.0 | 1452573.0 | 56553.990 | 98356.995 | 169661.970 | 435436.50 | 900000.0 | 1306309.5 | 3442.5 | 6885.0 | 6885.0 | 435436.5 | 900000.0 | 1306309.5 | 0.000000 | 0.100061 | 0.050030 | -2341 | -746 | -1305.0 | 10.0 | 30.0 | -48754.5 | 756.0 | -146263.5 | 1.057664 | 1.150980 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 50.0 | 150.0 | 150.0 | 0.015272 | 0.018533 | . 100004 1 | 20106.00 | 20106.0 | 20106.0 | 5357.250 | 5357.250 | 5357.250 | 24282.00 | 24282.0 | 24282.0 | 4860.0 | 4860.0 | 4860.0 | 24282.0 | 24282.0 | 24282.0 | 0.212008 | 0.212008 | 0.212008 | -815 | -815 | -815.0 | 4.0 | 4.0 | 4176.0 | 4176.0 | 4176.0 | 0.828021 | 0.828021 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 30.0 | 30.0 | 30.0 | 0.016450 | 0.016450 | . 100005 2 | 20076.75 | 40153.5 | 40153.5 | 4813.200 | 4813.200 | 4813.200 | 22308.75 | 44617.5 | 44617.5 | 4464.0 | 4464.0 | 4464.0 | 44617.5 | 44617.5 | 44617.5 | 0.108964 | 0.108964 | 0.108964 | -757 | -315 | -536.0 | 12.0 | 12.0 | 2232.0 | 4464.0 | 4464.0 | 0.899950 | 0.899950 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 90.0 | 90.0 | 90.0 | 0.036537 | 0.036537 | . SK_ID_CURR&#48324;&#47196; NAME_CONTRACT_STATUS&#44032; Refused &#51068; &#44221;&#50864;&#51032; &#44148;&#49688; &#48143; &#44284;&#44144; &#45824;&#52636;&#44148; &#45824;&#48708; &#48708;&#50984; . Group by &#44592;&#51456; &#52972;&#47100; &#44592;&#48152;&#50640;&#49436; &#45796;&#47480; &#52972;&#47100;&#46308;&#51032; &#44592;&#51456;&#50640; &#46384;&#46972; &#49464;&#48516;&#54868;&#46108; aggregation &#49688;&#54665;. . prev[&#39;NAME_CONTRACT_STATUS&#39;].value_counts() . Approved 1036781 Canceled 316319 Refused 290678 Unused offer 26436 Name: NAME_CONTRACT_STATUS, dtype: int64 . Pandas&#45716; &#50896; DataFrame &#50640; groupby &#51201;&#50857;&#46108; DataFrame &#44284; &#49464;&#48512;&#44592;&#51456;&#51004;&#47196; filtering &#46108; DataFrame&#50640; groupby &#51201;&#50857;&#46108; DataFrame &#51012; &#51312;&#51064;&#54616;&#50668; &#49373;&#49457;. . NAME_CONTRACT_STATUS == &#39;Refused&#39; 세부 기준으로 filtering 및 filtering 된 DataFrame에 groupby 적용 | groupby 완료 후 기존 prev_amt_agg와 조인 | 효율적인 오류 방지를 위해서 groupby 시 적용후 groupby key값을 DataFrame의 Index가 아닌 일반 컬럼으로 변경. | . cond_refused = (prev[&#39;NAME_CONTRACT_STATUS&#39;] == &#39;Refused&#39;) prev_refused = prev[cond_refused] prev_refused.shape, prev.shape . ((290678, 44), (1670214, 44)) . prev_refused_agg = prev_refused.groupby([&#39;SK_ID_CURR&#39;])[&#39;SK_ID_CURR&#39;].count() print(prev_amt_agg.shape, prev_refused_agg .shape) prev_refused_agg.head(10) . (338857, 39) (118277,) . SK_ID_CURR 100006 1 100011 1 100027 1 100030 10 100035 8 100036 3 100037 2 100043 5 100046 1 100047 2 Name: SK_ID_CURR, dtype: int64 . # Series와 DataFrame 조인 시 Series를 DataFrame으로 내부 변환하는데 조인 시 Index명과 컬럼명이 서로 충돌하여 오류. # prev_amt_refused_agg = prev_amt_agg.merge(prev_refused_agg, on=&#39;SK_ID_CURR&#39;, how=&#39;left&#39;) pd.DataFrame(prev_refused_agg) # 일반적으로 groupby key를 INDEX로 하는 것보다 일반 컬럼으로 하는 것이 여러가지 오류 예방에 효율적. prev_refused_agg = prev_refused_agg.reset_index(name=&#39;PREV_REFUSED_COUNT&#39;) prev_amt_agg = prev_amt_agg.reset_index() prev_amt_refused_agg = prev_amt_agg.merge(prev_refused_agg, on=&#39;SK_ID_CURR&#39;, how=&#39;left&#39;) prev_amt_refused_agg.head() . SK_ID_CURR PREV_SK_ID_CURR_COUNT PREV_AMT_CREDIT_MEAN PREV_AMT_CREDIT_MAX PREV_AMT_CREDIT_SUM PREV_AMT_ANNUITY_MEAN PREV_AMT_ANNUITY_MAX PREV_AMT_ANNUITY_SUM PREV_AMT_APPLICATION_MEAN PREV_AMT_APPLICATION_MAX PREV_AMT_APPLICATION_SUM PREV_AMT_DOWN_PAYMENT_MEAN PREV_AMT_DOWN_PAYMENT_MAX PREV_AMT_DOWN_PAYMENT_SUM PREV_AMT_GOODS_PRICE_MEAN PREV_AMT_GOODS_PRICE_MAX PREV_AMT_GOODS_PRICE_SUM PREV_RATE_DOWN_PAYMENT_MIN PREV_RATE_DOWN_PAYMENT_MAX PREV_RATE_DOWN_PAYMENT_MEAN PREV_DAYS_DECISION_MIN PREV_DAYS_DECISION_MAX PREV_DAYS_DECISION_MEAN PREV_CNT_PAYMENT_MEAN PREV_CNT_PAYMENT_SUM PREV_PREV_CREDIT_DIFF_MEAN PREV_PREV_CREDIT_DIFF_MAX PREV_PREV_CREDIT_DIFF_SUM PREV_PREV_CREDIT_APPL_RATIO_MEAN PREV_PREV_CREDIT_APPL_RATIO_MAX PREV_PREV_GOODS_DIFF_MEAN PREV_PREV_GOODS_DIFF_MAX PREV_PREV_GOODS_DIFF_SUM PREV_PREV_GOODS_APPL_RATIO_MEAN PREV_PREV_GOODS_APPL_RATIO_MAX PREV_PREV_DAYS_LAST_DUE_DIFF_MEAN PREV_PREV_DAYS_LAST_DUE_DIFF_MAX PREV_PREV_DAYS_LAST_DUE_DIFF_SUM PREV_PREV_INTERESTS_RATE_MEAN PREV_PREV_INTERESTS_RATE_MAX PREV_REFUSED_COUNT . 0 100001 | 1 | 23787.00 | 23787.0 | 23787.0 | 3951.000 | 3951.000 | 3951.000 | 24835.50 | 24835.5 | 24835.5 | 2520.0 | 2520.0 | 2520.0 | 24835.5 | 24835.5 | 24835.5 | 0.104326 | 0.104326 | 0.104326 | -1740 | -1740 | -1740.0 | 8.0 | 8.0 | 1048.5 | 1048.5 | 1048.5 | 0.957782 | 0.957782 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 120.0 | 120.0 | 120.0 | 0.041099 | 0.041099 | NaN | . 1 100002 | 1 | 179055.00 | 179055.0 | 179055.0 | 9251.775 | 9251.775 | 9251.775 | 179055.00 | 179055.0 | 179055.0 | 0.0 | 0.0 | 0.0 | 179055.0 | 179055.0 | 179055.0 | 0.000000 | 0.000000 | 0.000000 | -606 | -606 | -606.0 | 24.0 | 24.0 | 0.0 | 0.0 | 0.0 | 1.000000 | 1.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 150.0 | 150.0 | 150.0 | 0.010003 | 0.010003 | NaN | . 2 100003 | 3 | 484191.00 | 1035882.0 | 1452573.0 | 56553.990 | 98356.995 | 169661.970 | 435436.50 | 900000.0 | 1306309.5 | 3442.5 | 6885.0 | 6885.0 | 435436.5 | 900000.0 | 1306309.5 | 0.000000 | 0.100061 | 0.050030 | -2341 | -746 | -1305.0 | 10.0 | 30.0 | -48754.5 | 756.0 | -146263.5 | 1.057664 | 1.150980 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 50.0 | 150.0 | 150.0 | 0.015272 | 0.018533 | NaN | . 3 100004 | 1 | 20106.00 | 20106.0 | 20106.0 | 5357.250 | 5357.250 | 5357.250 | 24282.00 | 24282.0 | 24282.0 | 4860.0 | 4860.0 | 4860.0 | 24282.0 | 24282.0 | 24282.0 | 0.212008 | 0.212008 | 0.212008 | -815 | -815 | -815.0 | 4.0 | 4.0 | 4176.0 | 4176.0 | 4176.0 | 0.828021 | 0.828021 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 30.0 | 30.0 | 30.0 | 0.016450 | 0.016450 | NaN | . 4 100005 | 2 | 20076.75 | 40153.5 | 40153.5 | 4813.200 | 4813.200 | 4813.200 | 22308.75 | 44617.5 | 44617.5 | 4464.0 | 4464.0 | 4464.0 | 44617.5 | 44617.5 | 44617.5 | 0.108964 | 0.108964 | 0.108964 | -757 | -315 | -536.0 | 12.0 | 12.0 | 2232.0 | 4464.0 | 4464.0 | 0.899950 | 0.899950 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 90.0 | 90.0 | 90.0 | 0.036537 | 0.036537 | NaN | . &#44228;&#49328;&#46108; PREV_REFUSED_COUNT &#51473; Null&#44050;&#51008; 0 &#51004;&#47196; &#48320;&#44221;&#54616;&#44256; SK_ID_CURR &#44060;&#48324; &#44148;&#49688; &#45824;&#48708; PREV_REFUSED_COUNT &#48708;&#50984; &#44228;&#49328; . prev_amt_refused_agg[&#39;PREV_REFUSED_COUNT&#39;].value_counts(dropna=False) . NaN 220580 1.0 54616 2.0 26793 3.0 14025 4.0 8243 5.0 4859 6.0 3147 7.0 2069 8.0 1247 9.0 871 10.0 656 11.0 437 12.0 340 13.0 233 14.0 157 15.0 149 16.0 94 17.0 62 18.0 54 19.0 48 20.0 28 21.0 25 22.0 21 24.0 18 25.0 12 26.0 11 23.0 11 27.0 8 29.0 5 28.0 5 32.0 4 31.0 3 41.0 3 30.0 3 37.0 3 33.0 2 34.0 2 39.0 2 35.0 2 36.0 2 47.0 1 43.0 1 64.0 1 54.0 1 58.0 1 51.0 1 68.0 1 Name: PREV_REFUSED_COUNT, dtype: int64 . prev_amt_refused_agg[&#39;PREV_REFUSED_COUNT&#39;] = prev_amt_refused_agg[&#39;PREV_REFUSED_COUNT&#39;].fillna(0) # SK_ID_CURR별 과거 대출건수 대비 REFUSED_COUNT 비율 생성. prev_amt_refused_agg[&#39;PREV_REFUSED_RATIO&#39;] = prev_amt_refused_agg[&#39;PREV_REFUSED_COUNT&#39;]/prev_amt_refused_agg[&#39;PREV_SK_ID_CURR_COUNT&#39;] . prev_amt_refused_agg.head(30) . SK_ID_CURR PREV_SK_ID_CURR_COUNT PREV_AMT_CREDIT_MEAN PREV_AMT_CREDIT_MAX PREV_AMT_CREDIT_SUM PREV_AMT_ANNUITY_MEAN PREV_AMT_ANNUITY_MAX PREV_AMT_ANNUITY_SUM PREV_AMT_APPLICATION_MEAN PREV_AMT_APPLICATION_MAX PREV_AMT_APPLICATION_SUM PREV_AMT_DOWN_PAYMENT_MEAN PREV_AMT_DOWN_PAYMENT_MAX PREV_AMT_DOWN_PAYMENT_SUM PREV_AMT_GOODS_PRICE_MEAN PREV_AMT_GOODS_PRICE_MAX PREV_AMT_GOODS_PRICE_SUM PREV_RATE_DOWN_PAYMENT_MIN PREV_RATE_DOWN_PAYMENT_MAX PREV_RATE_DOWN_PAYMENT_MEAN PREV_DAYS_DECISION_MIN PREV_DAYS_DECISION_MAX PREV_DAYS_DECISION_MEAN PREV_CNT_PAYMENT_MEAN PREV_CNT_PAYMENT_SUM PREV_PREV_CREDIT_DIFF_MEAN PREV_PREV_CREDIT_DIFF_MAX PREV_PREV_CREDIT_DIFF_SUM PREV_PREV_CREDIT_APPL_RATIO_MEAN PREV_PREV_CREDIT_APPL_RATIO_MAX PREV_PREV_GOODS_DIFF_MEAN PREV_PREV_GOODS_DIFF_MAX PREV_PREV_GOODS_DIFF_SUM PREV_PREV_GOODS_APPL_RATIO_MEAN PREV_PREV_GOODS_APPL_RATIO_MAX PREV_PREV_DAYS_LAST_DUE_DIFF_MEAN PREV_PREV_DAYS_LAST_DUE_DIFF_MAX PREV_PREV_DAYS_LAST_DUE_DIFF_SUM PREV_PREV_INTERESTS_RATE_MEAN PREV_PREV_INTERESTS_RATE_MAX PREV_REFUSED_COUNT PREV_REFUSED_RATIO . 0 100001 | 1 | 23787.000000 | 23787.0 | 23787.000 | 3951.000000 | 3951.000 | 3951.000 | 24835.500000 | 24835.5 | 24835.500 | 2520.000000 | 2520.0 | 2520.00 | 24835.500000 | 24835.5 | 24835.500 | 0.104326 | 0.104326 | 0.104326 | -1740 | -1740 | -1740.000000 | 8.000000 | 8.0 | 1048.500000 | 1048.5 | 1048.50 | 0.957782 | 0.957782 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 120.00 | 120.0 | 120.0 | 0.041099 | 0.041099 | 0.0 | 0.000000 | . 1 100002 | 1 | 179055.000000 | 179055.0 | 179055.000 | 9251.775000 | 9251.775 | 9251.775 | 179055.000000 | 179055.0 | 179055.000 | 0.000000 | 0.0 | 0.00 | 179055.000000 | 179055.0 | 179055.000 | 0.000000 | 0.000000 | 0.000000 | -606 | -606 | -606.000000 | 24.000000 | 24.0 | 0.000000 | 0.0 | 0.00 | 1.000000 | 1.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 150.00 | 150.0 | 150.0 | 0.010003 | 0.010003 | 0.0 | 0.000000 | . 2 100003 | 3 | 484191.000000 | 1035882.0 | 1452573.000 | 56553.990000 | 98356.995 | 169661.970 | 435436.500000 | 900000.0 | 1306309.500 | 3442.500000 | 6885.0 | 6885.00 | 435436.500000 | 900000.0 | 1306309.500 | 0.000000 | 0.100061 | 0.050030 | -2341 | -746 | -1305.000000 | 10.000000 | 30.0 | -48754.500000 | 756.0 | -146263.50 | 1.057664 | 1.150980 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 50.00 | 150.0 | 150.0 | 0.015272 | 0.018533 | 0.0 | 0.000000 | . 3 100004 | 1 | 20106.000000 | 20106.0 | 20106.000 | 5357.250000 | 5357.250 | 5357.250 | 24282.000000 | 24282.0 | 24282.000 | 4860.000000 | 4860.0 | 4860.00 | 24282.000000 | 24282.0 | 24282.000 | 0.212008 | 0.212008 | 0.212008 | -815 | -815 | -815.000000 | 4.000000 | 4.0 | 4176.000000 | 4176.0 | 4176.00 | 0.828021 | 0.828021 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 30.00 | 30.0 | 30.0 | 0.016450 | 0.016450 | 0.0 | 0.000000 | . 4 100005 | 2 | 20076.750000 | 40153.5 | 40153.500 | 4813.200000 | 4813.200 | 4813.200 | 22308.750000 | 44617.5 | 44617.500 | 4464.000000 | 4464.0 | 4464.00 | 44617.500000 | 44617.5 | 44617.500 | 0.108964 | 0.108964 | 0.108964 | -757 | -315 | -536.000000 | 12.000000 | 12.0 | 2232.000000 | 4464.0 | 4464.00 | 0.899950 | 0.899950 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 90.00 | 90.0 | 90.0 | 0.036537 | 0.036537 | 0.0 | 0.000000 | . 5 100006 | 9 | 291695.500000 | 906615.0 | 2625259.500 | 23651.175000 | 39954.510 | 141907.050 | 272203.260000 | 688500.0 | 2449829.340 | 34840.170000 | 66987.0 | 69680.34 | 408304.890000 | 688500.0 | 2449829.340 | 0.108994 | 0.217830 | 0.163412 | -617 | -181 | -272.444444 | 23.000000 | 138.0 | -19492.240000 | 66987.0 | -175430.16 | 1.012684 | 1.316797 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 810.00 | 1410.0 | 1620.0 | -inf | 0.027424 | 1.0 | 0.111111 | . 6 100007 | 6 | 166638.750000 | 284400.0 | 999832.500 | 12278.805000 | 22678.785 | 73672.830 | 150530.250000 | 247500.0 | 903181.500 | 3390.750000 | 3676.5 | 6781.50 | 150530.250000 | 247500.0 | 903181.500 | 0.100143 | 0.218890 | 0.159516 | -2357 | -374 | -1222.833333 | 20.666667 | 124.0 | -16108.500000 | 2560.5 | -96651.00 | 1.046356 | 1.264000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 7.50 | 30.0 | 30.0 | 0.026538 | 0.036164 | 0.0 | 0.000000 | . 7 100008 | 5 | 162767.700000 | 501975.0 | 813838.500 | 15839.696250 | 25309.575 | 63358.785 | 155701.800000 | 450000.0 | 778509.000 | 5548.500000 | 12145.5 | 16645.50 | 194627.250000 | 450000.0 | 778509.000 | 0.000000 | 0.110243 | 0.073051 | -2536 | -82 | -1192.000000 | 14.000000 | 56.0 | -7065.900000 | 12145.5 | -35329.50 | 0.978569 | 1.115500 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 165.00 | 660.0 | 660.0 | 0.018055 | 0.034034 | 0.0 | 0.000000 | . 8 100009 | 7 | 70137.642857 | 98239.5 | 490963.500 | 10051.412143 | 17341.605 | 70359.885 | 76741.714286 | 110160.0 | 537192.000 | 9203.142857 | 22032.0 | 64422.00 | 76741.714286 | 110160.0 | 537192.000 | 0.000000 | 0.209525 | 0.126602 | -1562 | -74 | -719.285714 | 8.000000 | 56.0 | 6604.071429 | 17671.5 | 46228.50 | 0.916226 | 1.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.019536 | 0.030703 | 0.0 | 0.000000 | . 9 100010 | 1 | 260811.000000 | 260811.0 | 260811.000 | 27463.410000 | 27463.410 | 27463.410 | 247212.000000 | 247212.0 | 247212.000 | 0.000000 | 0.0 | 0.00 | 247212.000000 | 247212.0 | 247212.000 | 0.000000 | 0.000000 | 0.000000 | -1070 | -1070 | -1070.000000 | 10.000000 | 10.0 | -13599.000000 | -13599.0 | -13599.00 | 1.055009 | 1.055009 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.005300 | 0.005300 | 0.0 | 0.000000 | . 10 100011 | 4 | 261840.375000 | 732915.0 | 1047361.500 | 18303.195000 | 31295.250 | 54909.585 | 202732.875000 | 675000.0 | 810931.500 | 6797.250000 | 13594.5 | 13594.50 | 270310.500000 | 675000.0 | 810931.500 | 0.000000 | 0.100010 | 0.050005 | -2508 | -1162 | -1784.500000 | 14.000000 | 42.0 | -59107.500000 | 1485.0 | -236430.00 | inf | inf | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | -inf | 0.025367 | 1.0 | 0.250000 | . 11 100012 | 4 | 74119.500000 | 158508.0 | 296478.000 | 7894.155000 | 11188.035 | 23682.465 | 60930.000000 | 135000.0 | 243720.000 | 0.000000 | 0.0 | 0.00 | 81240.000000 | 135000.0 | 243720.000 | 0.000000 | 0.000000 | 0.000000 | -1673 | -107 | -779.750000 | 18.000000 | 54.0 | -13189.500000 | 0.0 | -52758.00 | 1.236566 | 1.269700 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 100.00 | 180.0 | 300.0 | 0.033372 | 0.043775 | 0.0 | 0.000000 | . 12 100013 | 4 | 146134.125000 | 512370.0 | 584536.500 | 11478.195000 | 23153.985 | 34434.585 | 130871.250000 | 450000.0 | 523485.000 | 3375.000000 | 6750.0 | 6750.00 | 174495.000000 | 450000.0 | 523485.000 | 0.000000 | 0.134434 | 0.067217 | -1999 | -222 | -837.500000 | 17.333333 | 52.0 | -15262.875000 | 3096.0 | -61051.50 | 1.052363 | 1.138600 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 200.00 | 570.0 | 600.0 | 0.027617 | 0.036400 | 0.0 | 0.000000 | . 13 100014 | 2 | 102834.000000 | 131868.0 | 205668.000 | 12806.550000 | 14045.625 | 25613.100 | 96536.250000 | 119272.5 | 193072.500 | 0.000000 | 0.0 | 0.00 | 96536.250000 | 119272.5 | 193072.500 | 0.000000 | 0.000000 | 0.000000 | -844 | -102 | -473.000000 | 9.000000 | 18.0 | -6297.750000 | 0.0 | -12595.50 | 1.052801 | 1.105603 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.014020 | 0.023653 | 0.0 | 0.000000 | . 14 100015 | 1 | 67077.000000 | 67077.0 | 67077.000 | 7666.920000 | 7666.920 | 7666.920 | 68850.000000 | 68850.0 | 68850.000 | 6885.000000 | 6885.0 | 6885.00 | 68850.000000 | 68850.0 | 68850.000 | 0.101382 | 0.101382 | 0.101382 | -2396 | -2396 | -2396.000000 | 10.000000 | 10.0 | 1773.000000 | 1773.0 | 1773.00 | 0.974248 | 0.974248 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.014300 | 0.014300 | 0.0 | 0.000000 | . 15 100016 | 4 | 106221.375000 | 232200.0 | 424885.500 | 8593.155000 | 14480.460 | 34372.620 | 120745.125000 | 290250.0 | 482980.500 | 15690.375000 | 58050.0 | 62761.50 | 120745.125000 | 290250.0 | 482980.500 | 0.000000 | 0.217818 | 0.081682 | -2370 | -1049 | -1779.500000 | 17.000000 | 68.0 | 14523.750000 | 58050.0 | 58095.00 | 0.949761 | 1.099045 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 70.00 | 210.0 | 210.0 | 0.029580 | 0.033600 | 0.0 | 0.000000 | . 16 100017 | 2 | 175565.250000 | 198409.5 | 351130.500 | 14375.407500 | 16967.295 | 28750.815 | 176665.500000 | 195291.0 | 353331.000 | 19152.000000 | 22500.0 | 38304.00 | 176665.500000 | 195291.0 | 353331.000 | 0.102133 | 0.110926 | 0.106529 | -2441 | -612 | -1526.500000 | 17.000000 | 34.0 | 1100.250000 | 5319.0 | 2200.50 | 0.991156 | 1.015968 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.014412 | 0.017723 | 0.0 | 0.000000 | . 17 100018 | 4 | 285989.591250 | 536364.0 | 1143958.365 | 38669.703750 | 94906.035 | 154678.815 | 298363.466250 | 571774.5 | 1193453.865 | 14295.375000 | 57181.5 | 57181.50 | 298363.466250 | 571774.5 | 1193453.865 | 0.000000 | 0.108917 | 0.027229 | -1648 | -188 | -755.250000 | 10.000000 | 40.0 | 12373.875000 | 57181.5 | 49495.50 | 1.014609 | 1.158442 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | -9.00 | 0.0 | -18.0 | 0.018384 | 0.038257 | 0.0 | 0.000000 | . 18 100019 | 1 | 104683.500000 | 104683.5 | 104683.500 | 10518.615000 | 10518.615 | 10518.615 | 84136.500000 | 84136.5 | 84136.500 | 0.000000 | 0.0 | 0.00 | 84136.500000 | 84136.5 | 84136.500 | 0.000000 | 0.000000 | 0.000000 | -925 | -925 | -925.000000 | 12.000000 | 12.0 | -20547.000000 | -20547.0 | -20547.00 | 1.244210 | 1.244210 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 150.00 | 150.0 | 150.0 | 0.017147 | 0.017147 | 0.0 | 0.000000 | . 19 100020 | 2 | 41706.000000 | 48325.5 | 83412.000 | 7254.517500 | 8779.770 | 14509.035 | 39206.250000 | 45900.0 | 78412.500 | 0.000000 | 0.0 | 0.00 | 39206.250000 | 45900.0 | 78412.500 | 0.000000 | 0.000000 | 0.000000 | -472 | -243 | -357.500000 | 7.000000 | 14.0 | -2499.750000 | -2425.5 | -4999.50 | 1.066006 | 1.079170 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.026651 | 0.038290 | 0.0 | 0.000000 | . 20 100021 | 6 | 153615.750000 | 675000.0 | 921694.500 | 10686.165000 | 33750.000 | 64116.990 | 40961.250000 | 99189.0 | 245767.500 | 3770.100000 | 9922.5 | 18850.50 | 49153.500000 | 99189.0 | 245767.500 | 0.000000 | 0.433539 | 0.106606 | -2811 | -261 | -943.833333 | 7.000000 | 42.0 | -112654.500000 | 8928.0 | -675927.00 | inf | inf | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 90.00 | 180.0 | 270.0 | -inf | 0.034033 | 0.0 | 0.000000 | . 21 100022 | 1 | 103203.000000 | 103203.0 | 103203.000 | 6765.975000 | 6765.975 | 6765.975 | 89091.000000 | 89091.0 | 89091.000 | 0.000000 | 0.0 | 0.00 | 89091.000000 | 89091.0 | 89091.000 | 0.000000 | 0.000000 | 0.000000 | -239 | -239 | -239.000000 | 18.000000 | 18.0 | -14112.000000 | -14112.0 | -14112.00 | 1.158400 | 1.158400 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | NaN | NaN | 0.0 | 0.010004 | 0.010004 | 0.0 | 0.000000 | . 22 100023 | 4 | 113517.000000 | 239242.5 | 454068.000 | 8895.892500 | 16822.440 | 35583.570 | 98746.875000 | 180000.0 | 394987.500 | 8435.250000 | 9000.0 | 16870.50 | 98746.875000 | 180000.0 | 394987.500 | 0.095959 | 0.101380 | 0.098669 | -2424 | -262 | -1215.750000 | 12.500000 | 50.0 | -14770.125000 | 2025.0 | -59080.50 | 1.080951 | 1.329125 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 240.00 | 600.0 | 720.0 | -inf | 0.028649 | 0.0 | 0.000000 | . 23 100025 | 8 | 295425.781875 | 1042560.0 | 2363406.255 | 27645.595714 | 55678.095 | 193519.170 | 259795.344375 | 900000.0 | 2078362.755 | 5845.500000 | 23382.0 | 23382.00 | 296908.965000 | 900000.0 | 2078362.755 | 0.000000 | 0.423065 | 0.105766 | -2060 | -7 | -731.000000 | 13.714286 | 96.0 | -35630.437500 | 21645.0 | -285043.50 | 1.073930 | 1.269700 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 174.25 | 367.0 | 697.0 | 0.022884 | 0.027871 | 0.0 | 0.000000 | . 24 100026 | 3 | 292281.000000 | 744498.0 | 876843.000 | 33193.665000 | 42778.845 | 66387.330 | 269115.000000 | 675000.0 | 807345.000 | 0.000000 | 0.0 | 0.00 | 403672.500000 | 675000.0 | 807345.000 | 0.000000 | 0.000000 | 0.000000 | -1427 | -152 | -682.333333 | 21.000000 | 42.0 | -23166.000000 | 0.0 | -69498.00 | 1.051480 | 1.102960 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 510.00 | 1020.0 | 1020.0 | 0.020701 | 0.029682 | 0.0 | 0.000000 | . 25 100027 | 4 | 173795.625000 | 239850.0 | 695182.500 | 24067.755000 | 28077.570 | 72203.265 | 163621.125000 | 226984.5 | 654484.500 | 0.000000 | 0.0 | 0.00 | 218161.500000 | 226984.5 | 654484.500 | 0.000000 | 0.000000 | 0.000000 | -795 | -181 | -407.000000 | 11.333333 | 34.0 | -10174.500000 | 0.0 | -40698.00 | 1.062332 | 1.066000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.014849 | 0.017250 | 1.0 | 0.250000 | . 26 100028 | 5 | 92920.500000 | 225000.0 | 464602.500 | 8091.585000 | 11250.000 | 24274.755 | 49207.500000 | 130765.5 | 246037.500 | 3750.000000 | 11250.0 | 11250.00 | 82012.500000 | 130765.5 | 246037.500 | 0.000000 | 0.173124 | 0.057708 | -1805 | -531 | -1124.200000 | 11.333333 | 34.0 | -43713.000000 | 6435.0 | -218565.00 | inf | inf | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 75.00 | 150.0 | 150.0 | -inf | 0.017550 | 0.0 | 0.000000 | . 27 100029 | 1 | 126000.000000 | 126000.0 | 126000.000 | 22891.680000 | 22891.680 | 22891.680 | 126000.000000 | 126000.0 | 126000.000 | 0.000000 | 0.0 | 0.00 | 126000.000000 | 126000.0 | 126000.000 | 0.000000 | 0.000000 | 0.000000 | -324 | -324 | -324.000000 | 6.000000 | 6.0 | 0.000000 | 0.0 | 0.00 | 1.000000 | 1.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 60.00 | 60.0 | 60.0 | 0.015013 | 0.015013 | 0.0 | 0.000000 | . 28 100030 | 12 | 32534.643750 | 49734.0 | 390415.725 | 4731.041250 | 9035.685 | 37848.330 | 33497.268750 | 47241.0 | 401967.225 | 1262.625000 | 6714.0 | 15151.50 | 33497.268750 | 47241.0 | 401967.225 | 0.000000 | 0.210919 | 0.044804 | -2856 | -378 | -1981.083333 | 8.625000 | 69.0 | 962.625000 | 5607.0 | 11551.50 | 0.965475 | 1.052772 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 60.00 | 90.0 | 120.0 | 0.017772 | 0.037233 | 10.0 | 0.833333 | . 29 100032 | 1 | 42255.000000 | 42255.0 | 42255.000 | 7910.145000 | 7910.145 | 7910.145 | 69255.000000 | 69255.0 | 69255.000 | 27000.000000 | 27000.0 | 27000.00 | 69255.000000 | 69255.0 | 69255.000 | 0.424597 | 0.424597 | 0.424597 | -1497 | -1497 | -1497.000000 | 6.000000 | 6.0 | 27000.000000 | 27000.0 | 27000.00 | 0.610136 | 0.610136 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.020534 | 0.020534 | 0.0 | 0.000000 | . prev_refused_appr_group = prev[prev[&#39;NAME_CONTRACT_STATUS&#39;].isin([&#39;Approved&#39;, &#39;Refused&#39;])].groupby([ &#39;SK_ID_CURR&#39;, &#39;NAME_CONTRACT_STATUS&#39;]) prev_refused_appr_agg = prev_refused_appr_group[&#39;SK_ID_CURR&#39;].count().unstack() prev_refused_appr_agg.head(30) . NAME_CONTRACT_STATUS Approved Refused . SK_ID_CURR . 100001 1.0 | NaN | . 100002 1.0 | NaN | . 100003 3.0 | NaN | . 100004 1.0 | NaN | . 100005 1.0 | NaN | . 100006 5.0 | 1.0 | . 100007 6.0 | NaN | . 100008 4.0 | NaN | . 100009 7.0 | NaN | . 100010 1.0 | NaN | . 100011 3.0 | 1.0 | . 100012 3.0 | NaN | . 100013 3.0 | NaN | . 100014 2.0 | NaN | . 100015 1.0 | NaN | . 100016 4.0 | NaN | . 100017 2.0 | NaN | . 100018 4.0 | NaN | . 100019 1.0 | NaN | . 100020 2.0 | NaN | . 100021 6.0 | NaN | . 100022 1.0 | NaN | . 100023 4.0 | NaN | . 100025 7.0 | NaN | . 100026 2.0 | NaN | . 100027 2.0 | 1.0 | . 100028 3.0 | NaN | . 100029 1.0 | NaN | . 100030 2.0 | 10.0 | . 100032 1.0 | NaN | . &#52972;&#47100;&#47749; &#48320;&#44221;, Null &#52376;&#47532;, &#44536;&#47532;&#44256; &#44592;&#51316;&#51032; prev_amt_agg&#50752; &#51312;&#51064; &#54980; &#45936;&#51060;&#53552; &#44032;&#44277; . prev_refused_appr_agg.columns = [&#39;PREV_APPROVED_COUNT&#39;, &#39;PREV_REFUSED_COUNT&#39; ] # NaN값은 모두 0으로 변경. prev_refused_appr_agg = prev_refused_appr_agg.fillna(0) . prev_refused_appr_agg.head() . PREV_APPROVED_COUNT PREV_REFUSED_COUNT . SK_ID_CURR . 100001 1.0 | 0.0 | . 100002 1.0 | 0.0 | . 100003 3.0 | 0.0 | . 100004 1.0 | 0.0 | . 100005 1.0 | 0.0 | . prev_agg = prev_amt_agg.merge(prev_refused_appr_agg, on=&#39;SK_ID_CURR&#39;, how=&#39;left&#39;) # SK_ID_CURR별 과거 대출건수 대비 APPROVED_COUNT 및 REFUSED_COUNT 비율 생성. prev_agg[&#39;PREV_REFUSED_RATIO&#39;] = prev_agg[&#39;PREV_REFUSED_COUNT&#39;]/prev_agg[&#39;PREV_SK_ID_CURR_COUNT&#39;] prev_agg[&#39;PREV_APPROVED_RATIO&#39;] = prev_agg[&#39;PREV_APPROVED_COUNT&#39;]/prev_agg[&#39;PREV_SK_ID_CURR_COUNT&#39;] # &#39;PREV_REFUSED_COUNT&#39;, &#39;PREV_APPROVED_COUNT&#39; 컬럼 drop prev_agg = prev_agg.drop([&#39;PREV_REFUSED_COUNT&#39;, &#39;PREV_APPROVED_COUNT&#39;], axis=1) # prev_amt_agg와 prev_refused_appr_agg INDEX인 SK_ID_CURR이 조인 후 정식 컬럼으로 생성됨. prev_agg.head(30) . SK_ID_CURR PREV_SK_ID_CURR_COUNT PREV_AMT_CREDIT_MEAN PREV_AMT_CREDIT_MAX PREV_AMT_CREDIT_SUM PREV_AMT_ANNUITY_MEAN PREV_AMT_ANNUITY_MAX PREV_AMT_ANNUITY_SUM PREV_AMT_APPLICATION_MEAN PREV_AMT_APPLICATION_MAX PREV_AMT_APPLICATION_SUM PREV_AMT_DOWN_PAYMENT_MEAN PREV_AMT_DOWN_PAYMENT_MAX PREV_AMT_DOWN_PAYMENT_SUM PREV_AMT_GOODS_PRICE_MEAN PREV_AMT_GOODS_PRICE_MAX PREV_AMT_GOODS_PRICE_SUM PREV_RATE_DOWN_PAYMENT_MIN PREV_RATE_DOWN_PAYMENT_MAX PREV_RATE_DOWN_PAYMENT_MEAN PREV_DAYS_DECISION_MIN PREV_DAYS_DECISION_MAX PREV_DAYS_DECISION_MEAN PREV_CNT_PAYMENT_MEAN PREV_CNT_PAYMENT_SUM PREV_PREV_CREDIT_DIFF_MEAN PREV_PREV_CREDIT_DIFF_MAX PREV_PREV_CREDIT_DIFF_SUM PREV_PREV_CREDIT_APPL_RATIO_MEAN PREV_PREV_CREDIT_APPL_RATIO_MAX PREV_PREV_GOODS_DIFF_MEAN PREV_PREV_GOODS_DIFF_MAX PREV_PREV_GOODS_DIFF_SUM PREV_PREV_GOODS_APPL_RATIO_MEAN PREV_PREV_GOODS_APPL_RATIO_MAX PREV_PREV_DAYS_LAST_DUE_DIFF_MEAN PREV_PREV_DAYS_LAST_DUE_DIFF_MAX PREV_PREV_DAYS_LAST_DUE_DIFF_SUM PREV_PREV_INTERESTS_RATE_MEAN PREV_PREV_INTERESTS_RATE_MAX PREV_REFUSED_RATIO PREV_APPROVED_RATIO . 0 100001 | 1 | 23787.000000 | 23787.0 | 23787.000 | 3951.000000 | 3951.000 | 3951.000 | 24835.500000 | 24835.5 | 24835.500 | 2520.000000 | 2520.0 | 2520.00 | 24835.500000 | 24835.5 | 24835.500 | 0.104326 | 0.104326 | 0.104326 | -1740 | -1740 | -1740.000000 | 8.000000 | 8.0 | 1048.500000 | 1048.5 | 1048.50 | 0.957782 | 0.957782 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 120.00 | 120.0 | 120.0 | 0.041099 | 0.041099 | 0.000000 | 1.000000 | . 1 100002 | 1 | 179055.000000 | 179055.0 | 179055.000 | 9251.775000 | 9251.775 | 9251.775 | 179055.000000 | 179055.0 | 179055.000 | 0.000000 | 0.0 | 0.00 | 179055.000000 | 179055.0 | 179055.000 | 0.000000 | 0.000000 | 0.000000 | -606 | -606 | -606.000000 | 24.000000 | 24.0 | 0.000000 | 0.0 | 0.00 | 1.000000 | 1.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 150.00 | 150.0 | 150.0 | 0.010003 | 0.010003 | 0.000000 | 1.000000 | . 2 100003 | 3 | 484191.000000 | 1035882.0 | 1452573.000 | 56553.990000 | 98356.995 | 169661.970 | 435436.500000 | 900000.0 | 1306309.500 | 3442.500000 | 6885.0 | 6885.00 | 435436.500000 | 900000.0 | 1306309.500 | 0.000000 | 0.100061 | 0.050030 | -2341 | -746 | -1305.000000 | 10.000000 | 30.0 | -48754.500000 | 756.0 | -146263.50 | 1.057664 | 1.150980 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 50.00 | 150.0 | 150.0 | 0.015272 | 0.018533 | 0.000000 | 1.000000 | . 3 100004 | 1 | 20106.000000 | 20106.0 | 20106.000 | 5357.250000 | 5357.250 | 5357.250 | 24282.000000 | 24282.0 | 24282.000 | 4860.000000 | 4860.0 | 4860.00 | 24282.000000 | 24282.0 | 24282.000 | 0.212008 | 0.212008 | 0.212008 | -815 | -815 | -815.000000 | 4.000000 | 4.0 | 4176.000000 | 4176.0 | 4176.00 | 0.828021 | 0.828021 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 30.00 | 30.0 | 30.0 | 0.016450 | 0.016450 | 0.000000 | 1.000000 | . 4 100005 | 2 | 20076.750000 | 40153.5 | 40153.500 | 4813.200000 | 4813.200 | 4813.200 | 22308.750000 | 44617.5 | 44617.500 | 4464.000000 | 4464.0 | 4464.00 | 44617.500000 | 44617.5 | 44617.500 | 0.108964 | 0.108964 | 0.108964 | -757 | -315 | -536.000000 | 12.000000 | 12.0 | 2232.000000 | 4464.0 | 4464.00 | 0.899950 | 0.899950 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 90.00 | 90.0 | 90.0 | 0.036537 | 0.036537 | 0.000000 | 0.500000 | . 5 100006 | 9 | 291695.500000 | 906615.0 | 2625259.500 | 23651.175000 | 39954.510 | 141907.050 | 272203.260000 | 688500.0 | 2449829.340 | 34840.170000 | 66987.0 | 69680.34 | 408304.890000 | 688500.0 | 2449829.340 | 0.108994 | 0.217830 | 0.163412 | -617 | -181 | -272.444444 | 23.000000 | 138.0 | -19492.240000 | 66987.0 | -175430.16 | 1.012684 | 1.316797 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 810.00 | 1410.0 | 1620.0 | -inf | 0.027424 | 0.111111 | 0.555556 | . 6 100007 | 6 | 166638.750000 | 284400.0 | 999832.500 | 12278.805000 | 22678.785 | 73672.830 | 150530.250000 | 247500.0 | 903181.500 | 3390.750000 | 3676.5 | 6781.50 | 150530.250000 | 247500.0 | 903181.500 | 0.100143 | 0.218890 | 0.159516 | -2357 | -374 | -1222.833333 | 20.666667 | 124.0 | -16108.500000 | 2560.5 | -96651.00 | 1.046356 | 1.264000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 7.50 | 30.0 | 30.0 | 0.026538 | 0.036164 | 0.000000 | 1.000000 | . 7 100008 | 5 | 162767.700000 | 501975.0 | 813838.500 | 15839.696250 | 25309.575 | 63358.785 | 155701.800000 | 450000.0 | 778509.000 | 5548.500000 | 12145.5 | 16645.50 | 194627.250000 | 450000.0 | 778509.000 | 0.000000 | 0.110243 | 0.073051 | -2536 | -82 | -1192.000000 | 14.000000 | 56.0 | -7065.900000 | 12145.5 | -35329.50 | 0.978569 | 1.115500 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 165.00 | 660.0 | 660.0 | 0.018055 | 0.034034 | 0.000000 | 0.800000 | . 8 100009 | 7 | 70137.642857 | 98239.5 | 490963.500 | 10051.412143 | 17341.605 | 70359.885 | 76741.714286 | 110160.0 | 537192.000 | 9203.142857 | 22032.0 | 64422.00 | 76741.714286 | 110160.0 | 537192.000 | 0.000000 | 0.209525 | 0.126602 | -1562 | -74 | -719.285714 | 8.000000 | 56.0 | 6604.071429 | 17671.5 | 46228.50 | 0.916226 | 1.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.019536 | 0.030703 | 0.000000 | 1.000000 | . 9 100010 | 1 | 260811.000000 | 260811.0 | 260811.000 | 27463.410000 | 27463.410 | 27463.410 | 247212.000000 | 247212.0 | 247212.000 | 0.000000 | 0.0 | 0.00 | 247212.000000 | 247212.0 | 247212.000 | 0.000000 | 0.000000 | 0.000000 | -1070 | -1070 | -1070.000000 | 10.000000 | 10.0 | -13599.000000 | -13599.0 | -13599.00 | 1.055009 | 1.055009 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.005300 | 0.005300 | 0.000000 | 1.000000 | . 10 100011 | 4 | 261840.375000 | 732915.0 | 1047361.500 | 18303.195000 | 31295.250 | 54909.585 | 202732.875000 | 675000.0 | 810931.500 | 6797.250000 | 13594.5 | 13594.50 | 270310.500000 | 675000.0 | 810931.500 | 0.000000 | 0.100010 | 0.050005 | -2508 | -1162 | -1784.500000 | 14.000000 | 42.0 | -59107.500000 | 1485.0 | -236430.00 | inf | inf | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | -inf | 0.025367 | 0.250000 | 0.750000 | . 11 100012 | 4 | 74119.500000 | 158508.0 | 296478.000 | 7894.155000 | 11188.035 | 23682.465 | 60930.000000 | 135000.0 | 243720.000 | 0.000000 | 0.0 | 0.00 | 81240.000000 | 135000.0 | 243720.000 | 0.000000 | 0.000000 | 0.000000 | -1673 | -107 | -779.750000 | 18.000000 | 54.0 | -13189.500000 | 0.0 | -52758.00 | 1.236566 | 1.269700 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 100.00 | 180.0 | 300.0 | 0.033372 | 0.043775 | 0.000000 | 0.750000 | . 12 100013 | 4 | 146134.125000 | 512370.0 | 584536.500 | 11478.195000 | 23153.985 | 34434.585 | 130871.250000 | 450000.0 | 523485.000 | 3375.000000 | 6750.0 | 6750.00 | 174495.000000 | 450000.0 | 523485.000 | 0.000000 | 0.134434 | 0.067217 | -1999 | -222 | -837.500000 | 17.333333 | 52.0 | -15262.875000 | 3096.0 | -61051.50 | 1.052363 | 1.138600 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 200.00 | 570.0 | 600.0 | 0.027617 | 0.036400 | 0.000000 | 0.750000 | . 13 100014 | 2 | 102834.000000 | 131868.0 | 205668.000 | 12806.550000 | 14045.625 | 25613.100 | 96536.250000 | 119272.5 | 193072.500 | 0.000000 | 0.0 | 0.00 | 96536.250000 | 119272.5 | 193072.500 | 0.000000 | 0.000000 | 0.000000 | -844 | -102 | -473.000000 | 9.000000 | 18.0 | -6297.750000 | 0.0 | -12595.50 | 1.052801 | 1.105603 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.014020 | 0.023653 | 0.000000 | 1.000000 | . 14 100015 | 1 | 67077.000000 | 67077.0 | 67077.000 | 7666.920000 | 7666.920 | 7666.920 | 68850.000000 | 68850.0 | 68850.000 | 6885.000000 | 6885.0 | 6885.00 | 68850.000000 | 68850.0 | 68850.000 | 0.101382 | 0.101382 | 0.101382 | -2396 | -2396 | -2396.000000 | 10.000000 | 10.0 | 1773.000000 | 1773.0 | 1773.00 | 0.974248 | 0.974248 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.014300 | 0.014300 | 0.000000 | 1.000000 | . 15 100016 | 4 | 106221.375000 | 232200.0 | 424885.500 | 8593.155000 | 14480.460 | 34372.620 | 120745.125000 | 290250.0 | 482980.500 | 15690.375000 | 58050.0 | 62761.50 | 120745.125000 | 290250.0 | 482980.500 | 0.000000 | 0.217818 | 0.081682 | -2370 | -1049 | -1779.500000 | 17.000000 | 68.0 | 14523.750000 | 58050.0 | 58095.00 | 0.949761 | 1.099045 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 70.00 | 210.0 | 210.0 | 0.029580 | 0.033600 | 0.000000 | 1.000000 | . 16 100017 | 2 | 175565.250000 | 198409.5 | 351130.500 | 14375.407500 | 16967.295 | 28750.815 | 176665.500000 | 195291.0 | 353331.000 | 19152.000000 | 22500.0 | 38304.00 | 176665.500000 | 195291.0 | 353331.000 | 0.102133 | 0.110926 | 0.106529 | -2441 | -612 | -1526.500000 | 17.000000 | 34.0 | 1100.250000 | 5319.0 | 2200.50 | 0.991156 | 1.015968 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.014412 | 0.017723 | 0.000000 | 1.000000 | . 17 100018 | 4 | 285989.591250 | 536364.0 | 1143958.365 | 38669.703750 | 94906.035 | 154678.815 | 298363.466250 | 571774.5 | 1193453.865 | 14295.375000 | 57181.5 | 57181.50 | 298363.466250 | 571774.5 | 1193453.865 | 0.000000 | 0.108917 | 0.027229 | -1648 | -188 | -755.250000 | 10.000000 | 40.0 | 12373.875000 | 57181.5 | 49495.50 | 1.014609 | 1.158442 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | -9.00 | 0.0 | -18.0 | 0.018384 | 0.038257 | 0.000000 | 1.000000 | . 18 100019 | 1 | 104683.500000 | 104683.5 | 104683.500 | 10518.615000 | 10518.615 | 10518.615 | 84136.500000 | 84136.5 | 84136.500 | 0.000000 | 0.0 | 0.00 | 84136.500000 | 84136.5 | 84136.500 | 0.000000 | 0.000000 | 0.000000 | -925 | -925 | -925.000000 | 12.000000 | 12.0 | -20547.000000 | -20547.0 | -20547.00 | 1.244210 | 1.244210 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 150.00 | 150.0 | 150.0 | 0.017147 | 0.017147 | 0.000000 | 1.000000 | . 19 100020 | 2 | 41706.000000 | 48325.5 | 83412.000 | 7254.517500 | 8779.770 | 14509.035 | 39206.250000 | 45900.0 | 78412.500 | 0.000000 | 0.0 | 0.00 | 39206.250000 | 45900.0 | 78412.500 | 0.000000 | 0.000000 | 0.000000 | -472 | -243 | -357.500000 | 7.000000 | 14.0 | -2499.750000 | -2425.5 | -4999.50 | 1.066006 | 1.079170 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.026651 | 0.038290 | 0.000000 | 1.000000 | . 20 100021 | 6 | 153615.750000 | 675000.0 | 921694.500 | 10686.165000 | 33750.000 | 64116.990 | 40961.250000 | 99189.0 | 245767.500 | 3770.100000 | 9922.5 | 18850.50 | 49153.500000 | 99189.0 | 245767.500 | 0.000000 | 0.433539 | 0.106606 | -2811 | -261 | -943.833333 | 7.000000 | 42.0 | -112654.500000 | 8928.0 | -675927.00 | inf | inf | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 90.00 | 180.0 | 270.0 | -inf | 0.034033 | 0.000000 | 1.000000 | . 21 100022 | 1 | 103203.000000 | 103203.0 | 103203.000 | 6765.975000 | 6765.975 | 6765.975 | 89091.000000 | 89091.0 | 89091.000 | 0.000000 | 0.0 | 0.00 | 89091.000000 | 89091.0 | 89091.000 | 0.000000 | 0.000000 | 0.000000 | -239 | -239 | -239.000000 | 18.000000 | 18.0 | -14112.000000 | -14112.0 | -14112.00 | 1.158400 | 1.158400 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | NaN | NaN | 0.0 | 0.010004 | 0.010004 | 0.000000 | 1.000000 | . 22 100023 | 4 | 113517.000000 | 239242.5 | 454068.000 | 8895.892500 | 16822.440 | 35583.570 | 98746.875000 | 180000.0 | 394987.500 | 8435.250000 | 9000.0 | 16870.50 | 98746.875000 | 180000.0 | 394987.500 | 0.095959 | 0.101380 | 0.098669 | -2424 | -262 | -1215.750000 | 12.500000 | 50.0 | -14770.125000 | 2025.0 | -59080.50 | 1.080951 | 1.329125 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 240.00 | 600.0 | 720.0 | -inf | 0.028649 | 0.000000 | 1.000000 | . 23 100025 | 8 | 295425.781875 | 1042560.0 | 2363406.255 | 27645.595714 | 55678.095 | 193519.170 | 259795.344375 | 900000.0 | 2078362.755 | 5845.500000 | 23382.0 | 23382.00 | 296908.965000 | 900000.0 | 2078362.755 | 0.000000 | 0.423065 | 0.105766 | -2060 | -7 | -731.000000 | 13.714286 | 96.0 | -35630.437500 | 21645.0 | -285043.50 | 1.073930 | 1.269700 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 174.25 | 367.0 | 697.0 | 0.022884 | 0.027871 | 0.000000 | 0.875000 | . 24 100026 | 3 | 292281.000000 | 744498.0 | 876843.000 | 33193.665000 | 42778.845 | 66387.330 | 269115.000000 | 675000.0 | 807345.000 | 0.000000 | 0.0 | 0.00 | 403672.500000 | 675000.0 | 807345.000 | 0.000000 | 0.000000 | 0.000000 | -1427 | -152 | -682.333333 | 21.000000 | 42.0 | -23166.000000 | 0.0 | -69498.00 | 1.051480 | 1.102960 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 510.00 | 1020.0 | 1020.0 | 0.020701 | 0.029682 | 0.000000 | 0.666667 | . 25 100027 | 4 | 173795.625000 | 239850.0 | 695182.500 | 24067.755000 | 28077.570 | 72203.265 | 163621.125000 | 226984.5 | 654484.500 | 0.000000 | 0.0 | 0.00 | 218161.500000 | 226984.5 | 654484.500 | 0.000000 | 0.000000 | 0.000000 | -795 | -181 | -407.000000 | 11.333333 | 34.0 | -10174.500000 | 0.0 | -40698.00 | 1.062332 | 1.066000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.014849 | 0.017250 | 0.250000 | 0.500000 | . 26 100028 | 5 | 92920.500000 | 225000.0 | 464602.500 | 8091.585000 | 11250.000 | 24274.755 | 49207.500000 | 130765.5 | 246037.500 | 3750.000000 | 11250.0 | 11250.00 | 82012.500000 | 130765.5 | 246037.500 | 0.000000 | 0.173124 | 0.057708 | -1805 | -531 | -1124.200000 | 11.333333 | 34.0 | -43713.000000 | 6435.0 | -218565.00 | inf | inf | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 75.00 | 150.0 | 150.0 | -inf | 0.017550 | 0.000000 | 0.600000 | . 27 100029 | 1 | 126000.000000 | 126000.0 | 126000.000 | 22891.680000 | 22891.680 | 22891.680 | 126000.000000 | 126000.0 | 126000.000 | 0.000000 | 0.0 | 0.00 | 126000.000000 | 126000.0 | 126000.000 | 0.000000 | 0.000000 | 0.000000 | -324 | -324 | -324.000000 | 6.000000 | 6.0 | 0.000000 | 0.0 | 0.00 | 1.000000 | 1.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 60.00 | 60.0 | 60.0 | 0.015013 | 0.015013 | 0.000000 | 1.000000 | . 28 100030 | 12 | 32534.643750 | 49734.0 | 390415.725 | 4731.041250 | 9035.685 | 37848.330 | 33497.268750 | 47241.0 | 401967.225 | 1262.625000 | 6714.0 | 15151.50 | 33497.268750 | 47241.0 | 401967.225 | 0.000000 | 0.210919 | 0.044804 | -2856 | -378 | -1981.083333 | 8.625000 | 69.0 | 962.625000 | 5607.0 | 11551.50 | 0.965475 | 1.052772 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 60.00 | 90.0 | 120.0 | 0.017772 | 0.037233 | 0.833333 | 0.166667 | . 29 100032 | 1 | 42255.000000 | 42255.0 | 42255.000 | 7910.145000 | 7910.145 | 7910.145 | 69255.000000 | 69255.0 | 69255.000 | 27000.000000 | 27000.0 | 27000.00 | 69255.000000 | 69255.0 | 69255.000 | 0.424597 | 0.424597 | 0.424597 | -1497 | -1497 | -1497.000000 | 6.000000 | 6.0 | 27000.000000 | 27000.0 | 27000.00 | 0.610136 | 0.610136 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.00 | 0.0 | 0.0 | 0.020534 | 0.020534 | 0.000000 | 1.000000 | . &#44032;&#44277;&#46108; &#52572;&#51333; &#45936;&#51060;&#53552; &#49464;&#53944; &#49373;&#49457; . &#51648;&#44552;&#44620;&#51648; &#44284;&#51221;&#51012; &#54632;&#49688;&#54868; . &#51060;&#51204;&#50640; application &#45936;&#51060;&#53552; &#49464;&#53944;&#51032; feature engineering &#49688;&#54665; &#54980; &#49352;&#47213;&#44172; previous &#45936;&#51060;&#53552; &#49464;&#53944;&#47196; &#44032;&#44277;&#46108; &#45936;&#51060;&#53552;&#47484; &#51312;&#51064;. . import numpy as np import pandas as pd import gc import time import matplotlib.pyplot as plt import seaborn as sns import os %matplotlib inline pd.set_option(&#39;display.max_rows&#39;, 100) pd.set_option(&#39;display.max_columns&#39;, 200) . def get_dataset(): app_train = pd.read_csv(&#39;application_train.csv&#39;) app_test = pd.read_csv(&#39;application_test.csv&#39;) apps = pd.concat([app_train, app_test]) prev = pd.read_csv(&#39;previous_application.csv&#39;) return apps, prev apps, prev = get_dataset() .",
            "url": "https://gwonchankim.github.io/channee/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_3.html",
            "relUrl": "/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_3.html",
            "date": " • Aug 31, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Project2-2",
            "content": "&#51452;&#50836; Feature&#46308;&#50640; &#45824;&#54620; feature engineering &#49688;&#54665; . EXT_SOURCE &#44228;&#50676;&#44050; &#54869;&#51064;, EXT_SOURCE_X &#54588;&#52376;&#46308;&#51032; &#54217;&#44512;/&#52572;&#45824;/&#52572;&#49548;/&#54364;&#51456;&#54200;&#52264; &#54869;&#51064; . app_train[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].isnull().sum() . EXT_SOURCE_1 173378 EXT_SOURCE_2 660 EXT_SOURCE_3 60965 dtype: int64 . app_train[&#39;EXT_SOURCE_1&#39;].value_counts(dropna=False) . NaN 173378 0.546426 5 0.598469 5 0.622707 5 0.605152 5 ... 0.288500 1 0.807598 1 0.237983 1 0.412337 1 0.615170 1 Name: EXT_SOURCE_1, Length: 114585, dtype: int64 . app_train[&#39;EXT_SOURCE_2&#39;].value_counts(dropna=False) . 0.285898 721 NaN 660 0.262258 417 0.265256 343 0.159679 322 ... 0.009063 1 0.457284 1 0.017615 1 0.371661 1 0.064909 1 Name: EXT_SOURCE_2, Length: 119832, dtype: int64 . app_train[&#39;EXT_SOURCE_3&#39;].value_counts(dropna=False) . NaN 60965 0.746300 1460 0.713631 1315 0.694093 1276 0.670652 1191 ... 0.028674 1 0.896010 1 0.858178 1 0.017506 1 0.028275 1 Name: EXT_SOURCE_3, Length: 815, dtype: int64 . print(&#39;### mean ### n&#39;, app_train[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].mean()) print(&#39;### max ### n&#39;,app_train[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].max()) print(&#39;### min ### n&#39;,app_train[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].min()) print(&#39;### std ### n&#39;,app_train[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].std()) . ### mean ### EXT_SOURCE_1 0.502130 EXT_SOURCE_2 0.514393 EXT_SOURCE_3 0.510853 dtype: float64 ### max ### EXT_SOURCE_1 0.962693 EXT_SOURCE_2 0.855000 EXT_SOURCE_3 0.896010 dtype: float64 ### min ### EXT_SOURCE_1 1.456813e-02 EXT_SOURCE_2 8.173617e-08 EXT_SOURCE_3 5.272652e-04 dtype: float64 ### std ### EXT_SOURCE_1 0.211062 EXT_SOURCE_2 0.191060 EXT_SOURCE_3 0.194844 dtype: float64 . &#45936;&#51060;&#53552; &#44032;&#44277; &#51204; &#54617;&#49845;&#44284; &#53580;&#49828;&#53944;&#50857; &#45936;&#51060;&#53552; &#49464;&#53944; &#44208;&#54633; . apps = pd.concat([app_train, app_test]) print(apps.shape) . (356255, 122) . EXT_SOURCE_X FEATURE &#44032;&#44277; . EXT_SOURCE_X 피처들을 결합하여 평균과 표준 편차를 신규 생성. | . apps[&#39;APPS_EXT_SOURCE_MEAN&#39;] = apps[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].mean(axis=1) apps[&#39;APPS_EXT_SOURCE_STD&#39;] = apps[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].std(axis=1) #apps[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;, &#39;APPS_EXT_SOURCE_MEAN&#39;, &#39;APPS_EXT_SOURCE_STD&#39;]].head(10) . apps[&#39;APPS_EXT_SOURCE_STD&#39;].isnull().sum() . 40950 . apps[&#39;APPS_EXT_SOURCE_STD&#39;] = apps[&#39;APPS_EXT_SOURCE_STD&#39;].fillna(apps[&#39;APPS_EXT_SOURCE_STD&#39;].mean()) apps[&#39;APPS_EXT_SOURCE_STD&#39;].isnull().sum() . 0 . AMT_CREDIT &#48708;&#50984;&#47196; Feature &#44032;&#44277; . apps[&#39;APPS_ANNUITY_CREDIT_RATIO&#39;] = apps[&#39;AMT_ANNUITY&#39;]/apps[&#39;AMT_CREDIT&#39;] apps[&#39;APPS_GOODS_CREDIT_RATIO&#39;] = apps[&#39;AMT_GOODS_PRICE&#39;]/apps[&#39;AMT_CREDIT&#39;] apps[&#39;APPS_CREDIT_GOODS_DIFF&#39;] = apps[&#39;AMT_CREDIT&#39;] - apps[&#39;AMT_GOODS_PRICE&#39;] . AMT_INCOME_TOTAL &#48708;&#50984;&#47196; Feature &#44032;&#44277; . apps[&#39;APPS_ANNUITY_INCOME_RATIO&#39;] = apps[&#39;AMT_ANNUITY&#39;]/apps[&#39;AMT_INCOME_TOTAL&#39;] apps[&#39;APPS_CREDIT_INCOME_RATIO&#39;] = apps[&#39;AMT_CREDIT&#39;]/apps[&#39;AMT_INCOME_TOTAL&#39;] apps[&#39;APPS_GOODS_INCOME_RATIO&#39;] = apps[&#39;AMT_GOODS_PRICE&#39;]/apps[&#39;AMT_INCOME_TOTAL&#39;] # 가족수를 고려한 가처분 소득 피처 가공. apps[&#39;APPS_CNT_FAM_INCOME_RATIO&#39;] = apps[&#39;AMT_INCOME_TOTAL&#39;]/apps[&#39;CNT_FAM_MEMBERS&#39;] . DAYS_BIRTH, DAYS_EMPLOYED &#48708;&#50984;&#47196; Feature &#44032;&#44277;. . apps[&#39;APPS_EMPLOYED_BIRTH_RATIO&#39;] = apps[&#39;DAYS_EMPLOYED&#39;]/apps[&#39;DAYS_BIRTH&#39;] apps[&#39;APPS_INCOME_EMPLOYED_RATIO&#39;] = apps[&#39;AMT_INCOME_TOTAL&#39;]/apps[&#39;DAYS_EMPLOYED&#39;] apps[&#39;APPS_INCOME_BIRTH_RATIO&#39;] = apps[&#39;AMT_INCOME_TOTAL&#39;]/apps[&#39;DAYS_BIRTH&#39;] apps[&#39;APPS_CAR_BIRTH_RATIO&#39;] = apps[&#39;OWN_CAR_AGE&#39;] / apps[&#39;DAYS_BIRTH&#39;] apps[&#39;APPS_CAR_EMPLOYED_RATIO&#39;] = apps[&#39;OWN_CAR_AGE&#39;] / apps[&#39;DAYS_EMPLOYED&#39;] . &#45936;&#51060;&#53552; &#47112;&#51060;&#48660; &#51064;&#53076;&#46377;, NULL&#44050;&#51008; LightGBM &#45236;&#48512;&#50640;&#49436; &#52376;&#47532;&#54616;&#46020;&#47197; &#53945;&#48324;&#54620; &#48320;&#44221;&#54616;&#51648; &#50506;&#51020;. . object_columns = apps.dtypes[apps.dtypes == &#39;object&#39;].index.tolist() for column in object_columns: apps[column] = pd.factorize(apps[column])[0] apps.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 356255 entries, 0 to 48743 Columns: 136 entries, SK_ID_CURR to APPS_CAR_EMPLOYED_RATIO dtypes: float64(81), int64(55) memory usage: 372.4 MB . &#54617;&#49845; &#45936;&#51060;&#53552;&#50752; &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#45796;&#49884; &#48516;&#47532; . apps_train = apps[~apps[&#39;TARGET&#39;].isnull()] apps_test = apps[apps[&#39;TARGET&#39;].isnull()] apps_test = apps_test.drop(&#39;TARGET&#39;, axis=1) . &#54617;&#49845; &#45936;&#51060;&#53552;&#47484; &#44160;&#51613; &#45936;&#51060;&#53552;&#47196; &#48516;&#47532;&#54616;&#44256; LGBM Classifier&#47196; &#54617;&#49845; &#49688;&#54665;. . from sklearn.model_selection import train_test_split ftr_app = apps_train.drop([&#39;SK_ID_CURR&#39;, &#39;TARGET&#39;], axis=1) target_app = app_train[&#39;TARGET&#39;] train_x, valid_x, train_y, valid_y = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2021) train_x.shape, valid_x.shape . ((215257, 134), (92254, 134)) . from lightgbm import LGBMClassifier clf = LGBMClassifier( n_jobs=-1, n_estimators=1000, learning_rate=0.02, num_leaves=32, subsample=0.8, max_depth=12, silent=-1, verbose=-1 ) clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= &#39;auc&#39;, verbose= 100, early_stopping_rounds= 100) . Training until validation scores don&#39;t improve for 100 rounds [100] training&#39;s auc: 0.759928 training&#39;s binary_logloss: 0.247073 valid_1&#39;s auc: 0.748989 valid_1&#39;s binary_logloss: 0.250541 [200] training&#39;s auc: 0.780716 training&#39;s binary_logloss: 0.240017 valid_1&#39;s auc: 0.759834 valid_1&#39;s binary_logloss: 0.246548 [300] training&#39;s auc: 0.79523 training&#39;s binary_logloss: 0.235347 valid_1&#39;s auc: 0.76416 valid_1&#39;s binary_logloss: 0.245088 [400] training&#39;s auc: 0.80741 training&#39;s binary_logloss: 0.231545 valid_1&#39;s auc: 0.766056 valid_1&#39;s binary_logloss: 0.244437 [500] training&#39;s auc: 0.817543 training&#39;s binary_logloss: 0.228327 valid_1&#39;s auc: 0.766691 valid_1&#39;s binary_logloss: 0.244215 [600] training&#39;s auc: 0.826604 training&#39;s binary_logloss: 0.225323 valid_1&#39;s auc: 0.766923 valid_1&#39;s binary_logloss: 0.244104 [700] training&#39;s auc: 0.83524 training&#39;s binary_logloss: 0.222432 valid_1&#39;s auc: 0.767104 valid_1&#39;s binary_logloss: 0.244014 [800] training&#39;s auc: 0.842682 training&#39;s binary_logloss: 0.219748 valid_1&#39;s auc: 0.76728 valid_1&#39;s binary_logloss: 0.243931 [900] training&#39;s auc: 0.850266 training&#39;s binary_logloss: 0.217025 valid_1&#39;s auc: 0.767524 valid_1&#39;s binary_logloss: 0.243844 [1000] training&#39;s auc: 0.857485 training&#39;s binary_logloss: 0.21439 valid_1&#39;s auc: 0.767598 valid_1&#39;s binary_logloss: 0.243825 Did not meet early stopping. Best iteration is: [1000] training&#39;s auc: 0.857485 training&#39;s binary_logloss: 0.21439 valid_1&#39;s auc: 0.767598 valid_1&#39;s binary_logloss: 0.243825 . LGBMClassifier(learning_rate=0.02, max_depth=12, n_estimators=1000, num_leaves=32, silent=-1, subsample=0.8, verbose=-1) . &#44208;&#44284;&#48516;&#49437; . feature engineering이전의 AUC 값(0.757631) | feature engineering이후의 AUC 값(0.767598) | 결과가 좋아졌다는 사실을 알 수 있다. | . from lightgbm import plot_importance plot_importance(clf, figsize=(16, 32)) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;Feature importance&#39;, ylabel=&#39;Features&#39;&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T14:26:19.795990 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ def get_apps_processed(apps): # EXT_SOURCE_X FEATURE 가공 apps[&#39;APPS_EXT_SOURCE_MEAN&#39;] = apps[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].mean(axis=1) apps[&#39;APPS_EXT_SOURCE_STD&#39;] = apps[[&#39;EXT_SOURCE_1&#39;, &#39;EXT_SOURCE_2&#39;, &#39;EXT_SOURCE_3&#39;]].std(axis=1) apps[&#39;APPS_EXT_SOURCE_STD&#39;] = apps[&#39;APPS_EXT_SOURCE_STD&#39;].fillna(apps[&#39;APPS_EXT_SOURCE_STD&#39;].mean()) # AMT_CREDIT 비율로 Feature 가공 apps[&#39;APPS_ANNUITY_CREDIT_RATIO&#39;] = apps[&#39;AMT_ANNUITY&#39;]/apps[&#39;AMT_CREDIT&#39;] apps[&#39;APPS_GOODS_CREDIT_RATIO&#39;] = apps[&#39;AMT_GOODS_PRICE&#39;]/apps[&#39;AMT_CREDIT&#39;] # AMT_INCOME_TOTAL 비율로 Feature 가공 apps[&#39;APPS_ANNUITY_INCOME_RATIO&#39;] = apps[&#39;AMT_ANNUITY&#39;]/apps[&#39;AMT_INCOME_TOTAL&#39;] apps[&#39;APPS_CREDIT_INCOME_RATIO&#39;] = apps[&#39;AMT_CREDIT&#39;]/apps[&#39;AMT_INCOME_TOTAL&#39;] apps[&#39;APPS_GOODS_INCOME_RATIO&#39;] = apps[&#39;AMT_GOODS_PRICE&#39;]/apps[&#39;AMT_INCOME_TOTAL&#39;] apps[&#39;APPS_CNT_FAM_INCOME_RATIO&#39;] = apps[&#39;AMT_INCOME_TOTAL&#39;]/apps[&#39;CNT_FAM_MEMBERS&#39;] # DAYS_BIRTH, DAYS_EMPLOYED 비율로 Feature 가공 apps[&#39;APPS_EMPLOYED_BIRTH_RATIO&#39;] = apps[&#39;DAYS_EMPLOYED&#39;]/apps[&#39;DAYS_BIRTH&#39;] apps[&#39;APPS_INCOME_EMPLOYED_RATIO&#39;] = apps[&#39;AMT_INCOME_TOTAL&#39;]/apps[&#39;DAYS_EMPLOYED&#39;] apps[&#39;APPS_INCOME_BIRTH_RATIO&#39;] = apps[&#39;AMT_INCOME_TOTAL&#39;]/apps[&#39;DAYS_BIRTH&#39;] apps[&#39;APPS_CAR_BIRTH_RATIO&#39;] = apps[&#39;OWN_CAR_AGE&#39;] / apps[&#39;DAYS_BIRTH&#39;] apps[&#39;APPS_CAR_EMPLOYED_RATIO&#39;] = apps[&#39;OWN_CAR_AGE&#39;] / apps[&#39;DAYS_EMPLOYED&#39;] return apps `bbb` . from category_encoders import TargetEncoder from sklearn.impute import SimpleImputer, KNNImputer # 사이킷런 - 파이프라인 from sklearn.pipeline import make_pipeline # 사이킷런 - 학습모델 from sklearn.ensemble import RandomForestClassifier # 사이킷런 - 모델 평가 지표 from sklearn.metrics import accuracy_score, mean_absolute_error, f1_score # from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, validation_curve . from IPython.display import Image .",
            "url": "https://gwonchankim.github.io/channee/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_2.html",
            "relUrl": "/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_2.html",
            "date": " • Aug 31, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Project2-1",
            "content": "&#45936;&#51060;&#53552; &#49440;&#51221; . COVID-19의 영향으로 가계대출이 급증하였다. 에에 가계부채가 사상 최대치르 찍으며 주요 은행들로부터 대출 규졔가 시작되었습니다. | 이에 대해 은행 입장에서 채무(Target)를 갚으 것인지에 대한 예측을 해보고자 합니다. | 데이터는 Kaggle의 &#39;Home Credit Default Risk&#39;로 부터 가져왔습니다. | . import numpy as np import pandas as pd import gc import time import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline pd.set_option(&#39;display.max_rows&#39;, 100) pd.set_option(&#39;display.max_columns&#39;, 200) . 데이터를 불러와 살펴보겠습니다. | . app_train = pd.read_csv(&#39;application_train.csv&#39;) app_test = pd.read_csv(&#39;application_test.csv&#39;) . app_train.head() . SK_ID_CURR TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY CNT_CHILDREN AMT_INCOME_TOTAL AMT_CREDIT AMT_ANNUITY AMT_GOODS_PRICE NAME_TYPE_SUITE NAME_INCOME_TYPE NAME_EDUCATION_TYPE NAME_FAMILY_STATUS NAME_HOUSING_TYPE REGION_POPULATION_RELATIVE DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH OWN_CAR_AGE FLAG_MOBIL FLAG_EMP_PHONE FLAG_WORK_PHONE FLAG_CONT_MOBILE FLAG_PHONE FLAG_EMAIL OCCUPATION_TYPE CNT_FAM_MEMBERS REGION_RATING_CLIENT REGION_RATING_CLIENT_W_CITY WEEKDAY_APPR_PROCESS_START HOUR_APPR_PROCESS_START REG_REGION_NOT_LIVE_REGION REG_REGION_NOT_WORK_REGION LIVE_REGION_NOT_WORK_REGION REG_CITY_NOT_LIVE_CITY REG_CITY_NOT_WORK_CITY LIVE_CITY_NOT_WORK_CITY ORGANIZATION_TYPE EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3 APARTMENTS_AVG BASEMENTAREA_AVG YEARS_BEGINEXPLUATATION_AVG YEARS_BUILD_AVG COMMONAREA_AVG ELEVATORS_AVG ENTRANCES_AVG FLOORSMAX_AVG FLOORSMIN_AVG LANDAREA_AVG LIVINGAPARTMENTS_AVG LIVINGAREA_AVG NONLIVINGAPARTMENTS_AVG NONLIVINGAREA_AVG APARTMENTS_MODE BASEMENTAREA_MODE YEARS_BEGINEXPLUATATION_MODE YEARS_BUILD_MODE COMMONAREA_MODE ELEVATORS_MODE ENTRANCES_MODE FLOORSMAX_MODE FLOORSMIN_MODE LANDAREA_MODE LIVINGAPARTMENTS_MODE LIVINGAREA_MODE NONLIVINGAPARTMENTS_MODE NONLIVINGAREA_MODE APARTMENTS_MEDI BASEMENTAREA_MEDI YEARS_BEGINEXPLUATATION_MEDI YEARS_BUILD_MEDI COMMONAREA_MEDI ELEVATORS_MEDI ENTRANCES_MEDI FLOORSMAX_MEDI FLOORSMIN_MEDI LANDAREA_MEDI LIVINGAPARTMENTS_MEDI LIVINGAREA_MEDI NONLIVINGAPARTMENTS_MEDI NONLIVINGAREA_MEDI FONDKAPREMONT_MODE HOUSETYPE_MODE TOTALAREA_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE OBS_30_CNT_SOCIAL_CIRCLE DEF_30_CNT_SOCIAL_CIRCLE OBS_60_CNT_SOCIAL_CIRCLE DEF_60_CNT_SOCIAL_CIRCLE DAYS_LAST_PHONE_CHANGE FLAG_DOCUMENT_2 FLAG_DOCUMENT_3 FLAG_DOCUMENT_4 FLAG_DOCUMENT_5 FLAG_DOCUMENT_6 FLAG_DOCUMENT_7 FLAG_DOCUMENT_8 FLAG_DOCUMENT_9 FLAG_DOCUMENT_10 FLAG_DOCUMENT_11 FLAG_DOCUMENT_12 FLAG_DOCUMENT_13 FLAG_DOCUMENT_14 FLAG_DOCUMENT_15 FLAG_DOCUMENT_16 FLAG_DOCUMENT_17 FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR . 0 100002 | 1 | Cash loans | M | N | Y | 0 | 202500.0 | 406597.5 | 24700.5 | 351000.0 | Unaccompanied | Working | Secondary / secondary special | Single / not married | House / apartment | 0.018801 | -9461 | -637 | -3648.0 | -2120 | NaN | 1 | 1 | 0 | 1 | 1 | 0 | Laborers | 1.0 | 2 | 2 | WEDNESDAY | 10 | 0 | 0 | 0 | 0 | 0 | 0 | Business Entity Type 3 | 0.083037 | 0.262949 | 0.139376 | 0.0247 | 0.0369 | 0.9722 | 0.6192 | 0.0143 | 0.00 | 0.0690 | 0.0833 | 0.1250 | 0.0369 | 0.0202 | 0.0190 | 0.0000 | 0.0000 | 0.0252 | 0.0383 | 0.9722 | 0.6341 | 0.0144 | 0.0000 | 0.0690 | 0.0833 | 0.1250 | 0.0377 | 0.022 | 0.0198 | 0.0 | 0.0 | 0.0250 | 0.0369 | 0.9722 | 0.6243 | 0.0144 | 0.00 | 0.0690 | 0.0833 | 0.1250 | 0.0375 | 0.0205 | 0.0193 | 0.0000 | 0.00 | reg oper account | block of flats | 0.0149 | Stone, brick | No | 2.0 | 2.0 | 2.0 | 2.0 | -1134.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 1 100003 | 0 | Cash loans | F | N | N | 0 | 270000.0 | 1293502.5 | 35698.5 | 1129500.0 | Family | State servant | Higher education | Married | House / apartment | 0.003541 | -16765 | -1188 | -1186.0 | -291 | NaN | 1 | 1 | 0 | 1 | 1 | 0 | Core staff | 2.0 | 1 | 1 | MONDAY | 11 | 0 | 0 | 0 | 0 | 0 | 0 | School | 0.311267 | 0.622246 | NaN | 0.0959 | 0.0529 | 0.9851 | 0.7960 | 0.0605 | 0.08 | 0.0345 | 0.2917 | 0.3333 | 0.0130 | 0.0773 | 0.0549 | 0.0039 | 0.0098 | 0.0924 | 0.0538 | 0.9851 | 0.8040 | 0.0497 | 0.0806 | 0.0345 | 0.2917 | 0.3333 | 0.0128 | 0.079 | 0.0554 | 0.0 | 0.0 | 0.0968 | 0.0529 | 0.9851 | 0.7987 | 0.0608 | 0.08 | 0.0345 | 0.2917 | 0.3333 | 0.0132 | 0.0787 | 0.0558 | 0.0039 | 0.01 | reg oper account | block of flats | 0.0714 | Block | No | 1.0 | 0.0 | 1.0 | 0.0 | -828.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 100004 | 0 | Revolving loans | M | Y | Y | 0 | 67500.0 | 135000.0 | 6750.0 | 135000.0 | Unaccompanied | Working | Secondary / secondary special | Single / not married | House / apartment | 0.010032 | -19046 | -225 | -4260.0 | -2531 | 26.0 | 1 | 1 | 1 | 1 | 1 | 0 | Laborers | 1.0 | 2 | 2 | MONDAY | 9 | 0 | 0 | 0 | 0 | 0 | 0 | Government | NaN | 0.555912 | 0.729567 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.0 | 0.0 | 0.0 | 0.0 | -815.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 100006 | 0 | Cash loans | F | N | Y | 0 | 135000.0 | 312682.5 | 29686.5 | 297000.0 | Unaccompanied | Working | Secondary / secondary special | Civil marriage | House / apartment | 0.008019 | -19005 | -3039 | -9833.0 | -2437 | NaN | 1 | 1 | 0 | 1 | 0 | 0 | Laborers | 2.0 | 2 | 2 | WEDNESDAY | 17 | 0 | 0 | 0 | 0 | 0 | 0 | Business Entity Type 3 | NaN | 0.650442 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2.0 | 0.0 | 2.0 | 0.0 | -617.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 100007 | 0 | Cash loans | M | N | Y | 0 | 121500.0 | 513000.0 | 21865.5 | 513000.0 | Unaccompanied | Working | Secondary / secondary special | Single / not married | House / apartment | 0.028663 | -19932 | -3038 | -4311.0 | -3458 | NaN | 1 | 1 | 0 | 1 | 0 | 0 | Core staff | 1.0 | 2 | 2 | THURSDAY | 11 | 0 | 0 | 0 | 0 | 1 | 1 | Religion | NaN | 0.322738 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.0 | 0.0 | 0.0 | 0.0 | -1106.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Feature &#49444;&#47749;(&#45320;&#47924; &#47566;&#51008; Feature&#47196; &#51068;&#48512;&#47564; &#49548;&#44060;&#54616;&#46020;&#47197; &#54616;&#44192;&#49845;&#45768;&#45796;.) . SK_ID_CURR : 현재 대출 고유 ID | TARGET : 타겟값(채무 이행/불이행) | AMT_INCOME_TOTAL : 소득 | AMT_CREDIT : 대출금액 | AMT_ANNUITY : 월 대출 지급액 | AMT_GOODS_PRICE : 소비자 대출 상품액 | EXT_SOURCE_1 ~ EXT_SOURCE_3 : 정규화된 스코어 | APARTMENTS_AVG ~ EMERGENCYSTATE_MODE : 고객 거주지역의 특정값(정규화된 값) | OBS_30_CNT_SOCIAL_CIRCLE : 30일 연체된 횟수 | DEF_30_CNT_SOCIAL_CIRCLE : 30일 연체로 Default 된 횟수 | OBS_60_CNT_SOCIAL_CIRCLE : 60일 연체된 횟수 | DEF_60_CNT_SOCIAL_CIRCLE : 60일 연체로 Default 된 횟수 | DAYS_LAST_PHONE_CHANGE : 대출 신청전 핸드폰 변경 횟수 | FLAG_DOCUMENT_2 ~ FLAG_DOCUMENT_21 : 문서 제출 여부 | AMT_INSTALMENT : 대출 월 납입예정액 | AMT_PAYMENT : 실제 월 납입액 | . app_train.shape, app_test.shape # 매우 많은 feature과 Data.. . ((307511, 122), (48744, 121)) . TARGET &#44050; &#48516;&#54252; &#48143; AMT_INCOME_TOTAL &#44050; Histogram . TARGET값 별 분포도, Pandas, Matplotlib, Seaborn으로 histogram 표현 | . app_train[&#39;TARGET&#39;].value_counts() # 치우쳐진(Skew) Data이다. # 채무 이행을 하는 사람(0)이 불이행 하는 사람(1)보다 훨씬 많다. . 0 282686 1 24825 Name: TARGET, dtype: int64 . app_train[&#39;AMT_CREDIT&#39;].hist() # 고액 대출보다 소액대출의 비중이 높다. . &lt;AxesSubplot:&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T10:55:21.437202 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ sns.distplot(app_train[&#39;AMT_INCOME_TOTAL&#39;]) # 너무 명백하게 치우쳐진 그래프이다 # 월 소득이 적은 사람이 많은 사람보다 월등하게 많다. . C: Users channee anaconda3 lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;AxesSubplot:xlabel=&#39;AMT_INCOME_TOTAL&#39;, ylabel=&#39;Density&#39;&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T10:55:55.135078 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ sns.boxplot(app_train[&#39;AMT_INCOME_TOTAL&#39;]) . C: Users channee anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . &lt;AxesSubplot:xlabel=&#39;AMT_INCOME_TOTAL&#39;&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T10:55:59.305882 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ AMT_INCOME_TOTAL&#51008; &#47588;&#50864; &#50864;&#52769;&#51004;&#47196; &#52824;&#50864;&#52432;&#51652;(Skew) &#45936;&#51060;&#53552;&#51060;&#45796;. . AMT_INCOME_TOTAL&#51060; 1000000 &#51060;&#54616;&#51064; &#44050;&#50640; &#45824;&#54620; &#48516;&#54252;&#46020;&#47484; &#49332;&#54196;&#48372;&#51088;. . boolean indexing으로 filtering 후 histogram 표현 | . app_train[app_train[&#39;AMT_INCOME_TOTAL&#39;] &lt; 1000000] . SK_ID_CURR TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY CNT_CHILDREN AMT_INCOME_TOTAL AMT_CREDIT AMT_ANNUITY AMT_GOODS_PRICE NAME_TYPE_SUITE NAME_INCOME_TYPE NAME_EDUCATION_TYPE NAME_FAMILY_STATUS NAME_HOUSING_TYPE REGION_POPULATION_RELATIVE DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH OWN_CAR_AGE FLAG_MOBIL FLAG_EMP_PHONE FLAG_WORK_PHONE FLAG_CONT_MOBILE FLAG_PHONE FLAG_EMAIL OCCUPATION_TYPE CNT_FAM_MEMBERS REGION_RATING_CLIENT REGION_RATING_CLIENT_W_CITY WEEKDAY_APPR_PROCESS_START HOUR_APPR_PROCESS_START REG_REGION_NOT_LIVE_REGION REG_REGION_NOT_WORK_REGION LIVE_REGION_NOT_WORK_REGION REG_CITY_NOT_LIVE_CITY REG_CITY_NOT_WORK_CITY LIVE_CITY_NOT_WORK_CITY ORGANIZATION_TYPE EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3 APARTMENTS_AVG BASEMENTAREA_AVG YEARS_BEGINEXPLUATATION_AVG YEARS_BUILD_AVG COMMONAREA_AVG ELEVATORS_AVG ENTRANCES_AVG FLOORSMAX_AVG FLOORSMIN_AVG LANDAREA_AVG LIVINGAPARTMENTS_AVG LIVINGAREA_AVG NONLIVINGAPARTMENTS_AVG NONLIVINGAREA_AVG APARTMENTS_MODE BASEMENTAREA_MODE YEARS_BEGINEXPLUATATION_MODE YEARS_BUILD_MODE COMMONAREA_MODE ELEVATORS_MODE ENTRANCES_MODE FLOORSMAX_MODE FLOORSMIN_MODE LANDAREA_MODE LIVINGAPARTMENTS_MODE LIVINGAREA_MODE NONLIVINGAPARTMENTS_MODE NONLIVINGAREA_MODE APARTMENTS_MEDI BASEMENTAREA_MEDI YEARS_BEGINEXPLUATATION_MEDI YEARS_BUILD_MEDI COMMONAREA_MEDI ELEVATORS_MEDI ENTRANCES_MEDI FLOORSMAX_MEDI FLOORSMIN_MEDI LANDAREA_MEDI LIVINGAPARTMENTS_MEDI LIVINGAREA_MEDI NONLIVINGAPARTMENTS_MEDI NONLIVINGAREA_MEDI FONDKAPREMONT_MODE HOUSETYPE_MODE TOTALAREA_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE OBS_30_CNT_SOCIAL_CIRCLE DEF_30_CNT_SOCIAL_CIRCLE OBS_60_CNT_SOCIAL_CIRCLE DEF_60_CNT_SOCIAL_CIRCLE DAYS_LAST_PHONE_CHANGE FLAG_DOCUMENT_2 FLAG_DOCUMENT_3 FLAG_DOCUMENT_4 FLAG_DOCUMENT_5 FLAG_DOCUMENT_6 FLAG_DOCUMENT_7 FLAG_DOCUMENT_8 FLAG_DOCUMENT_9 FLAG_DOCUMENT_10 FLAG_DOCUMENT_11 FLAG_DOCUMENT_12 FLAG_DOCUMENT_13 FLAG_DOCUMENT_14 FLAG_DOCUMENT_15 FLAG_DOCUMENT_16 FLAG_DOCUMENT_17 FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR . 0 100002 | 1 | Cash loans | M | N | Y | 0 | 202500.0 | 406597.5 | 24700.5 | 351000.0 | Unaccompanied | Working | Secondary / secondary special | Single / not married | House / apartment | 0.018801 | -9461 | -637 | -3648.0 | -2120 | NaN | 1 | 1 | 0 | 1 | 1 | 0 | Laborers | 1.0 | 2 | 2 | WEDNESDAY | 10 | 0 | 0 | 0 | 0 | 0 | 0 | Business Entity Type 3 | 0.083037 | 0.262949 | 0.139376 | 0.0247 | 0.0369 | 0.9722 | 0.6192 | 0.0143 | 0.00 | 0.0690 | 0.0833 | 0.1250 | 0.0369 | 0.0202 | 0.0190 | 0.0000 | 0.0000 | 0.0252 | 0.0383 | 0.9722 | 0.6341 | 0.0144 | 0.0000 | 0.0690 | 0.0833 | 0.1250 | 0.0377 | 0.0220 | 0.0198 | 0.0 | 0.0000 | 0.0250 | 0.0369 | 0.9722 | 0.6243 | 0.0144 | 0.00 | 0.0690 | 0.0833 | 0.1250 | 0.0375 | 0.0205 | 0.0193 | 0.0000 | 0.0000 | reg oper account | block of flats | 0.0149 | Stone, brick | No | 2.0 | 2.0 | 2.0 | 2.0 | -1134.0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 1 100003 | 0 | Cash loans | F | N | N | 0 | 270000.0 | 1293502.5 | 35698.5 | 1129500.0 | Family | State servant | Higher education | Married | House / apartment | 0.003541 | -16765 | -1188 | -1186.0 | -291 | NaN | 1 | 1 | 0 | 1 | 1 | 0 | Core staff | 2.0 | 1 | 1 | MONDAY | 11 | 0 | 0 | 0 | 0 | 0 | 0 | School | 0.311267 | 0.622246 | NaN | 0.0959 | 0.0529 | 0.9851 | 0.7960 | 0.0605 | 0.08 | 0.0345 | 0.2917 | 0.3333 | 0.0130 | 0.0773 | 0.0549 | 0.0039 | 0.0098 | 0.0924 | 0.0538 | 0.9851 | 0.8040 | 0.0497 | 0.0806 | 0.0345 | 0.2917 | 0.3333 | 0.0128 | 0.0790 | 0.0554 | 0.0 | 0.0000 | 0.0968 | 0.0529 | 0.9851 | 0.7987 | 0.0608 | 0.08 | 0.0345 | 0.2917 | 0.3333 | 0.0132 | 0.0787 | 0.0558 | 0.0039 | 0.0100 | reg oper account | block of flats | 0.0714 | Block | No | 1.0 | 0.0 | 1.0 | 0.0 | -828.0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 100004 | 0 | Revolving loans | M | Y | Y | 0 | 67500.0 | 135000.0 | 6750.0 | 135000.0 | Unaccompanied | Working | Secondary / secondary special | Single / not married | House / apartment | 0.010032 | -19046 | -225 | -4260.0 | -2531 | 26.0 | 1 | 1 | 1 | 1 | 1 | 0 | Laborers | 1.0 | 2 | 2 | MONDAY | 9 | 0 | 0 | 0 | 0 | 0 | 0 | Government | NaN | 0.555912 | 0.729567 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.0 | 0.0 | 0.0 | 0.0 | -815.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 100006 | 0 | Cash loans | F | N | Y | 0 | 135000.0 | 312682.5 | 29686.5 | 297000.0 | Unaccompanied | Working | Secondary / secondary special | Civil marriage | House / apartment | 0.008019 | -19005 | -3039 | -9833.0 | -2437 | NaN | 1 | 1 | 0 | 1 | 0 | 0 | Laborers | 2.0 | 2 | 2 | WEDNESDAY | 17 | 0 | 0 | 0 | 0 | 0 | 0 | Business Entity Type 3 | NaN | 0.650442 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2.0 | 0.0 | 2.0 | 0.0 | -617.0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 100007 | 0 | Cash loans | M | N | Y | 0 | 121500.0 | 513000.0 | 21865.5 | 513000.0 | Unaccompanied | Working | Secondary / secondary special | Single / not married | House / apartment | 0.028663 | -19932 | -3038 | -4311.0 | -3458 | NaN | 1 | 1 | 0 | 1 | 0 | 0 | Core staff | 1.0 | 2 | 2 | THURSDAY | 11 | 0 | 0 | 0 | 0 | 1 | 1 | Religion | NaN | 0.322738 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.0 | 0.0 | 0.0 | 0.0 | -1106.0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 307506 456251 | 0 | Cash loans | M | N | N | 0 | 157500.0 | 254700.0 | 27558.0 | 225000.0 | Unaccompanied | Working | Secondary / secondary special | Separated | With parents | 0.032561 | -9327 | -236 | -8456.0 | -1982 | NaN | 1 | 1 | 0 | 1 | 0 | 0 | Sales staff | 1.0 | 1 | 1 | THURSDAY | 15 | 0 | 0 | 0 | 0 | 0 | 0 | Services | 0.145570 | 0.681632 | NaN | 0.2021 | 0.0887 | 0.9876 | 0.8300 | 0.0202 | 0.22 | 0.1034 | 0.6042 | 0.2708 | 0.0594 | 0.1484 | 0.1965 | 0.0753 | 0.1095 | 0.1008 | 0.0172 | 0.9782 | 0.7125 | 0.0172 | 0.0806 | 0.0345 | 0.4583 | 0.0417 | 0.0094 | 0.0882 | 0.0853 | 0.0 | 0.0125 | 0.2040 | 0.0887 | 0.9876 | 0.8323 | 0.0203 | 0.22 | 0.1034 | 0.6042 | 0.2708 | 0.0605 | 0.1509 | 0.2001 | 0.0757 | 0.1118 | reg oper account | block of flats | 0.2898 | Stone, brick | No | 0.0 | 0.0 | 0.0 | 0.0 | -273.0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | NaN | NaN | NaN | . 307507 456252 | 0 | Cash loans | F | N | Y | 0 | 72000.0 | 269550.0 | 12001.5 | 225000.0 | Unaccompanied | Pensioner | Secondary / secondary special | Widow | House / apartment | 0.025164 | -20775 | 365243 | -4388.0 | -4090 | NaN | 1 | 0 | 0 | 1 | 1 | 0 | NaN | 1.0 | 2 | 2 | MONDAY | 8 | 0 | 0 | 0 | 0 | 0 | 0 | XNA | NaN | 0.115992 | NaN | 0.0247 | 0.0435 | 0.9727 | 0.6260 | 0.0022 | 0.00 | 0.1034 | 0.0833 | 0.1250 | 0.0579 | 0.0202 | 0.0257 | 0.0000 | 0.0000 | 0.0252 | 0.0451 | 0.9727 | 0.6406 | 0.0022 | 0.0000 | 0.1034 | 0.0833 | 0.1250 | 0.0592 | 0.0220 | 0.0267 | 0.0 | 0.0000 | 0.0250 | 0.0435 | 0.9727 | 0.6310 | 0.0022 | 0.00 | 0.1034 | 0.0833 | 0.1250 | 0.0589 | 0.0205 | 0.0261 | 0.0000 | 0.0000 | reg oper account | block of flats | 0.0214 | Stone, brick | No | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | NaN | NaN | NaN | . 307508 456253 | 0 | Cash loans | F | N | Y | 0 | 153000.0 | 677664.0 | 29979.0 | 585000.0 | Unaccompanied | Working | Higher education | Separated | House / apartment | 0.005002 | -14966 | -7921 | -6737.0 | -5150 | NaN | 1 | 1 | 0 | 1 | 0 | 1 | Managers | 1.0 | 3 | 3 | THURSDAY | 9 | 0 | 0 | 0 | 0 | 1 | 1 | School | 0.744026 | 0.535722 | 0.218859 | 0.1031 | 0.0862 | 0.9816 | 0.7484 | 0.0123 | 0.00 | 0.2069 | 0.1667 | 0.2083 | NaN | 0.0841 | 0.9279 | 0.0000 | 0.0000 | 0.1050 | 0.0894 | 0.9816 | 0.7583 | 0.0124 | 0.0000 | 0.2069 | 0.1667 | 0.2083 | NaN | 0.0918 | 0.9667 | 0.0 | 0.0000 | 0.1041 | 0.0862 | 0.9816 | 0.7518 | 0.0124 | 0.00 | 0.2069 | 0.1667 | 0.2083 | NaN | 0.0855 | 0.9445 | 0.0000 | 0.0000 | reg oper account | block of flats | 0.7970 | Panel | No | 6.0 | 0.0 | 6.0 | 0.0 | -1909.0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | . 307509 456254 | 1 | Cash loans | F | N | Y | 0 | 171000.0 | 370107.0 | 20205.0 | 319500.0 | Unaccompanied | Commercial associate | Secondary / secondary special | Married | House / apartment | 0.005313 | -11961 | -4786 | -2562.0 | -931 | NaN | 1 | 1 | 0 | 1 | 0 | 0 | Laborers | 2.0 | 2 | 2 | WEDNESDAY | 9 | 0 | 0 | 0 | 1 | 1 | 0 | Business Entity Type 1 | NaN | 0.514163 | 0.661024 | 0.0124 | NaN | 0.9771 | NaN | NaN | NaN | 0.0690 | 0.0417 | NaN | NaN | NaN | 0.0061 | NaN | NaN | 0.0126 | NaN | 0.9772 | NaN | NaN | NaN | 0.0690 | 0.0417 | NaN | NaN | NaN | 0.0063 | NaN | NaN | 0.0125 | NaN | 0.9771 | NaN | NaN | NaN | 0.0690 | 0.0417 | NaN | NaN | NaN | 0.0062 | NaN | NaN | NaN | block of flats | 0.0086 | Stone, brick | No | 0.0 | 0.0 | 0.0 | 0.0 | -322.0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 307510 456255 | 0 | Cash loans | F | N | N | 0 | 157500.0 | 675000.0 | 49117.5 | 675000.0 | Unaccompanied | Commercial associate | Higher education | Married | House / apartment | 0.046220 | -16856 | -1262 | -5128.0 | -410 | NaN | 1 | 1 | 1 | 1 | 1 | 0 | Laborers | 2.0 | 1 | 1 | THURSDAY | 20 | 0 | 0 | 0 | 0 | 1 | 1 | Business Entity Type 3 | 0.734460 | 0.708569 | 0.113922 | 0.0742 | 0.0526 | 0.9881 | NaN | 0.0176 | 0.08 | 0.0690 | 0.3750 | NaN | NaN | NaN | 0.0791 | NaN | 0.0000 | 0.0756 | 0.0546 | 0.9881 | NaN | 0.0178 | 0.0806 | 0.0690 | 0.3750 | NaN | NaN | NaN | 0.0824 | NaN | 0.0000 | 0.0749 | 0.0526 | 0.9881 | NaN | 0.0177 | 0.08 | 0.0690 | 0.3750 | NaN | NaN | NaN | 0.0805 | NaN | 0.0000 | NaN | block of flats | 0.0718 | Panel | No | 0.0 | 0.0 | 0.0 | 0.0 | -787.0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 1.0 | . 307261 rows × 122 columns . app_train[app_train[&#39;AMT_INCOME_TOTAL&#39;] &lt; 1000000][&#39;AMT_INCOME_TOTAL&#39;].hist() . &lt;AxesSubplot:&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T10:57:23.563149 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ sns.displot(app_train[app_train[&#39;AMT_INCOME_TOTAL&#39;] &lt; 1000000][&#39;AMT_INCOME_TOTAL&#39;]) . &lt;seaborn.axisgrid.FacetGrid at 0x1bb2a4e6640&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T11:52:38.850247 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ sns.distplot(app_train[app_train[&#39;AMT_CREDIT&#39;] &lt; 1000000][&#39;AMT_CREDIT&#39;])) . C: Users channee anaconda3 lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;AxesSubplot:xlabel=&#39;AMT_CREDIT&#39;, ylabel=&#39;Density&#39;&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T10:57:56.702975 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ TARGET &#44050;&#50640; &#46384;&#47480; AMT_INCOME_TOTAL&#44050; &#48516;&#54252;&#46020; &#48708;&#44368; . distplot과 violinplot 시각화 | plt.subplots() 기반으로 seaborn의 distplot과 violinplot으로 분포도 비교 시각화 | . cond1 = (app_train[&#39;TARGET&#39;] == 1) cond0 = (app_train[&#39;TARGET&#39;] == 0) cond_amt = (app_train[&#39;AMT_INCOME_TOTAL&#39;] &lt; 500000) # 2개의 subplot을 생성하고 왼쪽에는 violinplot을 오른쪽에는 distplot을 표현 fig, axs = plt.subplots(figsize=(12, 4), nrows=1, ncols=2, squeeze=False) # violin plot을 왼쪽 subplot에 그림. sns.violinplot(x=&#39;TARGET&#39;, y=&#39;AMT_INCOME_TOTAL&#39;, data=app_train[cond_amt], ax=axs[0][0] ) # Histogram을 오른쪽 subplot에 그림. sns.distplot(app_train[cond0 &amp; cond_amt][&#39;AMT_INCOME_TOTAL&#39;], ax=axs[0][1], label=&#39;0&#39;, color=&#39;blue&#39;) sns.distplot(app_train[cond1 &amp; cond_amt][&#39;AMT_INCOME_TOTAL&#39;], ax=axs[0][1], label=&#39;1&#39;, color=&#39;red&#39;) . C: Users channee anaconda3 lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) C: Users channee anaconda3 lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;AxesSubplot:xlabel=&#39;AMT_INCOME_TOTAL&#39;, ylabel=&#39;Density&#39;&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T11:55:37.086908 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ AMT_INCOME_TOTAL이 100000~150000사이의 값에서 대부분 data가 모여있는것을 알 수 있다. | 시각화를 하는 코드를 함수로 구현해보았다 | . def show_column_hist_by_target(df, column, is_amt=False): cond1 = (df[&#39;TARGET&#39;] == 1) cond0 = (df[&#39;TARGET&#39;] == 0) fig, axs = plt.subplots(figsize=(12, 4), nrows=1, ncols=2, squeeze=False) # is_amt가 True이면 &lt; 500000 조건으로 filtering cond_amt = True if is_amt: cond_amt = df[column] &lt; 500000 sns.violinplot(x=&#39;TARGET&#39;, y=column, data=df[cond_amt], ax=axs[0][0] ) sns.distplot(df[cond0 &amp; cond_amt][column], ax=axs[0][1], label=&#39;0&#39;, color=&#39;blue&#39;) sns.distplot(df[cond1 &amp; cond_amt][column], ax=axs[0][1], label=&#39;1&#39;, color=&#39;red&#39;) show_column_hist_by_target(app_train, &#39;AMT_INCOME_TOTAL&#39;, is_amt=True) . C: Users channee anaconda3 lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) C: Users channee anaconda3 lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T12:12:34.150902 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ Preprocessing . app_train&#44284; app_test&#47484; &#54633;&#52432;&#49436; &#54620;&#48264;&#50640; &#45936;&#51060;&#53552; preprocessing &#49688;&#54665;. . app_train.shape, app_test.shape . ((307511, 122), (48744, 121)) . apps = pd.concat([app_train, app_test]) apps.shape . (356255, 122) . apps[&#39;TARGET&#39;].value_counts(dropna=False) . 0.0 282686 NaN 48744 1.0 24825 Name: TARGET, dtype: int64 . Object feature&#46308;&#51012; Label Encoding . pandas의 factorize()를 이용 | . apps.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 356255 entries, 0 to 48743 Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR dtypes: float64(66), int64(40), object(16) memory usage: 334.3+ MB . object_columns = apps.dtypes[apps.dtypes == &#39;object&#39;].index.tolist() . object_columns . [&#39;NAME_CONTRACT_TYPE&#39;, &#39;CODE_GENDER&#39;, &#39;FLAG_OWN_CAR&#39;, &#39;FLAG_OWN_REALTY&#39;, &#39;NAME_TYPE_SUITE&#39;, &#39;NAME_INCOME_TYPE&#39;, &#39;NAME_EDUCATION_TYPE&#39;, &#39;NAME_FAMILY_STATUS&#39;, &#39;NAME_HOUSING_TYPE&#39;, &#39;OCCUPATION_TYPE&#39;, &#39;WEEKDAY_APPR_PROCESS_START&#39;, &#39;ORGANIZATION_TYPE&#39;, &#39;FONDKAPREMONT_MODE&#39;, &#39;HOUSETYPE_MODE&#39;, &#39;WALLSMATERIAL_MODE&#39;, &#39;EMERGENCYSTATE_MODE&#39;] . apps[&#39;CODE_GENDER&#39;] . 0 M 1 F 2 M 3 F 4 M .. 48739 F 48740 F 48741 F 48742 M 48743 F Name: CODE_GENDER, Length: 356255, dtype: object . # pd.factorize(Category컬럼 Series)는 Label인코딩된 Series와 uniq한 Category값을 반환함. # [0]을 이용하여 Label인코딩 Series만 취함. apps[&#39;CODE_GENDER&#39;] = pd.factorize(apps[&#39;CODE_GENDER&#39;])[0] apps[&#39;CODE_GENDER&#39;] . 0 0 1 1 2 0 3 1 4 0 .. 48739 1 48740 1 48741 1 48742 0 48743 1 Name: CODE_GENDER, Length: 356255, dtype: int64 . # pd.factorize()는 한개의 컬럼만 Label 인코딩이 가능하므로 object형 컬럼들을 iteration하면서 변환 수행. for column in object_columns: apps[column] = pd.factorize(apps[column])[0] . apps.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 356255 entries, 0 to 48743 Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR dtypes: float64(66), int64(56) memory usage: 334.3 MB . &#44208;&#52769;&#52824; &#52376;&#47532; . apps.isnull().sum().head(100) . SK_ID_CURR 0 TARGET 48744 NAME_CONTRACT_TYPE 0 CODE_GENDER 0 FLAG_OWN_CAR 0 FLAG_OWN_REALTY 0 CNT_CHILDREN 0 AMT_INCOME_TOTAL 0 AMT_CREDIT 0 AMT_ANNUITY 36 AMT_GOODS_PRICE 278 NAME_TYPE_SUITE 0 NAME_INCOME_TYPE 0 NAME_EDUCATION_TYPE 0 NAME_FAMILY_STATUS 0 NAME_HOUSING_TYPE 0 REGION_POPULATION_RELATIVE 0 DAYS_BIRTH 0 DAYS_EMPLOYED 0 DAYS_REGISTRATION 0 DAYS_ID_PUBLISH 0 OWN_CAR_AGE 235241 FLAG_MOBIL 0 FLAG_EMP_PHONE 0 FLAG_WORK_PHONE 0 FLAG_CONT_MOBILE 0 FLAG_PHONE 0 FLAG_EMAIL 0 OCCUPATION_TYPE 0 CNT_FAM_MEMBERS 2 REGION_RATING_CLIENT 0 REGION_RATING_CLIENT_W_CITY 0 WEEKDAY_APPR_PROCESS_START 0 HOUR_APPR_PROCESS_START 0 REG_REGION_NOT_LIVE_REGION 0 REG_REGION_NOT_WORK_REGION 0 LIVE_REGION_NOT_WORK_REGION 0 REG_CITY_NOT_LIVE_CITY 0 REG_CITY_NOT_WORK_CITY 0 LIVE_CITY_NOT_WORK_CITY 0 ORGANIZATION_TYPE 0 EXT_SOURCE_1 193910 EXT_SOURCE_2 668 EXT_SOURCE_3 69633 APARTMENTS_AVG 179948 BASEMENTAREA_AVG 207584 YEARS_BEGINEXPLUATATION_AVG 172863 YEARS_BUILD_AVG 236306 COMMONAREA_AVG 248360 ELEVATORS_AVG 189080 ENTRANCES_AVG 178407 FLOORSMAX_AVG 176341 FLOORSMIN_AVG 241108 LANDAREA_AVG 210844 LIVINGAPARTMENTS_AVG 242979 LIVINGAREA_AVG 177902 NONLIVINGAPARTMENTS_AVG 246861 NONLIVINGAREA_AVG 195766 APARTMENTS_MODE 179948 BASEMENTAREA_MODE 207584 YEARS_BEGINEXPLUATATION_MODE 172863 YEARS_BUILD_MODE 236306 COMMONAREA_MODE 248360 ELEVATORS_MODE 189080 ENTRANCES_MODE 178407 FLOORSMAX_MODE 176341 FLOORSMIN_MODE 241108 LANDAREA_MODE 210844 LIVINGAPARTMENTS_MODE 242979 LIVINGAREA_MODE 177902 NONLIVINGAPARTMENTS_MODE 246861 NONLIVINGAREA_MODE 195766 APARTMENTS_MEDI 179948 BASEMENTAREA_MEDI 207584 YEARS_BEGINEXPLUATATION_MEDI 172863 YEARS_BUILD_MEDI 236306 COMMONAREA_MEDI 248360 ELEVATORS_MEDI 189080 ENTRANCES_MEDI 178407 FLOORSMAX_MEDI 176341 FLOORSMIN_MEDI 241108 LANDAREA_MEDI 210844 LIVINGAPARTMENTS_MEDI 242979 LIVINGAREA_MEDI 177902 NONLIVINGAPARTMENTS_MEDI 246861 NONLIVINGAREA_MEDI 195766 FONDKAPREMONT_MODE 0 HOUSETYPE_MODE 0 TOTALAREA_MODE 171055 WALLSMATERIAL_MODE 0 EMERGENCYSTATE_MODE 0 OBS_30_CNT_SOCIAL_CIRCLE 1050 DEF_30_CNT_SOCIAL_CIRCLE 1050 OBS_60_CNT_SOCIAL_CIRCLE 1050 DEF_60_CNT_SOCIAL_CIRCLE 1050 DAYS_LAST_PHONE_CHANGE 1 FLAG_DOCUMENT_2 0 FLAG_DOCUMENT_3 0 FLAG_DOCUMENT_4 0 FLAG_DOCUMENT_5 0 dtype: int64 . # apps = apps.fillna(-999) . &#54617;&#49845; &#45936;&#51060;&#53552;&#50752; &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#45796;&#49884; &#48516;&#47532; . app_train = apps[apps[&#39;TARGET&#39;] != -999] app_test = apps[apps[&#39;TARGET&#39;]== -999] app_train.shape, app_test.shape . ((307511, 122), (48744, 122)) . app_test = app_test.drop(&#39;TARGET&#39;, axis=1) . app_test.shape . (48744, 121) . app_test.dtypes . SK_ID_CURR int64 NAME_CONTRACT_TYPE int64 CODE_GENDER int64 FLAG_OWN_CAR int64 FLAG_OWN_REALTY int64 ... AMT_REQ_CREDIT_BUREAU_DAY float64 AMT_REQ_CREDIT_BUREAU_WEEK float64 AMT_REQ_CREDIT_BUREAU_MON float64 AMT_REQ_CREDIT_BUREAU_QRT float64 AMT_REQ_CREDIT_BUREAU_YEAR float64 Length: 121, dtype: object . &#54617;&#49845; &#45936;&#51060;&#53552;&#47484; &#44160;&#51613; &#45936;&#51060;&#53552;&#47196; &#48516;&#47532;&#54616;&#44256; LGBM Classifier&#47196; &#54617;&#49845; &#49688;&#54665;. . 피처용 데이터와 타겟 데이터 분리 | 학습용/검증용 데이터 세트 분리 | . ftr_app = app_train.drop([&#39;SK_ID_CURR&#39;, &#39;TARGET&#39;], axis=1) target_app = app_train[&#39;TARGET&#39;] . from sklearn.model_selection import train_test_split train_x, valid_x, train_y, valid_y = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2021) train_x.shape, valid_x.shape . ((215257, 120), (92254, 120)) . from lightgbm import LGBMClassifier clf = LGBMClassifier( n_jobs=-1, n_estimators=1000, learning_rate=0.02, num_leaves=32, subsample=0.8, max_depth=12, silent=-1, verbose=-1 ) clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= &#39;auc&#39;, verbose= 100, early_stopping_rounds= 50) . Training until validation scores don&#39;t improve for 50 rounds [100] training&#39;s auc: 0.753149 training&#39;s binary_logloss: 0.249708 valid_1&#39;s auc: 0.740531 valid_1&#39;s binary_logloss: 0.253105 [200] training&#39;s auc: 0.7723 training&#39;s binary_logloss: 0.242948 valid_1&#39;s auc: 0.751634 valid_1&#39;s binary_logloss: 0.24893 [300] training&#39;s auc: 0.784721 training&#39;s binary_logloss: 0.238878 valid_1&#39;s auc: 0.755786 valid_1&#39;s binary_logloss: 0.247481 [400] training&#39;s auc: 0.796331 training&#39;s binary_logloss: 0.235508 valid_1&#39;s auc: 0.756972 valid_1&#39;s binary_logloss: 0.247029 [500] training&#39;s auc: 0.806504 training&#39;s binary_logloss: 0.232526 valid_1&#39;s auc: 0.757287 valid_1&#39;s binary_logloss: 0.246869 [600] training&#39;s auc: 0.815748 training&#39;s binary_logloss: 0.229736 valid_1&#39;s auc: 0.757548 valid_1&#39;s binary_logloss: 0.246756 Early stopping, best iteration is: [612] training&#39;s auc: 0.816579 training&#39;s binary_logloss: 0.229435 valid_1&#39;s auc: 0.757631 valid_1&#39;s binary_logloss: 0.246736 . LGBMClassifier(learning_rate=0.02, max_depth=12, n_estimators=1000, num_leaves=32, silent=-1, subsample=0.8, verbose=-1) . from lightgbm import plot_importance plot_importance(clf, figsize=(16, 32)) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;Feature importance&#39;, ylabel=&#39;Features&#39;&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-08-31T12:42:59.859410 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ &#51076;&#49884;&#44208;&#47200; . 데이터 Preprocessing 없이 가볍게 Lightgbm을 수행시키고 주요 Feature들을 확인하였습니다. | 이를 baseline으로 해서 EDA부터 다시 시행해보고자 한다. | .",
            "url": "https://gwonchankim.github.io/channee/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_1.html",
            "relUrl": "/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_1.html",
            "date": " • Aug 31, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "LightGBM!!",
            "content": "import lightgbm print(lightgbm.__version__) . 2.1.0 . LightGBM &#51201;&#50857; &#8211; &#50948;&#49828;&#53080;&#49888; Breast Cancer Prediction . from lightgbm import LGBMClassifier import pandas as pd import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split dataset = load_breast_cancer() ftr = dataset.data target = dataset.target # 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출 X_train, X_test, y_train, y_test=train_test_split(ftr, target, test_size=0.2, random_state=156 ) # 앞서 XGBoost와 동일하게 n_estimators는 400 설정. lgbm_wrapper = LGBMClassifier(n_estimators=400) # LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능. evals = [(X_test, y_test)] lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;logloss&quot;, eval_set=evals, verbose=True) preds = lgbm_wrapper.predict(X_test) pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1] . [1] valid_0&#39;s binary_logloss: 0.614872 Training until validation scores don&#39;t improve for 100 rounds. [2] valid_0&#39;s binary_logloss: 0.550424 [3] valid_0&#39;s binary_logloss: 0.497095 [4] valid_0&#39;s binary_logloss: 0.449481 [5] valid_0&#39;s binary_logloss: 0.414368 [6] valid_0&#39;s binary_logloss: 0.381005 [7] valid_0&#39;s binary_logloss: 0.349425 [8] valid_0&#39;s binary_logloss: 0.325225 [9] valid_0&#39;s binary_logloss: 0.300915 [10] valid_0&#39;s binary_logloss: 0.283436 [11] valid_0&#39;s binary_logloss: 0.265498 [12] valid_0&#39;s binary_logloss: 0.250054 [13] valid_0&#39;s binary_logloss: 0.236562 [14] valid_0&#39;s binary_logloss: 0.22509 [15] valid_0&#39;s binary_logloss: 0.214741 [16] valid_0&#39;s binary_logloss: 0.206876 [17] valid_0&#39;s binary_logloss: 0.198085 [18] valid_0&#39;s binary_logloss: 0.190907 [19] valid_0&#39;s binary_logloss: 0.181494 [20] valid_0&#39;s binary_logloss: 0.173462 [21] valid_0&#39;s binary_logloss: 0.1672 [22] valid_0&#39;s binary_logloss: 0.160914 [23] valid_0&#39;s binary_logloss: 0.156162 [24] valid_0&#39;s binary_logloss: 0.152263 [25] valid_0&#39;s binary_logloss: 0.144638 [26] valid_0&#39;s binary_logloss: 0.140135 [27] valid_0&#39;s binary_logloss: 0.135741 [28] valid_0&#39;s binary_logloss: 0.132252 [29] valid_0&#39;s binary_logloss: 0.130786 [30] valid_0&#39;s binary_logloss: 0.13122 [31] valid_0&#39;s binary_logloss: 0.127583 [32] valid_0&#39;s binary_logloss: 0.125351 [33] valid_0&#39;s binary_logloss: 0.120765 [34] valid_0&#39;s binary_logloss: 0.116044 [35] valid_0&#39;s binary_logloss: 0.114636 [36] valid_0&#39;s binary_logloss: 0.115021 [37] valid_0&#39;s binary_logloss: 0.11225 [38] valid_0&#39;s binary_logloss: 0.112672 [39] valid_0&#39;s binary_logloss: 0.112078 [40] valid_0&#39;s binary_logloss: 0.111472 [41] valid_0&#39;s binary_logloss: 0.112868 [42] valid_0&#39;s binary_logloss: 0.113805 [43] valid_0&#39;s binary_logloss: 0.111621 [44] valid_0&#39;s binary_logloss: 0.113316 [45] valid_0&#39;s binary_logloss: 0.112703 [46] valid_0&#39;s binary_logloss: 0.112621 [47] valid_0&#39;s binary_logloss: 0.114047 [48] valid_0&#39;s binary_logloss: 0.11646 [49] valid_0&#39;s binary_logloss: 0.11804 [50] valid_0&#39;s binary_logloss: 0.117546 [51] valid_0&#39;s binary_logloss: 0.116963 [52] valid_0&#39;s binary_logloss: 0.117518 [53] valid_0&#39;s binary_logloss: 0.117933 [54] valid_0&#39;s binary_logloss: 0.118838 [55] valid_0&#39;s binary_logloss: 0.119101 [56] valid_0&#39;s binary_logloss: 0.120001 [57] valid_0&#39;s binary_logloss: 0.118411 [58] valid_0&#39;s binary_logloss: 0.118695 [59] valid_0&#39;s binary_logloss: 0.117512 [60] valid_0&#39;s binary_logloss: 0.117793 [61] valid_0&#39;s binary_logloss: 0.119033 [62] valid_0&#39;s binary_logloss: 0.118869 [63] valid_0&#39;s binary_logloss: 0.12047 [64] valid_0&#39;s binary_logloss: 0.11909 [65] valid_0&#39;s binary_logloss: 0.118759 [66] valid_0&#39;s binary_logloss: 0.121258 [67] valid_0&#39;s binary_logloss: 0.119557 [68] valid_0&#39;s binary_logloss: 0.119182 [69] valid_0&#39;s binary_logloss: 0.118941 [70] valid_0&#39;s binary_logloss: 0.118697 [71] valid_0&#39;s binary_logloss: 0.119549 [72] valid_0&#39;s binary_logloss: 0.120924 [73] valid_0&#39;s binary_logloss: 0.120274 [74] valid_0&#39;s binary_logloss: 0.122415 [75] valid_0&#39;s binary_logloss: 0.122802 [76] valid_0&#39;s binary_logloss: 0.121592 [77] valid_0&#39;s binary_logloss: 0.123246 [78] valid_0&#39;s binary_logloss: 0.125394 [79] valid_0&#39;s binary_logloss: 0.126496 [80] valid_0&#39;s binary_logloss: 0.126031 [81] valid_0&#39;s binary_logloss: 0.12885 [82] valid_0&#39;s binary_logloss: 0.129289 [83] valid_0&#39;s binary_logloss: 0.130373 [84] valid_0&#39;s binary_logloss: 0.131823 [85] valid_0&#39;s binary_logloss: 0.133759 [86] valid_0&#39;s binary_logloss: 0.133971 [87] valid_0&#39;s binary_logloss: 0.135448 [88] valid_0&#39;s binary_logloss: 0.137522 [89] valid_0&#39;s binary_logloss: 0.139486 [90] valid_0&#39;s binary_logloss: 0.141503 [91] valid_0&#39;s binary_logloss: 0.143603 [92] valid_0&#39;s binary_logloss: 0.145284 [93] valid_0&#39;s binary_logloss: 0.147958 [94] valid_0&#39;s binary_logloss: 0.145053 [95] valid_0&#39;s binary_logloss: 0.146998 [96] valid_0&#39;s binary_logloss: 0.145528 [97] valid_0&#39;s binary_logloss: 0.14453 [98] valid_0&#39;s binary_logloss: 0.146676 [99] valid_0&#39;s binary_logloss: 0.148082 [100] valid_0&#39;s binary_logloss: 0.148896 [101] valid_0&#39;s binary_logloss: 0.148408 [102] valid_0&#39;s binary_logloss: 0.15026 [103] valid_0&#39;s binary_logloss: 0.151574 [104] valid_0&#39;s binary_logloss: 0.154725 [105] valid_0&#39;s binary_logloss: 0.15821 [106] valid_0&#39;s binary_logloss: 0.158945 [107] valid_0&#39;s binary_logloss: 0.160266 [108] valid_0&#39;s binary_logloss: 0.158948 [109] valid_0&#39;s binary_logloss: 0.161068 [110] valid_0&#39;s binary_logloss: 0.164555 [111] valid_0&#39;s binary_logloss: 0.164262 [112] valid_0&#39;s binary_logloss: 0.163297 [113] valid_0&#39;s binary_logloss: 0.164523 [114] valid_0&#39;s binary_logloss: 0.163787 [115] valid_0&#39;s binary_logloss: 0.165961 [116] valid_0&#39;s binary_logloss: 0.163576 [117] valid_0&#39;s binary_logloss: 0.166783 [118] valid_0&#39;s binary_logloss: 0.168178 [119] valid_0&#39;s binary_logloss: 0.166892 [120] valid_0&#39;s binary_logloss: 0.170601 [121] valid_0&#39;s binary_logloss: 0.174079 [122] valid_0&#39;s binary_logloss: 0.175411 [123] valid_0&#39;s binary_logloss: 0.178159 [124] valid_0&#39;s binary_logloss: 0.17829 [125] valid_0&#39;s binary_logloss: 0.179993 [126] valid_0&#39;s binary_logloss: 0.1786 [127] valid_0&#39;s binary_logloss: 0.180176 [128] valid_0&#39;s binary_logloss: 0.182569 [129] valid_0&#39;s binary_logloss: 0.180566 [130] valid_0&#39;s binary_logloss: 0.182573 [131] valid_0&#39;s binary_logloss: 0.179911 [132] valid_0&#39;s binary_logloss: 0.181185 [133] valid_0&#39;s binary_logloss: 0.181359 [134] valid_0&#39;s binary_logloss: 0.184895 [135] valid_0&#39;s binary_logloss: 0.183728 [136] valid_0&#39;s binary_logloss: 0.187889 [137] valid_0&#39;s binary_logloss: 0.190915 [138] valid_0&#39;s binary_logloss: 0.194048 [139] valid_0&#39;s binary_logloss: 0.197882 [140] valid_0&#39;s binary_logloss: 0.198944 Early stopping, best iteration is: [40] valid_0&#39;s binary_logloss: 0.111472 . C: ProgramData Anaconda3 lib site-packages sklearn preprocessing label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty. if diff: . from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import precision_score, recall_score from sklearn.metrics import f1_score, roc_auc_score # 수정된 get_clf_eval() 함수 def get_clf_eval(y_test, pred=None, pred_proba=None): confusion = confusion_matrix( y_test, pred) accuracy = accuracy_score(y_test , pred) precision = precision_score(y_test , pred) recall = recall_score(y_test , pred) f1 = f1_score(y_test,pred) # ROC-AUC 추가 roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) # ROC-AUC print 추가 print(&#39;정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}&#39;.format(accuracy, precision, recall, f1, roc_auc)) . get_clf_eval(y_test, preds, pred_proba) . 오차 행렬 [[33 4] [ 2 75]] 정확도: 0.9474, 정밀도: 0.9494, 재현율: 0.9740, F1: 0.9615, AUC:0.9933 . from lightgbm import plot_importance import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(10, 12)) # 사이킷런 래퍼 클래스를 입력해도 무방. plot_importance(lgbm_wrapper, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2497e27e5f8&gt; . print(dataset.feature_names) .",
            "url": "https://gwonchankim.github.io/channee/jupyter/ml/machinelearning/2021/08/31/_08_31_LightGBM.html",
            "relUrl": "/jupyter/ml/machinelearning/2021/08/31/_08_31_LightGBM.html",
            "date": " • Aug 31, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://gwonchankim.github.io/channee/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gwonchankim.github.io/channee/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "머신러닝, 딥러닝, AI를 공부하는 사람입니다. 최대한 다양한 내용을 다루어 보고자 합니다. 여러 코멘트, 조언은 언제나 환영합니다! . About me . 코드스테이트 AIbootcamp 수료 중 . Pandas . Machine Learning . Deep Learning .",
          "url": "https://gwonchankim.github.io/channee/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Sample",
          "content": "안녕 세상!! . 안녕! | 안녕! | 안녕! | .",
          "url": "https://gwonchankim.github.io/channee/sample/",
          "relUrl": "/sample/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gwonchankim.github.io/channee/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}