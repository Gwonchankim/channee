<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Project2-4 | 안녕 세상!!</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Project2-4" />
<meta name="author" content="channee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="ML을 이용한 대출 채무이행 판별" />
<meta property="og:description" content="ML을 이용한 대출 채무이행 판별" />
<link rel="canonical" href="https://gwonchankim.github.io/channee/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_4.html" />
<meta property="og:url" content="https://gwonchankim.github.io/channee/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_4.html" />
<meta property="og:site_name" content="안녕 세상!!" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-08-31T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2021-08-31T00:00:00-05:00","url":"https://gwonchankim.github.io/channee/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_4.html","@type":"BlogPosting","headline":"Project2-4","dateModified":"2021-08-31T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gwonchankim.github.io/channee/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_4.html"},"author":{"@type":"Person","name":"channee"},"description":"ML을 이용한 대출 채무이행 판별","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/channee/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://gwonchankim.github.io/channee/feed.xml" title="안녕 세상!!" /><link rel="shortcut icon" type="image/x-icon" href="/channee/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/channee/">안녕 세상!!</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/channee/about/">About Me</a><a class="page-link" href="/channee/sample/">Sample</a><a class="page-link" href="/channee/search/">Search</a><a class="page-link" href="/channee/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Project2-4</h1><p class="page-description">ML을 이용한 대출 채무이행 판별</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-08-31T00:00:00-05:00" itemprop="datePublished">
        Aug 31, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">channee</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      129 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/channee/categories/#jupyter">jupyter</a>
        &nbsp;
      
        <a class="category-tags-link" href="/channee/categories/#ML">ML</a>
        &nbsp;
      
        <a class="category-tags-link" href="/channee/categories/#MachineLearning">MachineLearning</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/Gwonchankim/channee/tree/master/_notebooks/2021_09_01_Project2_4.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/channee/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/Gwonchankim/channee/master?filepath=_notebooks%2F2021_09_01_Project2_4.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/channee/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/Gwonchankim/channee/blob/master/_notebooks/2021_09_01_Project2_4.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/channee/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h4"><a href="#가공된-최종-데이터-세트-생성">가공된 최종 데이터 세트 생성 </a>
<ul>
<li class="toc-entry toc-h5"><a href="#지금까지-과정을-함수화">지금까지 과정을 함수화 </a></li>
<li class="toc-entry toc-h5"><a href="#이전에-application-데이터-세트의-feature-engineering-수행-후-새롭게-previous-데이터-세트로-가공된-데이터를-조인.">이전에 application 데이터 세트의 feature engineering 수행 후 새롭게 previous 데이터 세트로 가공된 데이터를 조인. </a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#이전-application-데이터의-feature-engineering-함수-복사">이전 application 데이터의 feature engineering 함수 복사 </a></li>
<li class="toc-entry toc-h4"><a href="#previous-데이터-가공후-인코딩-및-최종-데이터-집합-생성하는-함수-선언">previous 데이터 가공후 인코딩 및 최종 데이터 집합 생성하는 함수 선언 </a>
<ul>
<li class="toc-entry toc-h5"><a href="#최종-집합-생성-및-인코딩,-학습/테스트-데이터-분리,-학습/검증-피처와-타겟-데이터-분리">최종 집합 생성 및 인코딩, 학습/테스트 데이터 분리, 학습/검증 피처와 타겟 데이터 분리 </a></li>
<li class="toc-entry toc-h5"><a href="#Iteration-결과-Dictionary에서-최대-target값을-가지는-index-추출하고-그때의-parameter-값을-추출.">Iteration 결과 Dictionary에서 최대 target값을 가지는 index 추출하고 그때의 parameter 값을 추출. </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#최적화된-하이퍼-파라미터를-기반으로-재-테스트">최적화된 하이퍼 파라미터를 기반으로 재 테스트 </a>
<ul>
<li class="toc-entry toc-h5"><a href="#cross-validation-으로-hyper-parameter-재-tuning">cross validation 으로 hyper parameter 재 tuning </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021_09_01_Project2_4.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="가공된-최종-데이터-세트-생성">
<a class="anchor" href="#%EA%B0%80%EA%B3%B5%EB%90%9C-%EC%B5%9C%EC%A2%85-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%84%B8%ED%8A%B8-%EC%83%9D%EC%84%B1" aria-hidden="true"><span class="octicon octicon-link"></span></a>가공된 최종 데이터 세트 생성<a class="anchor-link" href="#%EA%B0%80%EA%B3%B5%EB%90%9C-%EC%B5%9C%EC%A2%85-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%84%B8%ED%8A%B8-%EC%83%9D%EC%84%B1"> </a>
</h4>
<h5 id="지금까지-과정을-함수화">
<a class="anchor" href="#%EC%A7%80%EA%B8%88%EA%B9%8C%EC%A7%80-%EA%B3%BC%EC%A0%95%EC%9D%84-%ED%95%A8%EC%88%98%ED%99%94" aria-hidden="true"><span class="octicon octicon-link"></span></a>지금까지 과정을 함수화<a class="anchor-link" href="#%EC%A7%80%EA%B8%88%EA%B9%8C%EC%A7%80-%EA%B3%BC%EC%A0%95%EC%9D%84-%ED%95%A8%EC%88%98%ED%99%94"> </a>
</h5>
<h5 id="이전에-application-데이터-세트의-feature-engineering-수행-후-새롭게-previous-데이터-세트로-가공된-데이터를-조인.">
<a class="anchor" href="#%EC%9D%B4%EC%A0%84%EC%97%90-application-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%84%B8%ED%8A%B8%EC%9D%98-feature-engineering-%EC%88%98%ED%96%89-%ED%9B%84-%EC%83%88%EB%A1%AD%EA%B2%8C-previous-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%84%B8%ED%8A%B8%EB%A1%9C-%EA%B0%80%EA%B3%B5%EB%90%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EC%A1%B0%EC%9D%B8." aria-hidden="true"><span class="octicon octicon-link"></span></a>이전에 application 데이터 세트의 feature engineering 수행 후 새롭게 previous 데이터 세트로 가공된 데이터를 조인.<a class="anchor-link" href="#%EC%9D%B4%EC%A0%84%EC%97%90-application-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%84%B8%ED%8A%B8%EC%9D%98-feature-engineering-%EC%88%98%ED%96%89-%ED%9B%84-%EC%83%88%EB%A1%AD%EA%B2%8C-previous-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%84%B8%ED%8A%B8%EB%A1%9C-%EA%B0%80%EA%B3%B5%EB%90%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EC%A1%B0%EC%9D%B8."> </a>
</h5>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">gc</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">'display.max_rows'</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">'display.max_columns'</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_dataset</span><span class="p">():</span>
    <span class="n">app_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'application_train.csv'</span><span class="p">)</span>
    <span class="n">app_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'application_test.csv'</span><span class="p">)</span>
    <span class="n">apps</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">app_train</span><span class="p">,</span> <span class="n">app_test</span><span class="p">])</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'previous_application.csv'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">apps</span><span class="p">,</span> <span class="n">prev</span>

<span class="n">apps</span><span class="p">,</span> <span class="n">prev</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">()</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="이전-application-데이터의-feature-engineering-함수-복사">
<a class="anchor" href="#%EC%9D%B4%EC%A0%84-application-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98-feature-engineering-%ED%95%A8%EC%88%98-%EB%B3%B5%EC%82%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>이전 application 데이터의 feature engineering 함수 복사<a class="anchor-link" href="#%EC%9D%B4%EC%A0%84-application-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98-feature-engineering-%ED%95%A8%EC%88%98-%EB%B3%B5%EC%82%AC"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_apps_processed</span><span class="p">(</span><span class="n">apps</span><span class="p">):</span>
    
    <span class="c1"># EXT_SOURCE_X FEATURE 가공</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_EXT_SOURCE_MEAN'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[[</span><span class="s1">'EXT_SOURCE_1'</span><span class="p">,</span> <span class="s1">'EXT_SOURCE_2'</span><span class="p">,</span> <span class="s1">'EXT_SOURCE_3'</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_EXT_SOURCE_STD'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[[</span><span class="s1">'EXT_SOURCE_1'</span><span class="p">,</span> <span class="s1">'EXT_SOURCE_2'</span><span class="p">,</span> <span class="s1">'EXT_SOURCE_3'</span><span class="p">]]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_EXT_SOURCE_STD'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_EXT_SOURCE_STD'</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_EXT_SOURCE_STD'</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    
    <span class="c1"># AMT_CREDIT 비율로 Feature 가공</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_ANNUITY_CREDIT_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_ANNUITY'</span><span class="p">]</span><span class="o">/</span><span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_CREDIT'</span><span class="p">]</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_GOODS_CREDIT_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_GOODS_PRICE'</span><span class="p">]</span><span class="o">/</span><span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_CREDIT'</span><span class="p">]</span>
    
    <span class="c1"># AMT_INCOME_TOTAL 비율로 Feature 가공</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_ANNUITY_INCOME_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_ANNUITY'</span><span class="p">]</span><span class="o">/</span><span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_INCOME_TOTAL'</span><span class="p">]</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_CREDIT_INCOME_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_CREDIT'</span><span class="p">]</span><span class="o">/</span><span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_INCOME_TOTAL'</span><span class="p">]</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_GOODS_INCOME_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_GOODS_PRICE'</span><span class="p">]</span><span class="o">/</span><span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_INCOME_TOTAL'</span><span class="p">]</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_CNT_FAM_INCOME_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_INCOME_TOTAL'</span><span class="p">]</span><span class="o">/</span><span class="n">apps</span><span class="p">[</span><span class="s1">'CNT_FAM_MEMBERS'</span><span class="p">]</span>
    
    <span class="c1"># DAYS_BIRTH, DAYS_EMPLOYED 비율로 Feature 가공</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_EMPLOYED_BIRTH_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'DAYS_EMPLOYED'</span><span class="p">]</span><span class="o">/</span><span class="n">apps</span><span class="p">[</span><span class="s1">'DAYS_BIRTH'</span><span class="p">]</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_INCOME_EMPLOYED_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_INCOME_TOTAL'</span><span class="p">]</span><span class="o">/</span><span class="n">apps</span><span class="p">[</span><span class="s1">'DAYS_EMPLOYED'</span><span class="p">]</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_INCOME_BIRTH_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'AMT_INCOME_TOTAL'</span><span class="p">]</span><span class="o">/</span><span class="n">apps</span><span class="p">[</span><span class="s1">'DAYS_BIRTH'</span><span class="p">]</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_CAR_BIRTH_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'OWN_CAR_AGE'</span><span class="p">]</span> <span class="o">/</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'DAYS_BIRTH'</span><span class="p">]</span>
    <span class="n">apps</span><span class="p">[</span><span class="s1">'APPS_CAR_EMPLOYED_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'OWN_CAR_AGE'</span><span class="p">]</span> <span class="o">/</span> <span class="n">apps</span><span class="p">[</span><span class="s1">'DAYS_EMPLOYED'</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">apps</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="previous-데이터-가공후-인코딩-및-최종-데이터-집합-생성하는-함수-선언">
<a class="anchor" href="#previous-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%80%EA%B3%B5%ED%9B%84-%EC%9D%B8%EC%BD%94%EB%94%A9-%EB%B0%8F-%EC%B5%9C%EC%A2%85-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A7%91%ED%95%A9-%EC%83%9D%EC%84%B1%ED%95%98%EB%8A%94-%ED%95%A8%EC%88%98-%EC%84%A0%EC%96%B8" aria-hidden="true"><span class="octicon octicon-link"></span></a>previous 데이터 가공후 인코딩 및 최종 데이터 집합 생성하는 함수 선언<a class="anchor-link" href="#previous-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%80%EA%B3%B5%ED%9B%84-%EC%9D%B8%EC%BD%94%EB%94%A9-%EB%B0%8F-%EC%B5%9C%EC%A2%85-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A7%91%ED%95%A9-%EC%83%9D%EC%84%B1%ED%95%98%EB%8A%94-%ED%95%A8%EC%88%98-%EC%84%A0%EC%96%B8"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span>

<span class="k">def</span> <span class="nf">get_prev_processed</span><span class="p">(</span><span class="n">prev</span><span class="p">):</span>
    <span class="c1"># 대출 신청 금액과 실제 대출액/대출 상품금액 차이 및 비율</span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'PREV_CREDIT_DIFF'</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_APPLICATION'</span><span class="p">]</span> <span class="o">-</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_CREDIT'</span><span class="p">]</span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'PREV_GOODS_DIFF'</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_APPLICATION'</span><span class="p">]</span> <span class="o">-</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_GOODS_PRICE'</span><span class="p">]</span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'PREV_CREDIT_APPL_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_CREDIT'</span><span class="p">]</span><span class="o">/</span><span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_APPLICATION'</span><span class="p">]</span>
    <span class="c1"># prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']</span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'PREV_GOODS_APPL_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_GOODS_PRICE'</span><span class="p">]</span><span class="o">/</span><span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_APPLICATION'</span><span class="p">]</span>
    
    <span class="n">prev</span><span class="p">[</span><span class="s1">'DAYS_FIRST_DRAWING'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">365243</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'DAYS_FIRST_DUE'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">365243</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'DAYS_LAST_DUE_1ST_VERSION'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">365243</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'DAYS_LAST_DUE'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">365243</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'DAYS_TERMINATION'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">365243</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="c1"># 첫번째 만기일과 마지막 만기일까지의 기간</span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'PREV_DAYS_LAST_DUE_DIFF'</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'DAYS_LAST_DUE_1ST_VERSION'</span><span class="p">]</span> <span class="o">-</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'DAYS_LAST_DUE'</span><span class="p">]</span>
    <span class="c1"># 매월 납부 금액과 납부 횟수 곱해서 전체 납부 금액 구함. </span>
    <span class="n">all_pay</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_ANNUITY'</span><span class="p">]</span> <span class="o">*</span> <span class="n">prev</span><span class="p">[</span><span class="s1">'CNT_PAYMENT'</span><span class="p">]</span>
    <span class="c1"># 전체 납부 금액 대비 AMT_CREDIT 비율을 구하고 여기에 다시 납부횟수로 나누어서 이자율 계산. </span>
    <span class="n">prev</span><span class="p">[</span><span class="s1">'PREV_INTERESTS_RATE'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">all_pay</span><span class="o">/</span><span class="n">prev</span><span class="p">[</span><span class="s1">'AMT_CREDIT'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">prev</span><span class="p">[</span><span class="s1">'CNT_PAYMENT'</span><span class="p">]</span>
        
    <span class="k">return</span> <span class="n">prev</span>
    
    
<span class="k">def</span> <span class="nf">get_prev_amt_agg</span><span class="p">(</span><span class="n">prev</span><span class="p">):</span>
    <span class="c1"># 새롭게 생성된 대출 신청액 대비 다른 금액 차이 및 비율로 aggregation 수행. </span>
    <span class="n">agg_dict</span> <span class="o">=</span> <span class="p">{</span>
         <span class="c1"># 기존 컬럼. </span>
        <span class="s1">'SK_ID_CURR'</span><span class="p">:[</span><span class="s1">'count'</span><span class="p">],</span>
        <span class="s1">'AMT_CREDIT'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">],</span>
        <span class="s1">'AMT_ANNUITY'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">],</span> 
        <span class="s1">'AMT_APPLICATION'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">],</span>
        <span class="s1">'AMT_DOWN_PAYMENT'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">],</span>
        <span class="s1">'AMT_GOODS_PRICE'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">],</span>
        <span class="s1">'RATE_DOWN_PAYMENT'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">],</span>
        <span class="s1">'DAYS_DECISION'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">],</span>
        <span class="s1">'CNT_PAYMENT'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">],</span>
        <span class="c1"># 가공 컬럼</span>
        <span class="s1">'PREV_CREDIT_DIFF'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">],</span> 
        <span class="s1">'PREV_CREDIT_APPL_RATIO'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">],</span>
        <span class="s1">'PREV_GOODS_DIFF'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">],</span>
        <span class="s1">'PREV_GOODS_APPL_RATIO'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">],</span>
        <span class="s1">'PREV_DAYS_LAST_DUE_DIFF'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">],</span>
        <span class="s1">'PREV_INTERESTS_RATE'</span><span class="p">:[</span><span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">]</span>
    <span class="p">}</span>

    <span class="n">prev_group</span> <span class="o">=</span> <span class="n">prev</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">'SK_ID_CURR'</span><span class="p">)</span>
    <span class="n">prev_amt_agg</span> <span class="o">=</span> <span class="n">prev_group</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">agg_dict</span><span class="p">)</span>

    <span class="c1"># multi index 컬럼을 '_'로 연결하여 컬럼명 변경</span>
    <span class="n">prev_amt_agg</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"PREV_"</span><span class="o">+</span> <span class="s2">"_"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">prev_amt_agg</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>
    
    <span class="k">return</span> <span class="n">prev_amt_agg</span>

<span class="k">def</span> <span class="nf">get_prev_refused_appr_agg</span><span class="p">(</span><span class="n">prev</span><span class="p">):</span>
    <span class="c1"># 원래 groupby 컬럼 + 세부 기준 컬럼으로 groupby 수행. 세분화된 레벨로 aggregation 수행 한 뒤에 unstack()으로 컬럼레벨로 변형. </span>
    <span class="n">prev_refused_appr_group</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="n">prev</span><span class="p">[</span><span class="s1">'NAME_CONTRACT_STATUS'</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="s1">'Approved'</span><span class="p">,</span> <span class="s1">'Refused'</span><span class="p">])]</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span> <span class="s1">'SK_ID_CURR'</span><span class="p">,</span> <span class="s1">'NAME_CONTRACT_STATUS'</span><span class="p">])</span>
    <span class="n">prev_refused_appr_agg</span> <span class="o">=</span> <span class="n">prev_refused_appr_group</span><span class="p">[</span><span class="s1">'SK_ID_CURR'</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">()</span>
    <span class="c1"># 컬럼명 변경. </span>
    <span class="n">prev_refused_appr_agg</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'PREV_APPROVED_COUNT'</span><span class="p">,</span> <span class="s1">'PREV_REFUSED_COUNT'</span> <span class="p">]</span>
    <span class="c1"># NaN값은 모두 0으로 변경. </span>
    <span class="n">prev_refused_appr_agg</span> <span class="o">=</span> <span class="n">prev_refused_appr_agg</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">prev_refused_appr_agg</span>

    

<span class="k">def</span> <span class="nf">get_prev_agg</span><span class="p">(</span><span class="n">prev</span><span class="p">):</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="n">get_prev_processed</span><span class="p">(</span><span class="n">prev</span><span class="p">)</span>
    <span class="n">prev_amt_agg</span> <span class="o">=</span> <span class="n">get_prev_amt_agg</span><span class="p">(</span><span class="n">prev</span><span class="p">)</span>
    <span class="n">prev_refused_appr_agg</span> <span class="o">=</span> <span class="n">get_prev_refused_appr_agg</span><span class="p">(</span><span class="n">prev</span><span class="p">)</span>
    
    <span class="c1"># prev_amt_agg와 조인. </span>
    <span class="n">prev_agg</span> <span class="o">=</span> <span class="n">prev_amt_agg</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">prev_refused_appr_agg</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s1">'SK_ID_CURR'</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">'left'</span><span class="p">)</span>
    <span class="c1"># SK_ID_CURR별 과거 대출건수 대비 APPROVED_COUNT 및 REFUSED_COUNT 비율 생성. </span>
    <span class="n">prev_agg</span><span class="p">[</span><span class="s1">'PREV_REFUSED_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_agg</span><span class="p">[</span><span class="s1">'PREV_REFUSED_COUNT'</span><span class="p">]</span><span class="o">/</span><span class="n">prev_agg</span><span class="p">[</span><span class="s1">'PREV_SK_ID_CURR_COUNT'</span><span class="p">]</span>
    <span class="n">prev_agg</span><span class="p">[</span><span class="s1">'PREV_APPROVED_RATIO'</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_agg</span><span class="p">[</span><span class="s1">'PREV_APPROVED_COUNT'</span><span class="p">]</span><span class="o">/</span><span class="n">prev_agg</span><span class="p">[</span><span class="s1">'PREV_SK_ID_CURR_COUNT'</span><span class="p">]</span>
    <span class="c1"># 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT' 컬럼 drop </span>
    <span class="n">prev_agg</span> <span class="o">=</span> <span class="n">prev_agg</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'PREV_REFUSED_COUNT'</span><span class="p">,</span> <span class="s1">'PREV_APPROVED_COUNT'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">prev_agg</span>

<span class="k">def</span> <span class="nf">get_apps_all_with_prev_agg</span><span class="p">(</span><span class="n">apps</span><span class="p">,</span> <span class="n">prev</span><span class="p">):</span>
    <span class="n">apps_all</span> <span class="o">=</span>  <span class="n">get_apps_processed</span><span class="p">(</span><span class="n">apps</span><span class="p">)</span>
    <span class="n">prev_agg</span> <span class="o">=</span> <span class="n">get_prev_agg</span><span class="p">(</span><span class="n">prev</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'prev_agg shape:'</span><span class="p">,</span> <span class="n">prev_agg</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'apps_all before merge shape:'</span><span class="p">,</span> <span class="n">apps_all</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">apps_all</span> <span class="o">=</span> <span class="n">apps_all</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">prev_agg</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s1">'SK_ID_CURR'</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">'left'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'apps_all after merge with prev_agg shape:'</span><span class="p">,</span> <span class="n">apps_all</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">apps_all</span>

<span class="k">def</span> <span class="nf">get_apps_all_encoded</span><span class="p">(</span><span class="n">apps_all</span><span class="p">):</span>
    <span class="n">object_columns</span> <span class="o">=</span> <span class="n">apps_all</span><span class="o">.</span><span class="n">dtypes</span><span class="p">[</span><span class="n">apps_all</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="s1">'object'</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">object_columns</span><span class="p">:</span>
        <span class="n">apps_all</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">apps_all</span><span class="p">[</span><span class="n">column</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">apps_all</span>

<span class="k">def</span> <span class="nf">get_apps_all_train_test</span><span class="p">(</span><span class="n">apps_all</span><span class="p">):</span>
    <span class="n">apps_all_train</span> <span class="o">=</span> <span class="n">apps_all</span><span class="p">[</span><span class="o">~</span><span class="n">apps_all</span><span class="p">[</span><span class="s1">'TARGET'</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span>
    <span class="n">apps_all_test</span> <span class="o">=</span> <span class="n">apps_all</span><span class="p">[</span><span class="n">apps_all</span><span class="p">[</span><span class="s1">'TARGET'</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span>

    <span class="n">apps_all_test</span> <span class="o">=</span> <span class="n">apps_all_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'TARGET'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">apps_all_train</span><span class="p">,</span> <span class="n">apps_all_test</span>
    
<span class="k">def</span> <span class="nf">train_apps_all</span><span class="p">(</span><span class="n">apps_all_train</span><span class="p">):</span>
    <span class="n">ftr_app</span> <span class="o">=</span> <span class="n">apps_all_train</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'SK_ID_CURR'</span><span class="p">,</span> <span class="s1">'TARGET'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">target_app</span> <span class="o">=</span> <span class="n">apps_all_train</span><span class="p">[</span><span class="s1">'TARGET'</span><span class="p">]</span>

    <span class="n">train_x</span><span class="p">,</span> <span class="n">valid_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">valid_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">ftr_app</span><span class="p">,</span> <span class="n">target_app</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2020</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'train shape:'</span><span class="p">,</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">'valid shape:'</span><span class="p">,</span> <span class="n">valid_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">LGBMClassifier</span><span class="p">(</span>
                <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                <span class="n">n_estimators</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                <span class="n">num_leaves</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                <span class="n">max_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                <span class="n">reg_alpha</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span>
                <span class="n">reg_lambda</span><span class="o">=</span><span class="mf">0.07</span><span class="p">,</span>
                <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                <span class="n">silent</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>

    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">),</span> <span class="p">(</span><span class="n">valid_x</span><span class="p">,</span> <span class="n">valid_y</span><span class="p">)],</span> <span class="n">eval_metric</span><span class="o">=</span> <span class="s1">'auc'</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
                <span class="n">early_stopping_rounds</span><span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">clf</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="최종-집합-생성-및-인코딩,-학습/테스트-데이터-분리,-학습/검증-피처와-타겟-데이터-분리">
<a class="anchor" href="#%EC%B5%9C%EC%A2%85-%EC%A7%91%ED%95%A9-%EC%83%9D%EC%84%B1-%EB%B0%8F-%EC%9D%B8%EC%BD%94%EB%94%A9,-%ED%95%99%EC%8A%B5/%ED%85%8C%EC%8A%A4%ED%8A%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EB%A6%AC,-%ED%95%99%EC%8A%B5/%EA%B2%80%EC%A6%9D-%ED%94%BC%EC%B2%98%EC%99%80-%ED%83%80%EA%B2%9F-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EB%A6%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>최종 집합 생성 및 인코딩, 학습/테스트 데이터 분리, 학습/검증 피처와 타겟 데이터 분리<a class="anchor-link" href="#%EC%B5%9C%EC%A2%85-%EC%A7%91%ED%95%A9-%EC%83%9D%EC%84%B1-%EB%B0%8F-%EC%9D%B8%EC%BD%94%EB%94%A9,-%ED%95%99%EC%8A%B5/%ED%85%8C%EC%8A%A4%ED%8A%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EB%A6%AC,-%ED%95%99%EC%8A%B5/%EA%B2%80%EC%A6%9D-%ED%94%BC%EC%B2%98%EC%99%80-%ED%83%80%EA%B2%9F-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EB%A6%AC"> </a>
</h5>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">apps_all</span> <span class="o">=</span> <span class="n">get_apps_all_with_prev_agg</span><span class="p">(</span><span class="n">apps</span><span class="p">,</span> <span class="n">prev</span><span class="p">)</span>
<span class="n">apps_all</span> <span class="o">=</span> <span class="n">get_apps_all_encoded</span><span class="p">(</span><span class="n">apps_all</span><span class="p">)</span>
<span class="n">apps_all_train</span><span class="p">,</span> <span class="n">apps_all_test</span> <span class="o">=</span> <span class="n">get_apps_all_train_test</span><span class="p">(</span><span class="n">apps_all</span><span class="p">)</span>
<span class="n">ftr_app</span> <span class="o">=</span> <span class="n">apps_all_train</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'SK_ID_CURR'</span><span class="p">,</span> <span class="s1">'TARGET'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">target_app</span> <span class="o">=</span> <span class="n">apps_all_train</span><span class="p">[</span><span class="s1">'TARGET'</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">ftr_app</span><span class="p">,</span> <span class="n">target_app</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2020</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-cfb8ca96aa23&gt;:53: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.
  prev_amt_agg.columns = ["PREV_"+ "_".join(x).upper() for x in prev_amt_agg.columns.ravel()]
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>prev_agg shape: (338857, 41)
apps_all before merge shape: (356255, 135)
apps_all after merge with prev_agg shape: (356255, 176)
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">bayes_opt</span> <span class="kn">import</span> <span class="n">BayesianOptimization</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>필요한 부분을 최대한 처리하여 Pipeline은 필요 없을 듯 하다.</li>
<li>randomforest와 LightGBM, XGboost를 수행해보고자 한다</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install bayesian-optimization
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Requirement already satisfied: bayesian-optimization in c:\users\channee\anaconda3\lib\site-packages (1.2.0)
Requirement already satisfied: scikit-learn&gt;=0.18.0 in c:\users\channee\anaconda3\lib\site-packages (from bayesian-optimization) (0.24.1)
Requirement already satisfied: numpy&gt;=1.9.0 in c:\users\channee\anaconda3\lib\site-packages (from bayesian-optimization) (1.20.1)
Requirement already satisfied: scipy&gt;=0.14.0 in c:\users\channee\anaconda3\lib\site-packages (from bayesian-optimization) (1.6.2)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in c:\users\channee\anaconda3\lib\site-packages (from scikit-learn&gt;=0.18.0-&gt;bayesian-optimization) (2.1.0)
Requirement already satisfied: joblib&gt;=0.11 in c:\users\channee\anaconda3\lib\site-packages (from scikit-learn&gt;=0.18.0-&gt;bayesian-optimization) (1.0.1)
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">bayes_opt</span> <span class="kn">import</span> <span class="n">BayesianOptimization</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bayesian_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> 
    <span class="s1">'num_leaves'</span><span class="p">:</span> <span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> 
    <span class="s1">'min_child_samples'</span><span class="p">:</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> 
    <span class="s1">'min_child_weight'</span><span class="p">:(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="s1">'subsample'</span><span class="p">:(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
    <span class="s1">'colsample_bytree'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
    <span class="s1">'max_bin'</span><span class="p">:(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
    <span class="s1">'reg_lambda'</span><span class="p">:(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="s1">'reg_alpha'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span> 
<span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">lgb_roc_eval</span><span class="p">(</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">num_leaves</span><span class="p">,</span> <span class="n">min_child_samples</span><span class="p">,</span> <span class="n">min_child_weight</span><span class="p">,</span> <span class="n">subsample</span><span class="p">,</span> 
                <span class="n">colsample_bytree</span><span class="p">,</span><span class="n">max_bin</span><span class="p">,</span> <span class="n">reg_lambda</span><span class="p">,</span> <span class="n">reg_alpha</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"n_estimators"</span><span class="p">:</span><span class="mi">500</span><span class="p">,</span> <span class="s2">"learning_rate"</span><span class="p">:</span><span class="mf">0.02</span><span class="p">,</span>
        <span class="s1">'max_depth'</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">max_depth</span><span class="p">)),</span> <span class="c1">#  호출 시 실수형 값이 들어오므로 정수형 하이퍼 파라미터는 정수형으로 변경 </span>
        <span class="s1">'num_leaves'</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">num_leaves</span><span class="p">)),</span> 
        <span class="s1">'min_child_samples'</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">min_child_samples</span><span class="p">)),</span>
        <span class="s1">'min_child_weight'</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">min_child_weight</span><span class="p">)),</span>
        <span class="s1">'subsample'</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">subsample</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> 
        <span class="s1">'colsample_bytree'</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">colsample_bytree</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">'max_bin'</span><span class="p">:</span>  <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">max_bin</span><span class="p">)),</span><span class="mi">10</span><span class="p">),</span>
        <span class="s1">'reg_lambda'</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="n">reg_lambda</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
        <span class="s1">'reg_alpha'</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="n">reg_alpha</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="n">lgb_model</span> <span class="o">=</span> <span class="n">LGBMClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">lgb_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)],</span> <span class="n">eval_metric</span><span class="o">=</span> <span class="s1">'auc'</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
                <span class="n">early_stopping_rounds</span><span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">proba</span> <span class="o">=</span> <span class="n">lgb_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">valid_y</span><span class="p">,</span> <span class="n">valid_proba</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">roc_auc</span>   
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lgbBO</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span><span class="n">lgb_roc_eval</span><span class="p">,</span><span class="n">bayesian_params</span> <span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># 함수 반환값이 최대가 되는 입력값 유추를 위한 iteration 수행. </span>
<span class="n">lgbBO</span><span class="o">.</span><span class="n">maximize</span><span class="p">(</span><span class="n">init_points</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>|   iter    |  target   | colsam... |  max_bin  | max_depth | min_ch... | min_ch... | num_le... | reg_alpha | reg_la... | subsample |
-------------------------------------------------------------------------------------------------------------------------------------
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.769662	training's binary_logloss: 0.246032	valid_1's auc: 0.755483	valid_1's binary_logloss: 0.248917
[200]	training's auc: 0.787253	training's binary_logloss: 0.238487	valid_1's auc: 0.766224	valid_1's binary_logloss: 0.244243
[300]	training's auc: 0.798898	training's binary_logloss: 0.23398	valid_1's auc: 0.771467	valid_1's binary_logloss: 0.242344
[400]	training's auc: 0.807869	training's binary_logloss: 0.23065	valid_1's auc: 0.773972	valid_1's binary_logloss: 0.241458
[500]	training's auc: 0.815952	training's binary_logloss: 0.227669	valid_1's auc: 0.775806	valid_1's binary_logloss: 0.240831
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.815952	training's binary_logloss: 0.227669	valid_1's auc: 0.775806	valid_1's binary_logloss: 0.240831
|  1        |  0.7758   |  0.7744   |  360.4    |  12.03    |  113.5    |  21.76    |  49.84    |  21.88    |  8.918    |  0.9818   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.76256	training's binary_logloss: 0.247457	valid_1's auc: 0.7537	valid_1's binary_logloss: 0.249099
[200]	training's auc: 0.780205	training's binary_logloss: 0.24047	valid_1's auc: 0.765756	valid_1's binary_logloss: 0.244226
[300]	training's auc: 0.79096	training's binary_logloss: 0.236494	valid_1's auc: 0.771277	valid_1's binary_logloss: 0.242257
[400]	training's auc: 0.799189	training's binary_logloss: 0.233539	valid_1's auc: 0.774083	valid_1's binary_logloss: 0.241244
[500]	training's auc: 0.806102	training's binary_logloss: 0.231049	valid_1's auc: 0.775791	valid_1's binary_logloss: 0.240642
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.806102	training's binary_logloss: 0.231049	valid_1's auc: 0.775791	valid_1's binary_logloss: 0.240642
|  2        |  0.7758   |  0.6917   |  397.9    |  11.29    |  117.9    |  46.35    |  26.84    |  4.366    |  0.2032   |  0.9163   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.775644	training's binary_logloss: 0.243872	valid_1's auc: 0.757262	valid_1's binary_logloss: 0.247984
[200]	training's auc: 0.796903	training's binary_logloss: 0.235046	valid_1's auc: 0.768757	valid_1's binary_logloss: 0.243207
[300]	training's auc: 0.812041	training's binary_logloss: 0.229261	valid_1's auc: 0.773366	valid_1's binary_logloss: 0.24156
[400]	training's auc: 0.824984	training's binary_logloss: 0.224423	valid_1's auc: 0.776251	valid_1's binary_logloss: 0.240565
[500]	training's auc: 0.835704	training's binary_logloss: 0.220372	valid_1's auc: 0.777178	valid_1's binary_logloss: 0.24019
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.835704	training's binary_logloss: 0.220372	valid_1's auc: 0.777178	valid_1's binary_logloss: 0.24019
| <span class="ansi-magenta-intense-fg"> 3       </span> | <span class="ansi-magenta-intense-fg"> 0.7772  </span> | <span class="ansi-magenta-intense-fg"> 0.8891  </span> | <span class="ansi-magenta-intense-fg"> 436.3   </span> | <span class="ansi-magenta-intense-fg"> 15.79   </span> | <span class="ansi-magenta-intense-fg"> 161.8   </span> | <span class="ansi-magenta-intense-fg"> 23.61   </span> | <span class="ansi-magenta-intense-fg"> 55.22   </span> | <span class="ansi-magenta-intense-fg"> 5.923   </span> | <span class="ansi-magenta-intense-fg"> 6.4     </span> | <span class="ansi-magenta-intense-fg"> 0.5717  </span> |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.765842	training's binary_logloss: 0.246884	valid_1's auc: 0.753808	valid_1's binary_logloss: 0.249258
[200]	training's auc: 0.783137	training's binary_logloss: 0.239739	valid_1's auc: 0.765526	valid_1's binary_logloss: 0.244489
[300]	training's auc: 0.793648	training's binary_logloss: 0.23577	valid_1's auc: 0.770579	valid_1's binary_logloss: 0.242643
[400]	training's auc: 0.801582	training's binary_logloss: 0.232812	valid_1's auc: 0.773253	valid_1's binary_logloss: 0.241685
[500]	training's auc: 0.808241	training's binary_logloss: 0.23036	valid_1's auc: 0.774833	valid_1's binary_logloss: 0.241128
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.808241	training's binary_logloss: 0.23036	valid_1's auc: 0.774833	valid_1's binary_logloss: 0.241128
|  4        |  0.7748   |  0.9723   |  265.7    |  10.15    |  60.27    |  38.94    |  42.25    |  28.43    |  0.1889   |  0.8088   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.7659	training's binary_logloss: 0.247276	valid_1's auc: 0.753968	valid_1's binary_logloss: 0.24956
[200]	training's auc: 0.781741	training's binary_logloss: 0.240327	valid_1's auc: 0.764668	valid_1's binary_logloss: 0.244831
[300]	training's auc: 0.791758	training's binary_logloss: 0.236503	valid_1's auc: 0.769722	valid_1's binary_logloss: 0.242955
[400]	training's auc: 0.799313	training's binary_logloss: 0.233722	valid_1's auc: 0.772473	valid_1's binary_logloss: 0.241976
[500]	training's auc: 0.805605	training's binary_logloss: 0.231421	valid_1's auc: 0.774035	valid_1's binary_logloss: 0.241428
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.805605	training's binary_logloss: 0.231421	valid_1's auc: 0.774035	valid_1's binary_logloss: 0.241428
|  5        |  0.774    |  0.806    |  312.3    |  15.44    |  139.5    |  18.62    |  41.48    |  34.88    |  0.6032   |  0.8334   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.778416	training's binary_logloss: 0.243866	valid_1's auc: 0.758906	valid_1's binary_logloss: 0.248045
[200]	training's auc: 0.797547	training's binary_logloss: 0.235078	valid_1's auc: 0.768769	valid_1's binary_logloss: 0.243239
[300]	training's auc: 0.812388	training's binary_logloss: 0.229334	valid_1's auc: 0.773393	valid_1's binary_logloss: 0.241516
[400]	training's auc: 0.824856	training's binary_logloss: 0.224602	valid_1's auc: 0.77606	valid_1's binary_logloss: 0.240581
[500]	training's auc: 0.835757	training's binary_logloss: 0.220492	valid_1's auc: 0.777391	valid_1's binary_logloss: 0.240112
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.835757	training's binary_logloss: 0.220492	valid_1's auc: 0.777391	valid_1's binary_logloss: 0.240112
| <span class="ansi-magenta-intense-fg"> 6       </span> | <span class="ansi-magenta-intense-fg"> 0.7774  </span> | <span class="ansi-magenta-intense-fg"> 0.6405  </span> | <span class="ansi-magenta-intense-fg"> 435.0   </span> | <span class="ansi-magenta-intense-fg"> 13.5    </span> | <span class="ansi-magenta-intense-fg"> 169.3   </span> | <span class="ansi-magenta-intense-fg"> 26.92   </span> | <span class="ansi-magenta-intense-fg"> 57.69   </span> | <span class="ansi-magenta-intense-fg"> 5.768   </span> | <span class="ansi-magenta-intense-fg"> 9.196   </span> | <span class="ansi-magenta-intense-fg"> 0.613   </span> |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.772281	training's binary_logloss: 0.246014	valid_1's auc: 0.756518	valid_1's binary_logloss: 0.249054
[200]	training's auc: 0.789124	training's binary_logloss: 0.238224	valid_1's auc: 0.766602	valid_1's binary_logloss: 0.244218
[300]	training's auc: 0.800463	training's binary_logloss: 0.233735	valid_1's auc: 0.771115	valid_1's binary_logloss: 0.242475
[400]	training's auc: 0.809924	training's binary_logloss: 0.230144	valid_1's auc: 0.774036	valid_1's binary_logloss: 0.241453
[500]	training's auc: 0.81804	training's binary_logloss: 0.227105	valid_1's auc: 0.775536	valid_1's binary_logloss: 0.240903
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.81804	training's binary_logloss: 0.227105	valid_1's auc: 0.775536	valid_1's binary_logloss: 0.240903
|  7        |  0.7755   |  0.6778   |  477.2    |  11.75    |  194.9    |  46.85    |  59.47    |  26.32    |  8.388    |  0.7753   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.777583	training's binary_logloss: 0.243725	valid_1's auc: 0.758288	valid_1's binary_logloss: 0.247968
[200]	training's auc: 0.797949	training's binary_logloss: 0.234844	valid_1's auc: 0.768779	valid_1's binary_logloss: 0.24323
[300]	training's auc: 0.812857	training's binary_logloss: 0.229109	valid_1's auc: 0.773238	valid_1's binary_logloss: 0.241582
[400]	training's auc: 0.825851	training's binary_logloss: 0.22425	valid_1's auc: 0.776055	valid_1's binary_logloss: 0.240619
[500]	training's auc: 0.836858	training's binary_logloss: 0.220087	valid_1's auc: 0.777214	valid_1's binary_logloss: 0.240157
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.836858	training's binary_logloss: 0.220087	valid_1's auc: 0.777214	valid_1's binary_logloss: 0.240157
|  8        |  0.7772   |  0.7374   |  408.1    |  12.84    |  197.6    |  5.693    |  57.73    |  6.171    |  8.007    |  0.6427   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.776884	training's binary_logloss: 0.244363	valid_1's auc: 0.757931	valid_1's binary_logloss: 0.248329
[200]	training's auc: 0.795476	training's binary_logloss: 0.235896	valid_1's auc: 0.767942	valid_1's binary_logloss: 0.243577
[300]	training's auc: 0.809275	training's binary_logloss: 0.230547	valid_1's auc: 0.77254	valid_1's binary_logloss: 0.241869
[400]	training's auc: 0.82066	training's binary_logloss: 0.226218	valid_1's auc: 0.774854	valid_1's binary_logloss: 0.241044
[500]	training's auc: 0.830938	training's binary_logloss: 0.222269	valid_1's auc: 0.776395	valid_1's binary_logloss: 0.240484
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.830938	training's binary_logloss: 0.222269	valid_1's auc: 0.776395	valid_1's binary_logloss: 0.240484
|  9        |  0.7764   |  0.7749   |  388.9    |  14.3     |  193.9    |  49.91    |  62.85    |  13.15    |  9.906    |  0.6812   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.764203	training's binary_logloss: 0.247826	valid_1's auc: 0.752311	valid_1's binary_logloss: 0.249919
[200]	training's auc: 0.779974	training's binary_logloss: 0.241103	valid_1's auc: 0.763794	valid_1's binary_logloss: 0.245126
[300]	training's auc: 0.789033	training's binary_logloss: 0.237594	valid_1's auc: 0.768866	valid_1's binary_logloss: 0.243267
[400]	training's auc: 0.795627	training's binary_logloss: 0.235115	valid_1's auc: 0.771779	valid_1's binary_logloss: 0.242221
[500]	training's auc: 0.800884	training's binary_logloss: 0.233158	valid_1's auc: 0.773414	valid_1's binary_logloss: 0.241632
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.800884	training's binary_logloss: 0.233158	valid_1's auc: 0.773414	valid_1's binary_logloss: 0.241632
|  10       |  0.7734   |  0.9461   |  421.7    |  6.672    |  189.9    |  10.04    |  47.84    |  46.75    |  9.148    |  0.7801   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.764174	training's binary_logloss: 0.248029	valid_1's auc: 0.755162	valid_1's binary_logloss: 0.249577
[200]	training's auc: 0.778767	training's binary_logloss: 0.241173	valid_1's auc: 0.765754	valid_1's binary_logloss: 0.244443
[300]	training's auc: 0.788759	training's binary_logloss: 0.237386	valid_1's auc: 0.771036	valid_1's binary_logloss: 0.242441
[400]	training's auc: 0.795987	training's binary_logloss: 0.234694	valid_1's auc: 0.773675	valid_1's binary_logloss: 0.241458
[500]	training's auc: 0.80232	training's binary_logloss: 0.232393	valid_1's auc: 0.775601	valid_1's binary_logloss: 0.240772
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.80232	training's binary_logloss: 0.232393	valid_1's auc: 0.775601	valid_1's binary_logloss: 0.240772
|  11       |  0.7756   |  0.5017   |  393.7    |  10.71    |  119.0    |  45.57    |  28.2     |  10.95    |  4.662    |  0.9552   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.778162	training's binary_logloss: 0.243432	valid_1's auc: 0.759238	valid_1's binary_logloss: 0.247741
[200]	training's auc: 0.799162	training's binary_logloss: 0.234309	valid_1's auc: 0.769473	valid_1's binary_logloss: 0.24293
[300]	training's auc: 0.814812	training's binary_logloss: 0.228328	valid_1's auc: 0.773682	valid_1's binary_logloss: 0.241344
[400]	training's auc: 0.827839	training's binary_logloss: 0.223397	valid_1's auc: 0.776065	valid_1's binary_logloss: 0.240505
[500]	training's auc: 0.838855	training's binary_logloss: 0.219213	valid_1's auc: 0.777105	valid_1's binary_logloss: 0.240137
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.838855	training's binary_logloss: 0.219213	valid_1's auc: 0.777105	valid_1's binary_logloss: 0.240137
|  12       |  0.7771   |  0.6423   |  400.8    |  13.13    |  174.5    |  22.2     |  53.42    |  0.8135   |  3.857    |  0.603    |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.777272	training's binary_logloss: 0.243367	valid_1's auc: 0.759082	valid_1's binary_logloss: 0.247643
[200]	training's auc: 0.7988	training's binary_logloss: 0.234288	valid_1's auc: 0.769727	valid_1's binary_logloss: 0.24285
[300]	training's auc: 0.814442	training's binary_logloss: 0.228368	valid_1's auc: 0.773843	valid_1's binary_logloss: 0.241303
[400]	training's auc: 0.827084	training's binary_logloss: 0.223525	valid_1's auc: 0.776152	valid_1's binary_logloss: 0.240502
[500]	training's auc: 0.838169	training's binary_logloss: 0.219393	valid_1's auc: 0.777237	valid_1's binary_logloss: 0.240146
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.838169	training's binary_logloss: 0.219393	valid_1's auc: 0.777237	valid_1's binary_logloss: 0.240146
|  13       |  0.7772   |  0.6969   |  388.8    |  13.69    |  178.8    |  26.23    |  50.31    |  0.1285   |  1.032    |  0.8563   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.777861	training's binary_logloss: 0.242936	valid_1's auc: 0.757958	valid_1's binary_logloss: 0.247557
[200]	training's auc: 0.801461	training's binary_logloss: 0.233442	valid_1's auc: 0.769359	valid_1's binary_logloss: 0.242945
[300]	training's auc: 0.818486	training's binary_logloss: 0.226978	valid_1's auc: 0.77388	valid_1's binary_logloss: 0.241346
[400]	training's auc: 0.832854	training's binary_logloss: 0.221651	valid_1's auc: 0.776378	valid_1's binary_logloss: 0.240465
[500]	training's auc: 0.844619	training's binary_logloss: 0.217191	valid_1's auc: 0.777355	valid_1's binary_logloss: 0.240144
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.844619	training's binary_logloss: 0.217191	valid_1's auc: 0.777355	valid_1's binary_logloss: 0.240144
|  14       |  0.7774   |  0.9789   |  363.4    |  15.32    |  190.7    |  2.003    |  57.05    |  1.982    |  4.416    |  0.9291   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.779098	training's binary_logloss: 0.242597	valid_1's auc: 0.758713	valid_1's binary_logloss: 0.247432
[200]	training's auc: 0.802	training's binary_logloss: 0.233149	valid_1's auc: 0.769621	valid_1's binary_logloss: 0.242884
[300]	training's auc: 0.81865	training's binary_logloss: 0.22679	valid_1's auc: 0.773862	valid_1's binary_logloss: 0.241339
[400]	training's auc: 0.832546	training's binary_logloss: 0.221521	valid_1's auc: 0.776112	valid_1's binary_logloss: 0.24053
[500]	training's auc: 0.844492	training's binary_logloss: 0.216971	valid_1's auc: 0.776749	valid_1's binary_logloss: 0.240308
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.844492	training's binary_logloss: 0.216971	valid_1's auc: 0.776749	valid_1's binary_logloss: 0.240308
|  15       |  0.7767   |  0.8747   |  398.4    |  15.32    |  177.1    |  23.05    |  56.52    |  2.533    |  1.005    |  0.9827   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.76187	training's binary_logloss: 0.247879	valid_1's auc: 0.753913	valid_1's binary_logloss: 0.249302
[200]	training's auc: 0.778662	training's binary_logloss: 0.241031	valid_1's auc: 0.765519	valid_1's binary_logloss: 0.2444
[300]	training's auc: 0.78922	training's binary_logloss: 0.23712	valid_1's auc: 0.771074	valid_1's binary_logloss: 0.242375
[400]	training's auc: 0.797187	training's binary_logloss: 0.234247	valid_1's auc: 0.773953	valid_1's binary_logloss: 0.241344
[500]	training's auc: 0.804056	training's binary_logloss: 0.231805	valid_1's auc: 0.775929	valid_1's binary_logloss: 0.24065
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.804056	training's binary_logloss: 0.231805	valid_1's auc: 0.775929	valid_1's binary_logloss: 0.24065
|  16       |  0.7759   |  0.6846   |  368.2    |  14.93    |  197.8    |  25.54    |  25.02    |  4.245    |  2.664    |  0.8288   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.76672	training's binary_logloss: 0.247028	valid_1's auc: 0.756541	valid_1's binary_logloss: 0.248962
[200]	training's auc: 0.783669	training's binary_logloss: 0.239569	valid_1's auc: 0.767434	valid_1's binary_logloss: 0.24386
[300]	training's auc: 0.79523	training's binary_logloss: 0.235244	valid_1's auc: 0.772526	valid_1's binary_logloss: 0.241924
[400]	training's auc: 0.804427	training's binary_logloss: 0.231925	valid_1's auc: 0.775267	valid_1's binary_logloss: 0.240915
[500]	training's auc: 0.811987	training's binary_logloss: 0.229176	valid_1's auc: 0.776778	valid_1's binary_logloss: 0.240381
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.811987	training's binary_logloss: 0.229176	valid_1's auc: 0.776778	valid_1's binary_logloss: 0.240381
|  17       |  0.7768   |  0.5385   |  375.6    |  15.71    |  162.2    |  1.157    |  31.29    |  0.3692   |  9.23     |  0.5688   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.763487	training's binary_logloss: 0.247559	valid_1's auc: 0.753988	valid_1's binary_logloss: 0.249255
[200]	training's auc: 0.780223	training's binary_logloss: 0.240626	valid_1's auc: 0.76534	valid_1's binary_logloss: 0.244429
[300]	training's auc: 0.790465	training's binary_logloss: 0.236701	valid_1's auc: 0.770206	valid_1's binary_logloss: 0.242582
[400]	training's auc: 0.797595	training's binary_logloss: 0.234049	valid_1's auc: 0.772583	valid_1's binary_logloss: 0.24171
[500]	training's auc: 0.80371	training's binary_logloss: 0.231846	valid_1's auc: 0.774369	valid_1's binary_logloss: 0.241097
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.80371	training's binary_logloss: 0.231846	valid_1's auc: 0.774369	valid_1's binary_logloss: 0.241097
|  18       |  0.7744   |  0.7394   |  417.5    |  6.129    |  174.7    |  31.59    |  28.05    |  1.518    |  6.418    |  0.8383   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.776986	training's binary_logloss: 0.24363	valid_1's auc: 0.757374	valid_1's binary_logloss: 0.248025
[200]	training's auc: 0.797886	training's binary_logloss: 0.234776	valid_1's auc: 0.768483	valid_1's binary_logloss: 0.243304
[300]	training's auc: 0.813076	training's binary_logloss: 0.228954	valid_1's auc: 0.77327	valid_1's binary_logloss: 0.241566
[400]	training's auc: 0.825935	training's binary_logloss: 0.224135	valid_1's auc: 0.775806	valid_1's binary_logloss: 0.240681
[500]	training's auc: 0.836838	training's binary_logloss: 0.219956	valid_1's auc: 0.776992	valid_1's binary_logloss: 0.240235
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.836838	training's binary_logloss: 0.219956	valid_1's auc: 0.776992	valid_1's binary_logloss: 0.240235
|  19       |  0.777    |  0.8929   |  381.0    |  12.24    |  162.5    |  1.124    |  62.24    |  9.051    |  8.559    |  0.5757   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.774485	training's binary_logloss: 0.244852	valid_1's auc: 0.757004	valid_1's binary_logloss: 0.248424
[200]	training's auc: 0.792537	training's binary_logloss: 0.236941	valid_1's auc: 0.767318	valid_1's binary_logloss: 0.243762
[300]	training's auc: 0.804149	training's binary_logloss: 0.232344	valid_1's auc: 0.771777	valid_1's binary_logloss: 0.242089
[400]	training's auc: 0.8131	training's binary_logloss: 0.228878	valid_1's auc: 0.774161	valid_1's binary_logloss: 0.241232
[500]	training's auc: 0.821163	training's binary_logloss: 0.225782	valid_1's auc: 0.775404	valid_1's binary_logloss: 0.240783
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.821163	training's binary_logloss: 0.225782	valid_1's auc: 0.775404	valid_1's binary_logloss: 0.240783
|  20       |  0.7754   |  0.8395   |  369.5    |  7.312    |  179.4    |  17.39    |  58.63    |  13.71    |  4.133    |  0.8568   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.780021	training's binary_logloss: 0.243308	valid_1's auc: 0.759823	valid_1's binary_logloss: 0.24777
[200]	training's auc: 0.800958	training's binary_logloss: 0.233946	valid_1's auc: 0.769898	valid_1's binary_logloss: 0.24286
[300]	training's auc: 0.81698	training's binary_logloss: 0.22778	valid_1's auc: 0.774314	valid_1's binary_logloss: 0.241218
[400]	training's auc: 0.830722	training's binary_logloss: 0.222612	valid_1's auc: 0.776942	valid_1's binary_logloss: 0.240308
[500]	training's auc: 0.842042	training's binary_logloss: 0.218291	valid_1's auc: 0.778057	valid_1's binary_logloss: 0.239921
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.842042	training's binary_logloss: 0.218291	valid_1's auc: 0.778057	valid_1's binary_logloss: 0.239921
| <span class="ansi-magenta-intense-fg"> 21      </span> | <span class="ansi-magenta-intense-fg"> 0.7781  </span> | <span class="ansi-magenta-intense-fg"> 0.5967  </span> | <span class="ansi-magenta-intense-fg"> 403.2   </span> | <span class="ansi-magenta-intense-fg"> 12.91   </span> | <span class="ansi-magenta-intense-fg"> 158.8   </span> | <span class="ansi-magenta-intense-fg"> 7.338   </span> | <span class="ansi-magenta-intense-fg"> 58.12   </span> | <span class="ansi-magenta-intense-fg"> 0.6682  </span> | <span class="ansi-magenta-intense-fg"> 9.355   </span> | <span class="ansi-magenta-intense-fg"> 0.801   </span> |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.77898	training's binary_logloss: 0.243342	valid_1's auc: 0.758555	valid_1's binary_logloss: 0.247992
[200]	training's auc: 0.798959	training's binary_logloss: 0.234514	valid_1's auc: 0.76841	valid_1's binary_logloss: 0.243369
[300]	training's auc: 0.814164	training's binary_logloss: 0.228666	valid_1's auc: 0.773321	valid_1's binary_logloss: 0.241591
[400]	training's auc: 0.827085	training's binary_logloss: 0.223771	valid_1's auc: 0.776039	valid_1's binary_logloss: 0.240644
[500]	training's auc: 0.838285	training's binary_logloss: 0.219513	valid_1's auc: 0.777508	valid_1's binary_logloss: 0.240124
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.838285	training's binary_logloss: 0.219513	valid_1's auc: 0.777508	valid_1's binary_logloss: 0.240124
|  22       |  0.7775   |  0.6984   |  408.4    |  15.75    |  137.0    |  1.697    |  63.85    |  11.6     |  0.1763   |  0.864    |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.777057	training's binary_logloss: 0.243549	valid_1's auc: 0.757727	valid_1's binary_logloss: 0.247866
[200]	training's auc: 0.798708	training's binary_logloss: 0.234457	valid_1's auc: 0.769064	valid_1's binary_logloss: 0.243119
[300]	training's auc: 0.814158	training's binary_logloss: 0.228529	valid_1's auc: 0.773437	valid_1's binary_logloss: 0.241564
[400]	training's auc: 0.827436	training's binary_logloss: 0.223518	valid_1's auc: 0.776108	valid_1's binary_logloss: 0.240607
[500]	training's auc: 0.83867	training's binary_logloss: 0.219253	valid_1's auc: 0.777169	valid_1's binary_logloss: 0.240208
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.83867	training's binary_logloss: 0.219253	valid_1's auc: 0.777169	valid_1's binary_logloss: 0.240208
|  23       |  0.7772   |  0.9089   |  407.4    |  15.54    |  180.7    |  24.17    |  58.56    |  5.618    |  9.113    |  0.788    |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.7743	training's binary_logloss: 0.244663	valid_1's auc: 0.757346	valid_1's binary_logloss: 0.248295
[200]	training's auc: 0.7934	training's binary_logloss: 0.236415	valid_1's auc: 0.768102	valid_1's binary_logloss: 0.243544
[300]	training's auc: 0.806554	training's binary_logloss: 0.231308	valid_1's auc: 0.772786	valid_1's binary_logloss: 0.241797
[400]	training's auc: 0.817607	training's binary_logloss: 0.22716	valid_1's auc: 0.775328	valid_1's binary_logloss: 0.240878
[500]	training's auc: 0.827256	training's binary_logloss: 0.223529	valid_1's auc: 0.776919	valid_1's binary_logloss: 0.240307
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.827256	training's binary_logloss: 0.223529	valid_1's auc: 0.776919	valid_1's binary_logloss: 0.240307
|  24       |  0.7769   |  0.8291   |  408.9    |  10.8     |  198.5    |  10.39    |  54.8     |  12.48    |  6.565    |  0.5806   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.780987	training's binary_logloss: 0.242281	valid_1's auc: 0.759117	valid_1's binary_logloss: 0.247485
[200]	training's auc: 0.803399	training's binary_logloss: 0.232707	valid_1's auc: 0.769436	valid_1's binary_logloss: 0.242907
[300]	training's auc: 0.820217	training's binary_logloss: 0.226213	valid_1's auc: 0.773667	valid_1's binary_logloss: 0.241358
[400]	training's auc: 0.834774	training's binary_logloss: 0.220692	valid_1's auc: 0.776268	valid_1's binary_logloss: 0.240465
[500]	training's auc: 0.846897	training's binary_logloss: 0.215994	valid_1's auc: 0.777223	valid_1's binary_logloss: 0.240096
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.846897	training's binary_logloss: 0.215994	valid_1's auc: 0.777223	valid_1's binary_logloss: 0.240096
|  25       |  0.7772   |  0.8242   |  414.0    |  12.84    |  139.6    |  23.64    |  62.52    |  4.5      |  1.024    |  0.9178   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.781337	training's binary_logloss: 0.242246	valid_1's auc: 0.759542	valid_1's binary_logloss: 0.247353
[200]	training's auc: 0.80494	training's binary_logloss: 0.232417	valid_1's auc: 0.770308	valid_1's binary_logloss: 0.242697
[300]	training's auc: 0.822298	training's binary_logloss: 0.22572	valid_1's auc: 0.774614	valid_1's binary_logloss: 0.241111
[400]	training's auc: 0.836837	training's binary_logloss: 0.220186	valid_1's auc: 0.777086	valid_1's binary_logloss: 0.240274
[500]	training's auc: 0.848534	training's binary_logloss: 0.21562	valid_1's auc: 0.777809	valid_1's binary_logloss: 0.23998
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.848534	training's binary_logloss: 0.21562	valid_1's auc: 0.777809	valid_1's binary_logloss: 0.23998
|  26       |  0.7778   |  0.8364   |  438.5    |  12.59    |  126.3    |  7.767    |  63.26    |  0.3294   |  9.232    |  0.6026   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.777309	training's binary_logloss: 0.243448	valid_1's auc: 0.75775	valid_1's binary_logloss: 0.247903
[200]	training's auc: 0.799142	training's binary_logloss: 0.234375	valid_1's auc: 0.768792	valid_1's binary_logloss: 0.243165
[300]	training's auc: 0.814917	training's binary_logloss: 0.22837	valid_1's auc: 0.773507	valid_1's binary_logloss: 0.241467
[400]	training's auc: 0.828058	training's binary_logloss: 0.223386	valid_1's auc: 0.775906	valid_1's binary_logloss: 0.240582
[500]	training's auc: 0.839395	training's binary_logloss: 0.219123	valid_1's auc: 0.777117	valid_1's binary_logloss: 0.240155
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.839395	training's binary_logloss: 0.219123	valid_1's auc: 0.777117	valid_1's binary_logloss: 0.240155
|  27       |  0.7771   |  0.7062   |  438.3    |  13.16    |  93.45    |  3.168    |  56.62    |  4.489    |  5.242    |  0.5061   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.778145	training's binary_logloss: 0.243305	valid_1's auc: 0.757936	valid_1's binary_logloss: 0.2478
[200]	training's auc: 0.799265	training's binary_logloss: 0.234379	valid_1's auc: 0.768543	valid_1's binary_logloss: 0.243239
[300]	training's auc: 0.81363	training's binary_logloss: 0.228749	valid_1's auc: 0.772821	valid_1's binary_logloss: 0.241685
[400]	training's auc: 0.825123	training's binary_logloss: 0.224326	valid_1's auc: 0.774995	valid_1's binary_logloss: 0.240928
[500]	training's auc: 0.835617	training's binary_logloss: 0.220326	valid_1's auc: 0.776429	valid_1's binary_logloss: 0.240431
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.835617	training's binary_logloss: 0.220326	valid_1's auc: 0.776429	valid_1's binary_logloss: 0.240431
|  28       |  0.7764   |  0.9458   |  465.0    |  7.9      |  121.9    |  2.539    |  62.43    |  4.784    |  9.659    |  0.5056   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.768712	training's binary_logloss: 0.245667	valid_1's auc: 0.755179	valid_1's binary_logloss: 0.248495
[200]	training's auc: 0.788298	training's binary_logloss: 0.237819	valid_1's auc: 0.767231	valid_1's binary_logloss: 0.243688
[300]	training's auc: 0.801509	training's binary_logloss: 0.232922	valid_1's auc: 0.772532	valid_1's binary_logloss: 0.241848
[400]	training's auc: 0.811986	training's binary_logloss: 0.229057	valid_1's auc: 0.775444	valid_1's binary_logloss: 0.24083
[500]	training's auc: 0.820471	training's binary_logloss: 0.225952	valid_1's auc: 0.776356	valid_1's binary_logloss: 0.24049
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.820471	training's binary_logloss: 0.225952	valid_1's auc: 0.776356	valid_1's binary_logloss: 0.24049
|  29       |  0.7764   |  0.9001   |  432.8    |  12.59    |  121.5    |  4.114    |  38.13    |  2.886    |  7.886    |  0.9864   |
Training until validation scores don't improve for 100 rounds
[100]	training's auc: 0.78142	training's binary_logloss: 0.242343	valid_1's auc: 0.760102	valid_1's binary_logloss: 0.247216
[200]	training's auc: 0.803636	training's binary_logloss: 0.232854	valid_1's auc: 0.770109	valid_1's binary_logloss: 0.242694
[300]	training's auc: 0.819047	training's binary_logloss: 0.226767	valid_1's auc: 0.77406	valid_1's binary_logloss: 0.241196
[400]	training's auc: 0.831064	training's binary_logloss: 0.22211	valid_1's auc: 0.775997	valid_1's binary_logloss: 0.240495
[500]	training's auc: 0.841456	training's binary_logloss: 0.218102	valid_1's auc: 0.777091	valid_1's binary_logloss: 0.240091
Did not meet early stopping. Best iteration is:
[500]	training's auc: 0.841456	training's binary_logloss: 0.218102	valid_1's auc: 0.777091	valid_1's binary_logloss: 0.240091
|  30       |  0.7771   |  0.7396   |  442.3    |  9.243    |  126.5    |  31.85    |  63.82    |  0.6593   |  6.771    |  0.8922   |
=====================================================================================================================================
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lgbBO</span><span class="o">.</span><span class="n">res</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[{'target': 0.7758055093230539,
  'params': {'colsample_bytree': 0.7744067519636624,
   'max_bin': 360.44278952248555,
   'max_depth': 12.027633760716439,
   'min_child_samples': 113.52780476941041,
   'min_child_weight': 21.75908516760633,
   'num_leaves': 49.835764522666246,
   'reg_alpha': 21.884984691022,
   'reg_lambda': 8.917838234820016,
   'subsample': 0.9818313802505146}},
 {'target': 0.7757909289675659,
  'params': {'colsample_bytree': 0.6917207594128889,
   'max_bin': 397.94526866050563,
   'max_depth': 11.288949197529044,
   'min_child_samples': 117.92846660784714,
   'min_child_weight': 46.35423527634039,
   'num_leaves': 26.841442327915477,
   'reg_alpha': 4.36559369208002,
   'reg_lambda': 0.20316375600581688,
   'subsample': 0.916309922773969}},
 {'target': 0.7771779879367707,
  'params': {'colsample_bytree': 0.8890783754749252,
   'max_bin': 436.30595264094137,
   'max_depth': 15.78618342232764,
   'min_child_samples': 161.8401272011775,
   'min_child_weight': 23.61248875039366,
   'num_leaves': 55.22116705145822,
   'reg_alpha': 5.922538549187972,
   'reg_lambda': 6.3995702922539115,
   'subsample': 0.5716766437045232}},
 {'target': 0.7748333416046418,
  'params': {'colsample_bytree': 0.972334458524792,
   'max_bin': 265.70567765753515,
   'max_depth': 10.146619399905235,
   'min_child_samples': 60.265566299879126,
   'min_child_weight': 38.93745078227661,
   'num_leaves': 42.24601328866194,
   'reg_alpha': 28.426013103943742,
   'reg_lambda': 0.18887921456311507,
   'subsample': 0.8088177485379385}},
 {'target': 0.774035094542375,
  'params': {'colsample_bytree': 0.8060478613612108,
   'max_bin': 312.2976584686309,
   'max_depth': 15.437480785146242,
   'min_child_samples': 139.54585682966186,
   'min_child_weight': 18.615887128115514,
   'num_leaves': 41.481278151973655,
   'reg_alpha': 34.88458348440397,
   'reg_lambda': 0.6031944908210691,
   'subsample': 0.8333833577228338}},
 {'target': 0.7773908017189786,
  'params': {'colsample_bytree': 0.640481830861817,
   'max_bin': 435.0379450370509,
   'max_depth': 13.497244758196743,
   'min_child_samples': 169.30259380663517,
   'min_child_weight': 26.924368410534857,
   'num_leaves': 57.69153705029583,
   'reg_alpha': 5.7675960060342195,
   'reg_lambda': 9.196441703351635,
   'subsample': 0.6129607288812317}},
 {'target': 0.7755362516633085,
  'params': {'colsample_bytree': 0.6778196691000836,
   'max_bin': 477.176698811766,
   'max_depth': 11.752369565537183,
   'min_child_samples': 194.8929524178145,
   'min_child_weight': 46.84607882647774,
   'num_leaves': 59.472175385640206,
   'reg_alpha': 26.316833854656593,
   'reg_lambda': 8.387658534981561,
   'subsample': 0.7753185059732159}},
 {'target': 0.7772142029411041,
  'params': {'colsample_bytree': 0.7373732904804776,
   'max_bin': 408.1358313138957,
   'max_depth': 12.843992667819151,
   'min_child_samples': 197.5558677155646,
   'min_child_weight': 5.693485177437429,
   'num_leaves': 57.72941400376497,
   'reg_alpha': 6.171344107340323,
   'reg_lambda': 8.007058832279368,
   'subsample': 0.6427405880739969}},
 {'target': 0.7763952615906466,
  'params': {'colsample_bytree': 0.7748893532024008,
   'max_bin': 388.9169674423393,
   'max_depth': 14.298324669842952,
   'min_child_samples': 193.91589835191607,
   'min_child_weight': 49.909444261894315,
   'num_leaves': 62.84933349372062,
   'reg_alpha': 13.150761788903786,
   'reg_lambda': 9.906371801724482,
   'subsample': 0.6811573472682452}},
 {'target': 0.7734141467631326,
  'params': {'colsample_bytree': 0.9461483552083645,
   'max_bin': 421.69292633292594,
   'max_depth': 6.671600015870614,
   'min_child_samples': 189.89049916570497,
   'min_child_weight': 10.044424315906738,
   'num_leaves': 47.83526701748679,
   'reg_alpha': 46.74823035052019,
   'reg_lambda': 9.147970169890396,
   'subsample': 0.7801135437391282}},
 {'target': 0.7756008426857747,
  'params': {'colsample_bytree': 0.5017469237656516,
   'max_bin': 393.72190024490175,
   'max_depth': 10.707724066578827,
   'min_child_samples': 118.96813016814396,
   'min_child_weight': 45.56868876896979,
   'num_leaves': 28.201502549830405,
   'reg_alpha': 10.952704243299513,
   'reg_lambda': 4.662235191571802,
   'subsample': 0.9551699402948195}},
 {'target': 0.7771050305636831,
  'params': {'colsample_bytree': 0.6422839773084814,
   'max_bin': 400.82150970541625,
   'max_depth': 13.131175782431662,
   'min_child_samples': 174.45170668076625,
   'min_child_weight': 22.20273089632871,
   'num_leaves': 53.41657085105421,
   'reg_alpha': 0.8135160827380638,
   'reg_lambda': 3.856905787752113,
   'subsample': 0.6029851974682363}},
 {'target': 0.7772365047377109,
  'params': {'colsample_bytree': 0.6968897778533021,
   'max_bin': 388.82764448356886,
   'max_depth': 13.691810834514651,
   'min_child_samples': 178.80475184834663,
   'min_child_weight': 26.23145438502705,
   'num_leaves': 50.312746331501124,
   'reg_alpha': 0.1285436498176259,
   'reg_lambda': 1.032293662925894,
   'subsample': 0.8562506227206372}},
 {'target': 0.7773549981224318,
  'params': {'colsample_bytree': 0.9789353469971445,
   'max_bin': 363.4354295856673,
   'max_depth': 15.315211746017521,
   'min_child_samples': 190.73741827012313,
   'min_child_weight': 2.003360059766269,
   'num_leaves': 57.046953584464674,
   'reg_alpha': 1.9824621842535364,
   'reg_lambda': 4.416491356182225,
   'subsample': 0.9290899949750824}},
 {'target': 0.7767492867273098,
  'params': {'colsample_bytree': 0.8747411340191682,
   'max_bin': 398.36524681235744,
   'max_depth': 15.320951372373688,
   'min_child_samples': 177.08379767055555,
   'min_child_weight': 23.052918269012956,
   'num_leaves': 56.51625901289448,
   'reg_alpha': 2.5331760761204847,
   'reg_lambda': 1.0049109599081048,
   'subsample': 0.9826775193057701}},
 {'target': 0.775929160395352,
  'params': {'colsample_bytree': 0.6845927664319438,
   'max_bin': 368.221991524003,
   'max_depth': 14.927073385197513,
   'min_child_samples': 197.8030464042996,
   'min_child_weight': 25.535853809991778,
   'num_leaves': 25.021887939272,
   'reg_alpha': 4.2450984307780235,
   'reg_lambda': 2.664497197105028,
   'subsample': 0.8288176887623219}},
 {'target': 0.7767777231064307,
  'params': {'colsample_bytree': 0.538512544314991,
   'max_bin': 375.62078344659466,
   'max_depth': 15.712329472869794,
   'min_child_samples': 162.22115952435223,
   'min_child_weight': 1.156941735685324,
   'num_leaves': 31.294569230544013,
   'reg_alpha': 0.36916877863456377,
   'reg_lambda': 9.229532425566951,
   'subsample': 0.5688093468774877}},
 {'target': 0.7743690019971274,
  'params': {'colsample_bytree': 0.7394308873506115,
   'max_bin': 417.4989608386593,
   'max_depth': 6.128913120786649,
   'min_child_samples': 174.74416919268225,
   'min_child_weight': 31.59245109049789,
   'num_leaves': 28.05211885684618,
   'reg_alpha': 1.5180061549611963,
   'reg_lambda': 6.418396503524244,
   'subsample': 0.8382939851041469}},
 {'target': 0.7769920411479967,
  'params': {'colsample_bytree': 0.8929451865708424,
   'max_bin': 380.96408472669606,
   'max_depth': 12.24296875089054,
   'min_child_samples': 162.4681623947883,
   'min_child_weight': 1.124323100936236,
   'num_leaves': 62.237143754650376,
   'reg_alpha': 9.051348381820413,
   'reg_lambda': 8.55903813259641,
   'subsample': 0.5756763184847222}},
 {'target': 0.7754041675256128,
  'params': {'colsample_bytree': 0.8395378766641771,
   'max_bin': 369.45992318249404,
   'max_depth': 7.311653298584847,
   'min_child_samples': 179.448902083174,
   'min_child_weight': 17.38672526468209,
   'num_leaves': 58.6349743249024,
   'reg_alpha': 13.707733769542015,
   'reg_lambda': 4.132666603445228,
   'subsample': 0.8567763982467431}},
 {'target': 0.7780565770624691,
  'params': {'colsample_bytree': 0.596684514717646,
   'max_bin': 403.18943474881877,
   'max_depth': 12.907058961217114,
   'min_child_samples': 158.84286216603994,
   'min_child_weight': 7.338287048901049,
   'num_leaves': 58.11656509027165,
   'reg_alpha': 0.6681705096965275,
   'reg_lambda': 9.355145759543333,
   'subsample': 0.8010238277727275}},
 {'target': 0.7775081101221989,
  'params': {'colsample_bytree': 0.6983666820490788,
   'max_bin': 408.3845530220334,
   'max_depth': 15.750233779055964,
   'min_child_samples': 136.95205244907152,
   'min_child_weight': 1.6965708103918615,
   'num_leaves': 63.8506437238174,
   'reg_alpha': 11.603475539657392,
   'reg_lambda': 0.17633246697484675,
   'subsample': 0.8640107135672386}},
 {'target': 0.7771685255576835,
  'params': {'colsample_bytree': 0.9089270943590277,
   'max_bin': 407.4077569955801,
   'max_depth': 15.54111069785531,
   'min_child_samples': 180.7441063643676,
   'min_child_weight': 24.16966471165873,
   'num_leaves': 58.55774861930196,
   'reg_alpha': 5.617770487707293,
   'reg_lambda': 9.112524233329468,
   'subsample': 0.7879625370264551}},
 {'target': 0.7769187946775454,
  'params': {'colsample_bytree': 0.8291181953449183,
   'max_bin': 408.90814062859624,
   'max_depth': 10.799903523797198,
   'min_child_samples': 198.46670214232182,
   'min_child_weight': 10.393808863704972,
   'num_leaves': 54.79520213741949,
   'reg_alpha': 12.481921001697238,
   'reg_lambda': 6.564787886354399,
   'subsample': 0.580612232524619}},
 {'target': 0.7772230140569003,
  'params': {'colsample_bytree': 0.8242099119497381,
   'max_bin': 414.0195379290514,
   'max_depth': 12.838272481918226,
   'min_child_samples': 139.5861484641032,
   'min_child_weight': 23.638609111440616,
   'num_leaves': 62.52080831472535,
   'reg_alpha': 4.499765686038743,
   'reg_lambda': 1.0239192579133045,
   'subsample': 0.9178127659622564}},
 {'target': 0.777808998528086,
  'params': {'colsample_bytree': 0.8364260787813464,
   'max_bin': 438.5344980810344,
   'max_depth': 12.59368901950138,
   'min_child_samples': 126.31456889978566,
   'min_child_weight': 7.767143783785291,
   'num_leaves': 63.260425772040506,
   'reg_alpha': 0.32940336713078977,
   'reg_lambda': 9.23233466904062,
   'subsample': 0.602582344997781}},
 {'target': 0.7771167374184544,
  'params': {'colsample_bytree': 0.7062460990955068,
   'max_bin': 438.3043198858672,
   'max_depth': 13.162643244440723,
   'min_child_samples': 93.44971252277496,
   'min_child_weight': 3.1677333380244557,
   'num_leaves': 56.6248859454379,
   'reg_alpha': 4.488505501877722,
   'reg_lambda': 5.24181749541699,
   'subsample': 0.5061422738685454}},
 {'target': 0.7764292464153193,
  'params': {'colsample_bytree': 0.9458015786718881,
   'max_bin': 465.00800252186986,
   'max_depth': 7.899527803307271,
   'min_child_samples': 121.9201901620783,
   'min_child_weight': 2.5392614386800236,
   'num_leaves': 62.427676297468345,
   'reg_alpha': 4.784274264326323,
   'reg_lambda': 9.658699354226123,
   'subsample': 0.505629085733924}},
 {'target': 0.7763562620386317,
  'params': {'colsample_bytree': 0.900086638538324,
   'max_bin': 432.8385783874056,
   'max_depth': 12.593459218233793,
   'min_child_samples': 121.50703399935037,
   'min_child_weight': 4.114137212124302,
   'num_leaves': 38.12688146861727,
   'reg_alpha': 2.885555306126921,
   'reg_lambda': 7.88597353383862,
   'subsample': 0.9863664224720974}},
 {'target': 0.777091004576216,
  'params': {'colsample_bytree': 0.7395613472763644,
   'max_bin': 442.26433974553163,
   'max_depth': 9.243101646454075,
   'min_child_samples': 126.50175957072607,
   'min_child_weight': 31.84651381717126,
   'num_leaves': 63.82367461756897,
   'reg_alpha': 0.6593173679660734,
   'reg_lambda': 6.770945353052257,
   'subsample': 0.8921800827540989}}]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Iteration-결과-Dictionary에서-최대-target값을-가지는-index-추출하고-그때의-parameter-값을-추출.">
<a class="anchor" href="#Iteration-%EA%B2%B0%EA%B3%BC-Dictionary%EC%97%90%EC%84%9C-%EC%B5%9C%EB%8C%80-target%EA%B0%92%EC%9D%84-%EA%B0%80%EC%A7%80%EB%8A%94-index-%EC%B6%94%EC%B6%9C%ED%95%98%EA%B3%A0-%EA%B7%B8%EB%95%8C%EC%9D%98-parameter-%EA%B0%92%EC%9D%84-%EC%B6%94%EC%B6%9C." aria-hidden="true"><span class="octicon octicon-link"></span></a>Iteration 결과 Dictionary에서 최대 target값을 가지는 index 추출하고 그때의 parameter 값을 추출.<a class="anchor-link" href="#Iteration-%EA%B2%B0%EA%B3%BC-Dictionary%EC%97%90%EC%84%9C-%EC%B5%9C%EB%8C%80-target%EA%B0%92%EC%9D%84-%EA%B0%80%EC%A7%80%EB%8A%94-index-%EC%B6%94%EC%B6%9C%ED%95%98%EA%B3%A0-%EA%B7%B8%EB%95%8C%EC%9D%98-parameter-%EA%B0%92%EC%9D%84-%EC%B6%94%EC%B6%9C."> </a>
</h5>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">target_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">lgbBO</span><span class="o">.</span><span class="n">res</span><span class="p">:</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">'target'</span><span class="p">]</span>
    <span class="n">target_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_list</span><span class="p">)</span>
<span class="c1"># 가장 큰 target 값을 가지는 순번(index)를 추출</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'maximum target index:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target_list</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.7758055093230539, 0.7757909289675659, 0.7771779879367707, 0.7748333416046418, 0.774035094542375, 0.7773908017189786, 0.7755362516633085, 0.7772142029411041, 0.7763952615906466, 0.7734141467631326, 0.7756008426857747, 0.7771050305636831, 0.7772365047377109, 0.7773549981224318, 0.7767492867273098, 0.775929160395352, 0.7767777231064307, 0.7743690019971274, 0.7769920411479967, 0.7754041675256128, 0.7780565770624691, 0.7775081101221989, 0.7771685255576835, 0.7769187946775454, 0.7772230140569003, 0.777808998528086, 0.7771167374184544, 0.7764292464153193, 0.7763562620386317, 0.777091004576216]
maximum target index: 20
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_dict</span> <span class="o">=</span> <span class="n">lgbBO</span><span class="o">.</span><span class="n">res</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target_list</span><span class="p">))]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">max_dict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{'target': 0.7780565770624691, 'params': {'colsample_bytree': 0.596684514717646, 'max_bin': 403.18943474881877, 'max_depth': 12.907058961217114, 'min_child_samples': 158.84286216603994, 'min_child_weight': 7.338287048901049, 'num_leaves': 58.11656509027165, 'reg_alpha': 0.6681705096965275, 'reg_lambda': 9.355145759543333, 'subsample': 0.8010238277727275}}
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="최적화된-하이퍼-파라미터를-기반으로-재-테스트">
<a class="anchor" href="#%EC%B5%9C%EC%A0%81%ED%99%94%EB%90%9C-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%EB%A5%BC-%EA%B8%B0%EB%B0%98%EC%9C%BC%EB%A1%9C-%EC%9E%AC-%ED%85%8C%EC%8A%A4%ED%8A%B8" aria-hidden="true"><span class="octicon octicon-link"></span></a>최적화된 하이퍼 파라미터를 기반으로 재 테스트<a class="anchor-link" href="#%EC%B5%9C%EC%A0%81%ED%99%94%EB%90%9C-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%EB%A5%BC-%EA%B8%B0%EB%B0%98%EC%9C%BC%EB%A1%9C-%EC%9E%AC-%ED%85%8C%EC%8A%A4%ED%8A%B8"> </a>
</h1>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ftr_app</span> <span class="o">=</span> <span class="n">apps_all_train</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'SK_ID_CURR'</span><span class="p">,</span> <span class="s1">'TARGET'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">target_app</span> <span class="o">=</span> <span class="n">apps_all_train</span><span class="p">[</span><span class="s1">'TARGET'</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">ftr_app</span><span class="p">,</span> <span class="n">target_app</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2020</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'train shape:'</span><span class="p">,</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">'valid shape:'</span><span class="p">,</span> <span class="n">valid_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">lgbm_wrapper</span> <span class="o">=</span> <span class="n">LGBMClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">13</span><span class="p">,</span>
    <span class="n">num_leaves</span><span class="o">=</span><span class="mi">58</span><span class="p">,</span>
    <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.597</span><span class="p">,</span>
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.801</span><span class="p">,</span>
    <span class="n">max_bin</span><span class="o">=</span><span class="mi">403</span><span class="p">,</span>
    <span class="n">reg_alpha</span><span class="o">=</span><span class="mf">0.668</span><span class="p">,</span>
    <span class="n">reg_lambda</span><span class="o">=</span><span class="mf">9.355</span><span class="p">,</span>
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">min_child_samples</span><span class="o">=</span><span class="mi">159</span><span class="p">,</span>
    <span class="n">silent</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>



<span class="n">evals</span> <span class="o">=</span> <span class="p">[(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)]</span>
<span class="n">lgbm_wrapper</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">eval_metric</span><span class="o">=</span><span class="s2">"auc"</span><span class="p">,</span> <span class="n">eval_set</span><span class="o">=</span><span class="n">evals</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">lgbm_wrapper</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">pred_proba</span> <span class="o">=</span> <span class="n">lgbm_wrapper</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>train shape: (215257, 174) valid shape: (92254, 174)
Training until validation scores don't improve for 100 rounds
[100]	valid_0's auc: 0.759823	valid_0's binary_logloss: 0.24777
[200]	valid_0's auc: 0.769883	valid_0's binary_logloss: 0.242862
[300]	valid_0's auc: 0.773949	valid_0's binary_logloss: 0.241297
[400]	valid_0's auc: 0.776637	valid_0's binary_logloss: 0.240371
[500]	valid_0's auc: 0.77781	valid_0's binary_logloss: 0.239972
[600]	valid_0's auc: 0.778251	valid_0's binary_logloss: 0.239807
[700]	valid_0's auc: 0.778768	valid_0's binary_logloss: 0.239639
[800]	valid_0's auc: 0.778859	valid_0's binary_logloss: 0.239603
[900]	valid_0's auc: 0.779087	valid_0's binary_logloss: 0.239534
[1000]	valid_0's auc: 0.77916	valid_0's binary_logloss: 0.239504
Did not meet early stopping. Best iteration is:
[987]	valid_0's auc: 0.779182	valid_0's binary_logloss: 0.239501
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span> <span class="p">,</span> <span class="n">recall_score</span> <span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_recall_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span> 
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>

<span class="k">def</span> <span class="nf">get_clf_eval</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pred_proba</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">confusion</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span> <span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span> <span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span> <span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">pred</span><span class="p">)</span>
    <span class="c1"># ROC-AUC 추가 </span>
    <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_proba</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'오차 행렬'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">confusion</span><span class="p">)</span>
    <span class="c1"># ROC-AUC print 추가</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'정확도: </span><span class="si">{0:.4f}</span><span class="s1">, 정밀도: </span><span class="si">{1:.4f}</span><span class="s1">, 재현율: </span><span class="si">{2:.4f}</span><span class="s1">,</span><span class="se">\</span>
<span class="s1">          F1: </span><span class="si">{3:.4f}</span><span class="s1">, AUC:</span><span class="si">{4:.4f}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">roc_auc</span><span class="p">))</span>


<span class="n">get_clf_eval</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">pred_proba</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>오차 행렬
[[84653   180]
 [ 7201   220]]
정확도: 0.9200, 정밀도: 0.5500, 재현율: 0.0296,          F1: 0.0563, AUC:0.7792
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">roc_curve_plot</span><span class="p">(</span><span class="n">y_test</span> <span class="p">,</span> <span class="n">pred_proba_c1</span><span class="p">):</span>
    <span class="c1"># 임곗값에 따른 FPR, TPR 값을 반환 받음. </span>
    <span class="n">fprs</span> <span class="p">,</span> <span class="n">tprs</span> <span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span> <span class="p">,</span><span class="n">pred_proba_c1</span><span class="p">)</span>

    <span class="c1"># ROC Curve를 plot 곡선으로 그림. </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fprs</span> <span class="p">,</span> <span class="n">tprs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'ROC'</span><span class="p">)</span>
    <span class="c1"># 가운데 대각선 직선을 그림. </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">'k--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Random'</span><span class="p">)</span>
    
    <span class="c1"># FPR X 축의 Scale을 0.1 단위로 변경, X,Y 축명 설정등   </span>
    <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'FPR( 1 - Sensitivity )'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'TPR( Recall )'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">roc_curve_plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_proba</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_svg output_subarea ">
<?xml version="1.0" encoding="utf-8" standalone="no"?>
&lt;!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"&gt;
<!-- Created with matplotlib (https://matplotlib.org/) -->
<svg height="265.995469pt" version="1.1" viewbox="0 0 385.78125 265.995469" width="385.78125pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
 <metadata>
  <rdf xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <work>
    <type rdf:resource="http://purl.org/dc/dcmitype/StillImage"></type>
    <date>2021-09-01T09:46:50.009734</date>
    <format>image/svg+xml</format>
    <creator>
     <agent>
      <title>Matplotlib v3.3.4, https://matplotlib.org/</title>
     </agent>
    </creator>
   </work>
  </rdf>
 </metadata>
 <defs>
  <style type="text/css">*{stroke-linecap:butt;stroke-linejoin:round;}</style>
 </defs>
 <g id="figure_1">
  <g id="patch_1">
   <path d="M 0 265.995469 
L 385.78125 265.995469 
L 385.78125 0 
L 0 0 
z
" style="fill:none;"></path>
  </g>
  <g id="axes_1">
   <g id="patch_2">
    <path d="M 43.78125 228.439219 
L 378.58125 228.439219 
L 378.58125 10.999219 
L 43.78125 10.999219 
z
" style="fill:#ffffff;"></path>
   </g>
   <g id="matplotlib.axis_1">
    <g id="xtick_1">
     <g id="line2d_1">
      <defs>
       <path d="M 0 0 
L 0 3.5 
" id="mfab695a424" style="stroke:#000000;stroke-width:0.8;"></path>
      </defs>
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="60.52125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_1">
      <!-- 0.05 -->
      <g transform="translate(49.388438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 31.78125 66.40625 
Q 24.171875 66.40625 20.328125 58.90625 
Q 16.5 51.421875 16.5 36.375 
Q 16.5 21.390625 20.328125 13.890625 
Q 24.171875 6.390625 31.78125 6.390625 
Q 39.453125 6.390625 43.28125 13.890625 
Q 47.125 21.390625 47.125 36.375 
Q 47.125 51.421875 43.28125 58.90625 
Q 39.453125 66.40625 31.78125 66.40625 
z
M 31.78125 74.21875 
Q 44.046875 74.21875 50.515625 64.515625 
Q 56.984375 54.828125 56.984375 36.375 
Q 56.984375 17.96875 50.515625 8.265625 
Q 44.046875 -1.421875 31.78125 -1.421875 
Q 19.53125 -1.421875 13.0625 8.265625 
Q 6.59375 17.96875 6.59375 36.375 
Q 6.59375 54.828125 13.0625 64.515625 
Q 19.53125 74.21875 31.78125 74.21875 
z
" id="DejaVuSans-48"></path>
        <path d="M 10.6875 12.40625 
L 21 12.40625 
L 21 0 
L 10.6875 0 
z
" id="DejaVuSans-46"></path>
        <path d="M 10.796875 72.90625 
L 49.515625 72.90625 
L 49.515625 64.59375 
L 19.828125 64.59375 
L 19.828125 46.734375 
Q 21.96875 47.46875 24.109375 47.828125 
Q 26.265625 48.1875 28.421875 48.1875 
Q 40.625 48.1875 47.75 41.5 
Q 54.890625 34.8125 54.890625 23.390625 
Q 54.890625 11.625 47.5625 5.09375 
Q 40.234375 -1.421875 26.90625 -1.421875 
Q 22.3125 -1.421875 17.546875 -0.640625 
Q 12.796875 0.140625 7.71875 1.703125 
L 7.71875 11.625 
Q 12.109375 9.234375 16.796875 8.0625 
Q 21.484375 6.890625 26.703125 6.890625 
Q 35.15625 6.890625 40.078125 11.328125 
Q 45.015625 15.765625 45.015625 23.390625 
Q 45.015625 31 40.078125 35.4375 
Q 35.15625 39.890625 26.703125 39.890625 
Q 22.75 39.890625 18.8125 39.015625 
Q 14.890625 38.140625 10.796875 36.28125 
z
" id="DejaVuSans-53"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-48"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_2">
     <g id="line2d_2">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="94.00125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_2">
      <!-- 0.15 -->
      <g transform="translate(82.868437 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 12.40625 8.296875 
L 28.515625 8.296875 
L 28.515625 63.921875 
L 10.984375 60.40625 
L 10.984375 69.390625 
L 28.421875 72.90625 
L 38.28125 72.90625 
L 38.28125 8.296875 
L 54.390625 8.296875 
L 54.390625 0 
L 12.40625 0 
z
" id="DejaVuSans-49"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-49"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_3">
     <g id="line2d_3">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="127.48125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_3">
      <!-- 0.25 -->
      <g transform="translate(116.348438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 19.1875 8.296875 
L 53.609375 8.296875 
L 53.609375 0 
L 7.328125 0 
L 7.328125 8.296875 
Q 12.9375 14.109375 22.625 23.890625 
Q 32.328125 33.6875 34.8125 36.53125 
Q 39.546875 41.84375 41.421875 45.53125 
Q 43.3125 49.21875 43.3125 52.78125 
Q 43.3125 58.59375 39.234375 62.25 
Q 35.15625 65.921875 28.609375 65.921875 
Q 23.96875 65.921875 18.8125 64.3125 
Q 13.671875 62.703125 7.8125 59.421875 
L 7.8125 69.390625 
Q 13.765625 71.78125 18.9375 73 
Q 24.125 74.21875 28.421875 74.21875 
Q 39.75 74.21875 46.484375 68.546875 
Q 53.21875 62.890625 53.21875 53.421875 
Q 53.21875 48.921875 51.53125 44.890625 
Q 49.859375 40.875 45.40625 35.40625 
Q 44.1875 33.984375 37.640625 27.21875 
Q 31.109375 20.453125 19.1875 8.296875 
z
" id="DejaVuSans-50"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-50"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_4">
     <g id="line2d_4">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="160.96125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_4">
      <!-- 0.35 -->
      <g transform="translate(149.828438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 40.578125 39.3125 
Q 47.65625 37.796875 51.625 33 
Q 55.609375 28.21875 55.609375 21.1875 
Q 55.609375 10.40625 48.1875 4.484375 
Q 40.765625 -1.421875 27.09375 -1.421875 
Q 22.515625 -1.421875 17.65625 -0.515625 
Q 12.796875 0.390625 7.625 2.203125 
L 7.625 11.71875 
Q 11.71875 9.328125 16.59375 8.109375 
Q 21.484375 6.890625 26.8125 6.890625 
Q 36.078125 6.890625 40.9375 10.546875 
Q 45.796875 14.203125 45.796875 21.1875 
Q 45.796875 27.640625 41.28125 31.265625 
Q 36.765625 34.90625 28.71875 34.90625 
L 20.21875 34.90625 
L 20.21875 43.015625 
L 29.109375 43.015625 
Q 36.375 43.015625 40.234375 45.921875 
Q 44.09375 48.828125 44.09375 54.296875 
Q 44.09375 59.90625 40.109375 62.90625 
Q 36.140625 65.921875 28.71875 65.921875 
Q 24.65625 65.921875 20.015625 65.03125 
Q 15.375 64.15625 9.8125 62.3125 
L 9.8125 71.09375 
Q 15.4375 72.65625 20.34375 73.4375 
Q 25.25 74.21875 29.59375 74.21875 
Q 40.828125 74.21875 47.359375 69.109375 
Q 53.90625 64.015625 53.90625 55.328125 
Q 53.90625 49.265625 50.4375 45.09375 
Q 46.96875 40.921875 40.578125 39.3125 
z
" id="DejaVuSans-51"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-51"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_5">
     <g id="line2d_5">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="194.44125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_5">
      <!-- 0.45 -->
      <g transform="translate(183.308437 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 37.796875 64.3125 
L 12.890625 25.390625 
L 37.796875 25.390625 
z
M 35.203125 72.90625 
L 47.609375 72.90625 
L 47.609375 25.390625 
L 58.015625 25.390625 
L 58.015625 17.1875 
L 47.609375 17.1875 
L 47.609375 0 
L 37.796875 0 
L 37.796875 17.1875 
L 4.890625 17.1875 
L 4.890625 26.703125 
z
" id="DejaVuSans-52"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-52"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_6">
     <g id="line2d_6">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="227.92125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_6">
      <!-- 0.55 -->
      <g transform="translate(216.788438 243.037656)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-53"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_7">
     <g id="line2d_7">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="261.40125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_7">
      <!-- 0.65 -->
      <g transform="translate(250.268438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 33.015625 40.375 
Q 26.375 40.375 22.484375 35.828125 
Q 18.609375 31.296875 18.609375 23.390625 
Q 18.609375 15.53125 22.484375 10.953125 
Q 26.375 6.390625 33.015625 6.390625 
Q 39.65625 6.390625 43.53125 10.953125 
Q 47.40625 15.53125 47.40625 23.390625 
Q 47.40625 31.296875 43.53125 35.828125 
Q 39.65625 40.375 33.015625 40.375 
z
M 52.59375 71.296875 
L 52.59375 62.3125 
Q 48.875 64.0625 45.09375 64.984375 
Q 41.3125 65.921875 37.59375 65.921875 
Q 27.828125 65.921875 22.671875 59.328125 
Q 17.53125 52.734375 16.796875 39.40625 
Q 19.671875 43.65625 24.015625 45.921875 
Q 28.375 48.1875 33.59375 48.1875 
Q 44.578125 48.1875 50.953125 41.515625 
Q 57.328125 34.859375 57.328125 23.390625 
Q 57.328125 12.15625 50.6875 5.359375 
Q 44.046875 -1.421875 33.015625 -1.421875 
Q 20.359375 -1.421875 13.671875 8.265625 
Q 6.984375 17.96875 6.984375 36.375 
Q 6.984375 53.65625 15.1875 63.9375 
Q 23.390625 74.21875 37.203125 74.21875 
Q 40.921875 74.21875 44.703125 73.484375 
Q 48.484375 72.75 52.59375 71.296875 
z
" id="DejaVuSans-54"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-54"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_8">
     <g id="line2d_8">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="294.88125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_8">
      <!-- 0.75 -->
      <g transform="translate(283.748438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 8.203125 72.90625 
L 55.078125 72.90625 
L 55.078125 68.703125 
L 28.609375 0 
L 18.3125 0 
L 43.21875 64.59375 
L 8.203125 64.59375 
z
" id="DejaVuSans-55"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-55"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_9">
     <g id="line2d_9">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="328.36125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_9">
      <!-- 0.85 -->
      <g transform="translate(317.228437 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 31.78125 34.625 
Q 24.75 34.625 20.71875 30.859375 
Q 16.703125 27.09375 16.703125 20.515625 
Q 16.703125 13.921875 20.71875 10.15625 
Q 24.75 6.390625 31.78125 6.390625 
Q 38.8125 6.390625 42.859375 10.171875 
Q 46.921875 13.96875 46.921875 20.515625 
Q 46.921875 27.09375 42.890625 30.859375 
Q 38.875 34.625 31.78125 34.625 
z
M 21.921875 38.8125 
Q 15.578125 40.375 12.03125 44.71875 
Q 8.5 49.078125 8.5 55.328125 
Q 8.5 64.0625 14.71875 69.140625 
Q 20.953125 74.21875 31.78125 74.21875 
Q 42.671875 74.21875 48.875 69.140625 
Q 55.078125 64.0625 55.078125 55.328125 
Q 55.078125 49.078125 51.53125 44.71875 
Q 48 40.375 41.703125 38.8125 
Q 48.828125 37.15625 52.796875 32.3125 
Q 56.78125 27.484375 56.78125 20.515625 
Q 56.78125 9.90625 50.3125 4.234375 
Q 43.84375 -1.421875 31.78125 -1.421875 
Q 19.734375 -1.421875 13.25 4.234375 
Q 6.78125 9.90625 6.78125 20.515625 
Q 6.78125 27.484375 10.78125 32.3125 
Q 14.796875 37.15625 21.921875 38.8125 
z
M 18.3125 54.390625 
Q 18.3125 48.734375 21.84375 45.5625 
Q 25.390625 42.390625 31.78125 42.390625 
Q 38.140625 42.390625 41.71875 45.5625 
Q 45.3125 48.734375 45.3125 54.390625 
Q 45.3125 60.0625 41.71875 63.234375 
Q 38.140625 66.40625 31.78125 66.40625 
Q 25.390625 66.40625 21.84375 63.234375 
Q 18.3125 60.0625 18.3125 54.390625 
z
" id="DejaVuSans-56"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-56"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_10">
     <g id="line2d_10">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="361.84125" xlink:href="#mfab695a424" y="228.439219"></use>
      </g>
     </g>
     <g id="text_10">
      <!-- 0.95 -->
      <g transform="translate(350.708438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 10.984375 1.515625 
L 10.984375 10.5 
Q 14.703125 8.734375 18.5 7.8125 
Q 22.3125 6.890625 25.984375 6.890625 
Q 35.75 6.890625 40.890625 13.453125 
Q 46.046875 20.015625 46.78125 33.40625 
Q 43.953125 29.203125 39.59375 26.953125 
Q 35.25 24.703125 29.984375 24.703125 
Q 19.046875 24.703125 12.671875 31.3125 
Q 6.296875 37.9375 6.296875 49.421875 
Q 6.296875 60.640625 12.9375 67.421875 
Q 19.578125 74.21875 30.609375 74.21875 
Q 43.265625 74.21875 49.921875 64.515625 
Q 56.59375 54.828125 56.59375 36.375 
Q 56.59375 19.140625 48.40625 8.859375 
Q 40.234375 -1.421875 26.421875 -1.421875 
Q 22.703125 -1.421875 18.890625 -0.6875 
Q 15.09375 0.046875 10.984375 1.515625 
z
M 30.609375 32.421875 
Q 37.25 32.421875 41.125 36.953125 
Q 45.015625 41.5 45.015625 49.421875 
Q 45.015625 57.28125 41.125 61.84375 
Q 37.25 66.40625 30.609375 66.40625 
Q 23.96875 66.40625 20.09375 61.84375 
Q 16.21875 57.28125 16.21875 49.421875 
Q 16.21875 41.5 20.09375 36.953125 
Q 23.96875 32.421875 30.609375 32.421875 
z
" id="DejaVuSans-57"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-57"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="text_11">
     <!-- FPR( 1 - Sensitivity ) -->
     <g transform="translate(160.542969 256.715781)scale(0.1 -0.1)">
      <defs>
       <path d="M 9.8125 72.90625 
L 51.703125 72.90625 
L 51.703125 64.59375 
L 19.671875 64.59375 
L 19.671875 43.109375 
L 48.578125 43.109375 
L 48.578125 34.8125 
L 19.671875 34.8125 
L 19.671875 0 
L 9.8125 0 
z
" id="DejaVuSans-70"></path>
       <path d="M 19.671875 64.796875 
L 19.671875 37.40625 
L 32.078125 37.40625 
Q 38.96875 37.40625 42.71875 40.96875 
Q 46.484375 44.53125 46.484375 51.125 
Q 46.484375 57.671875 42.71875 61.234375 
Q 38.96875 64.796875 32.078125 64.796875 
z
M 9.8125 72.90625 
L 32.078125 72.90625 
Q 44.34375 72.90625 50.609375 67.359375 
Q 56.890625 61.8125 56.890625 51.125 
Q 56.890625 40.328125 50.609375 34.8125 
Q 44.34375 29.296875 32.078125 29.296875 
L 19.671875 29.296875 
L 19.671875 0 
L 9.8125 0 
z
" id="DejaVuSans-80"></path>
       <path d="M 44.390625 34.1875 
Q 47.5625 33.109375 50.5625 29.59375 
Q 53.5625 26.078125 56.59375 19.921875 
L 66.609375 0 
L 56 0 
L 46.6875 18.703125 
Q 43.0625 26.03125 39.671875 28.421875 
Q 36.28125 30.8125 30.421875 30.8125 
L 19.671875 30.8125 
L 19.671875 0 
L 9.8125 0 
L 9.8125 72.90625 
L 32.078125 72.90625 
Q 44.578125 72.90625 50.734375 67.671875 
Q 56.890625 62.453125 56.890625 51.90625 
Q 56.890625 45.015625 53.6875 40.46875 
Q 50.484375 35.9375 44.390625 34.1875 
z
M 19.671875 64.796875 
L 19.671875 38.921875 
L 32.078125 38.921875 
Q 39.203125 38.921875 42.84375 42.21875 
Q 46.484375 45.515625 46.484375 51.90625 
Q 46.484375 58.296875 42.84375 61.546875 
Q 39.203125 64.796875 32.078125 64.796875 
z
" id="DejaVuSans-82"></path>
       <path d="M 31 75.875 
Q 24.46875 64.65625 21.28125 53.65625 
Q 18.109375 42.671875 18.109375 31.390625 
Q 18.109375 20.125 21.3125 9.0625 
Q 24.515625 -2 31 -13.1875 
L 23.1875 -13.1875 
Q 15.875 -1.703125 12.234375 9.375 
Q 8.59375 20.453125 8.59375 31.390625 
Q 8.59375 42.28125 12.203125 53.3125 
Q 15.828125 64.359375 23.1875 75.875 
z
" id="DejaVuSans-40"></path>
       <path id="DejaVuSans-32"></path>
       <path d="M 4.890625 31.390625 
L 31.203125 31.390625 
L 31.203125 23.390625 
L 4.890625 23.390625 
z
" id="DejaVuSans-45"></path>
       <path d="M 53.515625 70.515625 
L 53.515625 60.890625 
Q 47.90625 63.578125 42.921875 64.890625 
Q 37.9375 66.21875 33.296875 66.21875 
Q 25.25 66.21875 20.875 63.09375 
Q 16.5 59.96875 16.5 54.203125 
Q 16.5 49.359375 19.40625 46.890625 
Q 22.3125 44.4375 30.421875 42.921875 
L 36.375 41.703125 
Q 47.40625 39.59375 52.65625 34.296875 
Q 57.90625 29 57.90625 20.125 
Q 57.90625 9.515625 50.796875 4.046875 
Q 43.703125 -1.421875 29.984375 -1.421875 
Q 24.8125 -1.421875 18.96875 -0.25 
Q 13.140625 0.921875 6.890625 3.21875 
L 6.890625 13.375 
Q 12.890625 10.015625 18.65625 8.296875 
Q 24.421875 6.59375 29.984375 6.59375 
Q 38.421875 6.59375 43.015625 9.90625 
Q 47.609375 13.234375 47.609375 19.390625 
Q 47.609375 24.75 44.3125 27.78125 
Q 41.015625 30.8125 33.5 32.328125 
L 27.484375 33.5 
Q 16.453125 35.6875 11.515625 40.375 
Q 6.59375 45.0625 6.59375 53.421875 
Q 6.59375 63.09375 13.40625 68.65625 
Q 20.21875 74.21875 32.171875 74.21875 
Q 37.3125 74.21875 42.625 73.28125 
Q 47.953125 72.359375 53.515625 70.515625 
z
" id="DejaVuSans-83"></path>
       <path d="M 56.203125 29.59375 
L 56.203125 25.203125 
L 14.890625 25.203125 
Q 15.484375 15.921875 20.484375 11.0625 
Q 25.484375 6.203125 34.421875 6.203125 
Q 39.59375 6.203125 44.453125 7.46875 
Q 49.3125 8.734375 54.109375 11.28125 
L 54.109375 2.78125 
Q 49.265625 0.734375 44.1875 -0.34375 
Q 39.109375 -1.421875 33.890625 -1.421875 
Q 20.796875 -1.421875 13.15625 6.1875 
Q 5.515625 13.8125 5.515625 26.8125 
Q 5.515625 40.234375 12.765625 48.109375 
Q 20.015625 56 32.328125 56 
Q 43.359375 56 49.78125 48.890625 
Q 56.203125 41.796875 56.203125 29.59375 
z
M 47.21875 32.234375 
Q 47.125 39.59375 43.09375 43.984375 
Q 39.0625 48.390625 32.421875 48.390625 
Q 24.90625 48.390625 20.390625 44.140625 
Q 15.875 39.890625 15.1875 32.171875 
z
" id="DejaVuSans-101"></path>
       <path d="M 54.890625 33.015625 
L 54.890625 0 
L 45.90625 0 
L 45.90625 32.71875 
Q 45.90625 40.484375 42.875 44.328125 
Q 39.84375 48.1875 33.796875 48.1875 
Q 26.515625 48.1875 22.3125 43.546875 
Q 18.109375 38.921875 18.109375 30.90625 
L 18.109375 0 
L 9.078125 0 
L 9.078125 54.6875 
L 18.109375 54.6875 
L 18.109375 46.1875 
Q 21.34375 51.125 25.703125 53.5625 
Q 30.078125 56 35.796875 56 
Q 45.21875 56 50.046875 50.171875 
Q 54.890625 44.34375 54.890625 33.015625 
z
" id="DejaVuSans-110"></path>
       <path d="M 44.28125 53.078125 
L 44.28125 44.578125 
Q 40.484375 46.53125 36.375 47.5 
Q 32.28125 48.484375 27.875 48.484375 
Q 21.1875 48.484375 17.84375 46.4375 
Q 14.5 44.390625 14.5 40.28125 
Q 14.5 37.15625 16.890625 35.375 
Q 19.28125 33.59375 26.515625 31.984375 
L 29.59375 31.296875 
Q 39.15625 29.25 43.1875 25.515625 
Q 47.21875 21.78125 47.21875 15.09375 
Q 47.21875 7.46875 41.1875 3.015625 
Q 35.15625 -1.421875 24.609375 -1.421875 
Q 20.21875 -1.421875 15.453125 -0.5625 
Q 10.6875 0.296875 5.421875 2 
L 5.421875 11.28125 
Q 10.40625 8.6875 15.234375 7.390625 
Q 20.0625 6.109375 24.8125 6.109375 
Q 31.15625 6.109375 34.5625 8.28125 
Q 37.984375 10.453125 37.984375 14.40625 
Q 37.984375 18.0625 35.515625 20.015625 
Q 33.0625 21.96875 24.703125 23.78125 
L 21.578125 24.515625 
Q 13.234375 26.265625 9.515625 29.90625 
Q 5.8125 33.546875 5.8125 39.890625 
Q 5.8125 47.609375 11.28125 51.796875 
Q 16.75 56 26.8125 56 
Q 31.78125 56 36.171875 55.265625 
Q 40.578125 54.546875 44.28125 53.078125 
z
" id="DejaVuSans-115"></path>
       <path d="M 9.421875 54.6875 
L 18.40625 54.6875 
L 18.40625 0 
L 9.421875 0 
z
M 9.421875 75.984375 
L 18.40625 75.984375 
L 18.40625 64.59375 
L 9.421875 64.59375 
z
" id="DejaVuSans-105"></path>
       <path d="M 18.3125 70.21875 
L 18.3125 54.6875 
L 36.8125 54.6875 
L 36.8125 47.703125 
L 18.3125 47.703125 
L 18.3125 18.015625 
Q 18.3125 11.328125 20.140625 9.421875 
Q 21.96875 7.515625 27.59375 7.515625 
L 36.8125 7.515625 
L 36.8125 0 
L 27.59375 0 
Q 17.1875 0 13.234375 3.875 
Q 9.28125 7.765625 9.28125 18.015625 
L 9.28125 47.703125 
L 2.6875 47.703125 
L 2.6875 54.6875 
L 9.28125 54.6875 
L 9.28125 70.21875 
z
" id="DejaVuSans-116"></path>
       <path d="M 2.984375 54.6875 
L 12.5 54.6875 
L 29.59375 8.796875 
L 46.6875 54.6875 
L 56.203125 54.6875 
L 35.6875 0 
L 23.484375 0 
z
" id="DejaVuSans-118"></path>
       <path d="M 32.171875 -5.078125 
Q 28.375 -14.84375 24.75 -17.8125 
Q 21.140625 -20.796875 15.09375 -20.796875 
L 7.90625 -20.796875 
L 7.90625 -13.28125 
L 13.1875 -13.28125 
Q 16.890625 -13.28125 18.9375 -11.515625 
Q 21 -9.765625 23.484375 -3.21875 
L 25.09375 0.875 
L 2.984375 54.6875 
L 12.5 54.6875 
L 29.59375 11.921875 
L 46.6875 54.6875 
L 56.203125 54.6875 
z
" id="DejaVuSans-121"></path>
       <path d="M 8.015625 75.875 
L 15.828125 75.875 
Q 23.140625 64.359375 26.78125 53.3125 
Q 30.421875 42.28125 30.421875 31.390625 
Q 30.421875 20.453125 26.78125 9.375 
Q 23.140625 -1.703125 15.828125 -13.1875 
L 8.015625 -13.1875 
Q 14.5 -2 17.703125 9.0625 
Q 20.90625 20.125 20.90625 31.390625 
Q 20.90625 42.671875 17.703125 53.65625 
Q 14.5 64.65625 8.015625 75.875 
z
" id="DejaVuSans-41"></path>
      </defs>
      <use xlink:href="#DejaVuSans-70"></use>
      <use x="57.519531" xlink:href="#DejaVuSans-80"></use>
      <use x="117.822266" xlink:href="#DejaVuSans-82"></use>
      <use x="187.304688" xlink:href="#DejaVuSans-40"></use>
      <use x="226.318359" xlink:href="#DejaVuSans-32"></use>
      <use x="258.105469" xlink:href="#DejaVuSans-49"></use>
      <use x="321.728516" xlink:href="#DejaVuSans-32"></use>
      <use x="353.515625" xlink:href="#DejaVuSans-45"></use>
      <use x="389.599609" xlink:href="#DejaVuSans-32"></use>
      <use x="421.386719" xlink:href="#DejaVuSans-83"></use>
      <use x="484.863281" xlink:href="#DejaVuSans-101"></use>
      <use x="546.386719" xlink:href="#DejaVuSans-110"></use>
      <use x="609.765625" xlink:href="#DejaVuSans-115"></use>
      <use x="661.865234" xlink:href="#DejaVuSans-105"></use>
      <use x="689.648438" xlink:href="#DejaVuSans-116"></use>
      <use x="728.857422" xlink:href="#DejaVuSans-105"></use>
      <use x="756.640625" xlink:href="#DejaVuSans-118"></use>
      <use x="815.820312" xlink:href="#DejaVuSans-105"></use>
      <use x="843.603516" xlink:href="#DejaVuSans-116"></use>
      <use x="882.8125" xlink:href="#DejaVuSans-121"></use>
      <use x="941.992188" xlink:href="#DejaVuSans-32"></use>
      <use x="973.779297" xlink:href="#DejaVuSans-41"></use>
     </g>
    </g>
   </g>
   <g id="matplotlib.axis_2">
    <g id="ytick_1">
     <g id="line2d_11">
      <defs>
       <path d="M 0 0 
L -3.5 0 
" id="m057f77c40a" style="stroke:#000000;stroke-width:0.8;"></path>
      </defs>
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m057f77c40a" y="228.439219"></use>
      </g>
     </g>
     <g id="text_12">
      <!-- 0.0 -->
      <g transform="translate(20.878125 232.238437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-48"></use>
      </g>
     </g>
    </g>
    <g id="ytick_2">
     <g id="line2d_12">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m057f77c40a" y="184.951219"></use>
      </g>
     </g>
     <g id="text_13">
      <!-- 0.2 -->
      <g transform="translate(20.878125 188.750437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-50"></use>
      </g>
     </g>
    </g>
    <g id="ytick_3">
     <g id="line2d_13">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m057f77c40a" y="141.463219"></use>
      </g>
     </g>
     <g id="text_14">
      <!-- 0.4 -->
      <g transform="translate(20.878125 145.262437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-52"></use>
      </g>
     </g>
    </g>
    <g id="ytick_4">
     <g id="line2d_14">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m057f77c40a" y="97.975219"></use>
      </g>
     </g>
     <g id="text_15">
      <!-- 0.6 -->
      <g transform="translate(20.878125 101.774437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-54"></use>
      </g>
     </g>
    </g>
    <g id="ytick_5">
     <g id="line2d_15">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m057f77c40a" y="54.487219"></use>
      </g>
     </g>
     <g id="text_16">
      <!-- 0.8 -->
      <g transform="translate(20.878125 58.286437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-56"></use>
      </g>
     </g>
    </g>
    <g id="ytick_6">
     <g id="line2d_16">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m057f77c40a" y="10.999219"></use>
      </g>
     </g>
     <g id="text_17">
      <!-- 1.0 -->
      <g transform="translate(20.878125 14.798437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-49"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-48"></use>
      </g>
     </g>
    </g>
    <g id="text_18">
     <!-- TPR( Recall ) -->
     <g transform="translate(14.798438 151.259062)rotate(-90)scale(0.1 -0.1)">
      <defs>
       <path d="M -0.296875 72.90625 
L 61.375 72.90625 
L 61.375 64.59375 
L 35.5 64.59375 
L 35.5 0 
L 25.59375 0 
L 25.59375 64.59375 
L -0.296875 64.59375 
z
" id="DejaVuSans-84"></path>
       <path d="M 48.78125 52.59375 
L 48.78125 44.1875 
Q 44.96875 46.296875 41.140625 47.34375 
Q 37.3125 48.390625 33.40625 48.390625 
Q 24.65625 48.390625 19.8125 42.84375 
Q 14.984375 37.3125 14.984375 27.296875 
Q 14.984375 17.28125 19.8125 11.734375 
Q 24.65625 6.203125 33.40625 6.203125 
Q 37.3125 6.203125 41.140625 7.25 
Q 44.96875 8.296875 48.78125 10.40625 
L 48.78125 2.09375 
Q 45.015625 0.34375 40.984375 -0.53125 
Q 36.96875 -1.421875 32.421875 -1.421875 
Q 20.0625 -1.421875 12.78125 6.34375 
Q 5.515625 14.109375 5.515625 27.296875 
Q 5.515625 40.671875 12.859375 48.328125 
Q 20.21875 56 33.015625 56 
Q 37.15625 56 41.109375 55.140625 
Q 45.0625 54.296875 48.78125 52.59375 
z
" id="DejaVuSans-99"></path>
       <path d="M 34.28125 27.484375 
Q 23.390625 27.484375 19.1875 25 
Q 14.984375 22.515625 14.984375 16.5 
Q 14.984375 11.71875 18.140625 8.90625 
Q 21.296875 6.109375 26.703125 6.109375 
Q 34.1875 6.109375 38.703125 11.40625 
Q 43.21875 16.703125 43.21875 25.484375 
L 43.21875 27.484375 
z
M 52.203125 31.203125 
L 52.203125 0 
L 43.21875 0 
L 43.21875 8.296875 
Q 40.140625 3.328125 35.546875 0.953125 
Q 30.953125 -1.421875 24.3125 -1.421875 
Q 15.921875 -1.421875 10.953125 3.296875 
Q 6 8.015625 6 15.921875 
Q 6 25.140625 12.171875 29.828125 
Q 18.359375 34.515625 30.609375 34.515625 
L 43.21875 34.515625 
L 43.21875 35.40625 
Q 43.21875 41.609375 39.140625 45 
Q 35.0625 48.390625 27.6875 48.390625 
Q 23 48.390625 18.546875 47.265625 
Q 14.109375 46.140625 10.015625 43.890625 
L 10.015625 52.203125 
Q 14.9375 54.109375 19.578125 55.046875 
Q 24.21875 56 28.609375 56 
Q 40.484375 56 46.34375 49.84375 
Q 52.203125 43.703125 52.203125 31.203125 
z
" id="DejaVuSans-97"></path>
       <path d="M 9.421875 75.984375 
L 18.40625 75.984375 
L 18.40625 0 
L 9.421875 0 
z
" id="DejaVuSans-108"></path>
      </defs>
      <use xlink:href="#DejaVuSans-84"></use>
      <use x="61.083984" xlink:href="#DejaVuSans-80"></use>
      <use x="121.386719" xlink:href="#DejaVuSans-82"></use>
      <use x="190.869141" xlink:href="#DejaVuSans-40"></use>
      <use x="229.882812" xlink:href="#DejaVuSans-32"></use>
      <use x="261.669922" xlink:href="#DejaVuSans-82"></use>
      <use x="326.652344" xlink:href="#DejaVuSans-101"></use>
      <use x="388.175781" xlink:href="#DejaVuSans-99"></use>
      <use x="443.15625" xlink:href="#DejaVuSans-97"></use>
      <use x="504.435547" xlink:href="#DejaVuSans-108"></use>
      <use x="532.21875" xlink:href="#DejaVuSans-108"></use>
      <use x="560.001953" xlink:href="#DejaVuSans-32"></use>
      <use x="591.789062" xlink:href="#DejaVuSans-41"></use>
     </g>
    </g>
   </g>
   <g id="line2d_17">
    <path clip-path="url(#p0fe17390f5)" d="M 43.78125 228.439219 
L 43.891754 227.003488 
L 43.899647 227.003488 
L 44.010151 226.124469 
L 44.014098 226.124469 
L 44.124602 225.27475 
L 44.144335 225.186848 
L 44.250893 224.161326 
L 44.274572 224.073424 
L 44.365343 223.047902 
L 44.404809 222.96 
L 44.511367 221.905178 
L 44.523207 221.817276 
L 44.613978 221.11406 
L 44.665283 221.055459 
L 44.763948 220.469446 
L 44.787627 220.381545 
L 44.894185 219.678329 
L 44.921811 219.590427 
L 45.032315 219.092317 
L 45.040208 219.004415 
L 45.150712 218.271899 
L 45.182285 218.183997 
L 45.292789 217.510083 
L 45.308575 217.451481 
L 45.41908 217.070573 
L 45.438813 216.982671 
L 45.54537 216.425959 
L 45.580889 216.338057 
L 45.687447 215.605541 
L 45.722966 215.517639 
L 45.83347 214.667921 
L 45.85715 214.580019 
L 45.959761 213.935405 
L 45.9716 213.935405 
L 46.078158 213.261491 
L 46.113677 213.173589 
L 46.220235 212.353171 
L 46.255754 212.265269 
L 46.366258 211.620655 
L 46.42151 211.532753 
L 46.532014 211.151845 
L 46.535961 211.151845 
L 46.638572 210.360728 
L 46.658305 210.272826 
L 46.741183 210.00912 
L 46.772756 210.00912 
L 46.88326 209.218003 
L 46.946405 209.130101 
L 47.056909 208.719893 
L 47.076642 208.631991 
L 47.187146 208.338984 
L 47.206879 208.251082 
L 47.305544 207.723671 
L 47.380529 207.635769 
L 47.491033 207.137658 
L 47.526552 207.049756 
L 47.637056 206.493044 
L 47.672575 206.405142 
L 47.779133 205.877731 
L 47.830438 205.789829 
L 47.905423 204.969411 
L 47.992248 204.88151 
L 48.098806 204.324798 
L 48.173791 204.236896 
L 48.280348 203.562981 
L 48.304028 203.475079 
L 48.414532 203.123472 
L 48.434265 203.03557 
L 48.544769 202.625361 
L 48.600021 202.56676 
L 48.702632 202.156551 
L 48.757884 202.068649 
L 48.868388 201.453335 
L 48.872335 201.453335 
L 48.982839 200.867323 
L 49.002572 200.779421 
L 49.093343 200.369212 
L 49.148595 200.28131 
L 49.231473 200.017604 
L 49.263046 200.017604 
L 49.37355 199.578095 
L 49.393283 199.490193 
L 49.499841 198.87488 
L 49.53536 198.786978 
L 49.641917 198.259566 
L 49.68533 198.171665 
L 49.795834 197.790756 
L 49.807674 197.702854 
L 49.886605 197.439149 
L 49.94975 197.380547 
L 50.052361 196.765234 
L 50.087881 196.677332 
L 50.182599 196.355025 
L 50.218118 196.296424 
L 50.328622 196.003418 
L 50.356248 195.915516 
L 50.454912 195.446706 
L 50.490432 195.358804 
L 50.53779 195.183 
L 50.640402 195.095098 
L 50.746959 194.479785 
L 50.794318 194.391883 
L 50.87325 194.128177 
L 50.956128 194.040275 
L 51.015326 193.747269 
L 51.102151 193.659367 
L 51.204762 193.190557 
L 51.236335 193.102655 
L 51.342892 192.663145 
L 51.382358 192.575244 
L 51.492862 192.165035 
L 51.544168 192.077133 
L 51.642832 191.666924 
L 51.694138 191.579022 
L 51.792802 191.315316 
L 51.863841 191.256715 
L 51.974345 190.846506 
L 51.990131 190.817206 
L 52.088795 190.172592 
L 52.16378 190.08469 
L 52.274285 189.64518 
L 52.341376 189.557278 
L 52.447934 189.205671 
L 52.47556 189.147069 
L 52.586064 188.531756 
L 52.664996 188.443854 
L 52.767607 188.297351 
L 52.803126 188.209449 
L 52.909684 187.682038 
L 52.968882 187.594136 
L 53.059653 187.008123 
L 53.142532 186.920221 
L 53.21357 186.685816 
L 53.268822 186.656516 
L 53.371433 186.158405 
L 53.414845 186.099804 
L 53.481937 185.777497 
L 53.564815 185.689595 
L 53.671373 185.044981 
L 53.746358 184.957079 
L 53.856862 184.576171 
L 53.904221 184.517569 
L 54.002885 184.07806 
L 54.07787 183.990158 
L 54.188375 183.667851 
L 54.231787 183.579949 
L 54.318612 183.286943 
L 54.417276 183.228342 
L 54.523834 182.730231 
L 54.555406 182.642329 
L 54.658017 182.349323 
L 54.729056 182.261421 
L 54.81588 181.880512 
L 54.894812 181.79261 
L 55.005316 181.587506 
L 55.028996 181.499604 
L 55.107927 181.2945 
L 55.230271 181.206598 
L 55.332882 180.972193 
L 55.40392 180.913591 
L 55.510478 180.620585 
L 55.569677 180.532683 
L 55.680181 180.151775 
L 55.699914 180.063873 
L 55.806471 179.800167 
L 55.826204 179.741566 
L 55.932762 179.44856 
L 55.99196 179.360658 
L 56.078785 179.184854 
L 56.126144 179.126253 
L 56.236648 178.716044 
L 56.244541 178.686743 
L 56.351099 178.042129 
L 56.418191 177.954228 
L 56.485283 177.749123 
L 56.583947 177.690522 
L 56.694451 177.251012 
L 56.72997 177.192411 
L 56.816795 176.899405 
L 56.911513 176.870104 
L 57.022017 176.459895 
L 57.06543 176.401294 
L 57.175934 176.108288 
L 57.223293 176.020386 
L 57.333797 175.668778 
L 57.385102 175.580876 
L 57.487713 175.258569 
L 57.546912 175.170667 
L 57.602164 175.024164 
L 57.720561 174.936262 
L 57.823172 174.613955 
L 57.909997 174.526053 
L 57.953409 174.35025 
L 58.123112 174.262348 
L 58.22967 173.998642 
L 58.284922 173.91074 
L 58.383586 173.734936 
L 58.458571 173.647034 
L 58.569075 173.119623 
L 58.612488 173.031721 
L 58.722992 172.738715 
L 58.750618 172.650813 
L 58.857176 172.269905 
L 58.892695 172.182003 
L 58.983466 171.947598 
L 59.054505 171.859696 
L 59.145276 171.654591 
L 59.263673 171.566689 
L 59.342605 171.332284 
L 59.41759 171.244382 
L 59.520201 170.804873 
L 59.571506 170.746272 
L 59.68201 170.21886 
L 59.776728 170.130958 
L 59.867499 169.72075 
L 59.938538 169.632848 
L 60.049042 169.369142 
L 60.100348 169.28124 
L 60.210852 168.929632 
L 60.234531 168.871031 
L 60.333196 168.519424 
L 60.384501 168.490123 
L 60.479219 168.138515 
L 60.554204 168.050613 
L 60.660762 167.728306 
L 60.696281 167.640405 
L 60.779159 167.288797 
L 60.917289 167.200895 
L 61.015953 166.732085 
L 61.098832 166.644183 
L 61.193549 166.468379 
L 61.236962 166.380477 
L 61.347466 165.970268 
L 61.394825 165.911667 
L 61.493489 165.677262 
L 61.568474 165.58936 
L 61.643459 165.501458 
L 61.694765 165.442857 
L 61.801322 165.091249 
L 61.828948 165.003348 
L 61.931559 164.475936 
L 61.955239 164.388034 
L 62.030224 164.095028 
L 62.172301 164.007126 
L 62.243339 163.772721 
L 62.373576 163.684819 
L 62.440668 163.479715 
L 62.551172 163.391813 
L 62.64589 163.040205 
L 62.689302 162.952303 
L 62.744554 162.747199 
L 62.874791 162.659297 
L 62.969509 162.33699 
L 63.0958 162.249088 
L 63.190518 162.043984 
L 63.285235 161.956082 
L 63.39574 161.721677 
L 63.466778 161.633775 
L 63.529923 161.457971 
L 63.644374 161.370069 
L 63.727252 161.135664 
L 63.818023 161.047762 
L 63.916688 160.784056 
L 63.964047 160.725455 
L 64.066658 160.49105 
L 64.169269 160.403148 
L 64.267933 160.256645 
L 64.370544 160.168743 
L 64.457369 159.817135 
L 64.579713 159.729234 
L 64.690217 159.406927 
L 64.702057 159.348325 
L 64.804668 159.055319 
L 64.923065 158.967417 
L 65.025676 158.64511 
L 65.061195 158.557208 
L 65.14802 158.205601 
L 65.230898 158.117699 
L 65.270364 158.029797 
L 65.372975 157.941895 
L 65.4598 157.619588 
L 65.538731 157.531686 
L 65.593983 157.385183 
L 65.692648 157.297281 
L 65.779472 157.150778 
L 65.862351 157.121477 
L 65.972855 156.740569 
L 66.04784 156.652667 
L 66.142558 156.506164 
L 66.241222 156.418262 
L 66.343833 156.271759 
L 66.399085 156.183857 
L 66.489856 155.920151 
L 66.572734 155.832249 
L 66.639826 155.656446 
L 66.734544 155.568544 
L 66.833209 155.246237 
L 66.959499 155.158335 
L 67.06211 154.982531 
L 67.144988 154.92393 
L 67.247599 154.396518 
L 67.318638 154.308616 
L 67.417302 154.103512 
L 67.52386 154.01561 
L 67.622524 153.927708 
L 67.689616 153.869107 
L 67.80012 153.634702 
L 67.839586 153.576101 
L 67.918517 153.341696 
L 68.08822 153.253794 
L 68.175045 153.165892 
L 68.230297 153.10729 
L 68.321068 152.726382 
L 68.356587 152.63848 
L 68.451305 152.316173 
L 68.534183 152.228271 
L 68.605222 151.993866 
L 68.684153 151.905964 
L 68.778871 151.671559 
L 68.917001 151.583658 
L 68.9762 151.378553 
L 69.066971 151.290651 
L 69.173529 151.056246 
L 69.244567 150.968344 
L 69.355072 150.675338 
L 69.398484 150.587436 
L 69.465576 150.528835 
L 69.560294 150.440933 
L 69.643172 150.118626 
L 69.71421 150.030724 
L 69.808928 149.85492 
L 69.899699 149.767018 
L 69.958898 149.561914 
L 70.053616 149.474012 
L 70.144387 149.268907 
L 70.29041 149.181006 
L 70.365395 149.005202 
L 70.424594 148.9173 
L 70.515365 148.770797 
L 70.606137 148.682895 
L 70.704801 148.47779 
L 70.807412 148.389888 
L 70.874504 148.155483 
L 71.087619 148.067582 
L 71.17839 147.774575 
L 71.312574 147.686673 
L 71.387559 147.481569 
L 71.482277 147.393667 
L 71.588834 147.100661 
L 71.624354 147.012759 
L 71.726965 146.749053 
L 71.798003 146.690452 
L 71.904561 146.397445 
L 71.94008 146.338844 
L 72.030851 145.987237 
L 72.09005 145.957936 
L 72.180821 145.811433 
L 72.275539 145.752831 
L 72.386043 145.577028 
L 72.433402 145.518426 
L 72.543906 145.254721 
L 72.583372 145.166819 
L 72.638624 145.078917 
L 72.741235 144.991015 
L 72.839899 144.639407 
L 72.903045 144.551506 
L 72.950404 144.405002 
L 73.053015 144.346401 
L 73.163519 144.053395 
L 73.250343 143.965493 
L 73.321382 143.84829 
L 73.39242 143.760388 
L 73.471352 143.584585 
L 73.550283 143.496683 
L 73.652894 143.35018 
L 73.743666 143.262278 
L 73.755505 143.203676 
L 73.988353 143.115774 
L 74.090964 142.969271 
L 74.150163 142.91067 
L 74.181736 142.793468 
L 74.276454 142.764167 
L 74.386958 142.44186 
L 74.41853 142.353958 
L 74.469836 142.178154 
L 74.552714 142.119553 
L 74.663218 141.797246 
L 74.753989 141.709344 
L 74.864494 141.474939 
L 74.963158 141.387037 
L 75.057876 140.976828 
L 75.164433 140.888926 
L 75.274938 140.654521 
L 75.499893 140.566619 
L 75.602504 140.302914 
L 75.673542 140.215012 
L 75.776153 139.980607 
L 75.926123 139.892705 
L 76.024787 139.746202 
L 76.123452 139.6583 
L 76.222116 139.511797 
L 76.308941 139.423895 
L 76.407605 139.18949 
L 76.466804 139.101588 
L 76.565468 138.896483 
L 76.668079 138.837882 
L 76.774637 138.515575 
L 76.810156 138.427673 
L 76.877248 138.28117 
L 76.995645 138.193268 
L 77.08247 138.046765 
L 77.216654 137.958863 
L 77.327158 137.81236 
L 77.362677 137.753759 
L 77.473181 137.402151 
L 77.5087 137.314249 
L 77.603418 137.109145 
L 77.745495 137.021243 
L 77.83232 136.845439 
L 77.919144 136.757537 
L 78.029649 136.669635 
L 78.084901 136.611034 
L 78.191458 136.200825 
L 78.234871 136.112923 
L 78.286176 135.995721 
L 78.475612 135.907819 
L 78.578223 135.732015 
L 78.668994 135.644113 
L 78.775552 135.439009 
L 78.913682 135.351107 
L 79.000507 135.263205 
L 79.079438 135.204603 
L 79.189942 134.911597 
L 79.304393 134.823695 
L 79.395164 134.647891 
L 79.493829 134.55999 
L 79.5846 134.325584 
L 79.667478 134.237683 
L 79.766143 134.003278 
L 79.864807 133.973977 
L 79.971365 133.65167 
L 80.042403 133.563768 
L 80.141068 133.358664 
L 80.208159 133.270762 
L 80.212106 133.21216 
L 80.401542 133.124259 
L 80.508099 133.065657 
L 80.559405 133.007056 
L 80.654123 132.71405 
L 80.76068 132.626148 
L 80.808039 132.538246 
L 80.965902 132.479645 
L 81.07246 132.303841 
L 81.261896 132.215939 
L 81.356613 131.952233 
L 81.463171 131.864331 
L 81.538156 131.747129 
L 81.699966 131.659227 
L 81.802577 131.307619 
L 81.877562 131.219717 
L 81.988066 130.985312 
L 82.08673 130.89741 
L 82.169608 130.809508 
L 82.288006 130.750907 
L 82.394563 130.340698 
L 82.441922 130.252796 
L 82.520854 130.106293 
L 82.643198 130.047692 
L 82.749755 129.842588 
L 82.927351 129.754686 
L 83.037855 129.578882 
L 83.164146 129.49098 
L 83.27465 129.344477 
L 83.452246 129.256575 
L 83.543017 129.139372 
L 83.653521 129.05147 
L 83.728506 128.963569 
L 83.831117 128.875667 
L 83.894263 128.611961 
L 84.044233 128.55336 
L 84.111324 128.348255 
L 84.221829 128.260353 
L 84.332333 128.055249 
L 84.383638 127.967347 
L 84.490196 127.850145 
L 84.573074 127.762243 
L 84.675685 127.586439 
L 84.774349 127.498537 
L 84.884854 127.322733 
L 84.940106 127.234831 
L 85.022984 127.000426 
L 85.090076 126.912524 
L 85.161114 126.766021 
L 85.35055 126.678119 
L 85.417641 126.560917 
L 85.611024 126.502315 
L 85.717581 126.209309 
L 85.891231 126.150708 
L 85.950429 126.004205 
L 86.029361 125.916303 
L 86.139865 125.740499 
L 86.163545 125.652597 
L 86.266156 125.388891 
L 86.629241 125.300989 
L 86.723959 125.095885 
L 86.783157 125.007983 
L 86.842356 124.920081 
L 87.110723 124.832179 
L 87.213334 124.627075 
L 87.304105 124.539173 
L 87.40277 124.216866 
L 87.489595 124.128964 
L 87.525114 124.011762 
L 87.623778 123.92386 
L 87.651404 123.806657 
L 87.825054 123.718755 
L 87.923718 123.542951 
L 88.046062 123.45505 
L 88.15262 123.279246 
L 88.211818 123.191344 
L 88.26707 123.103442 
L 88.42888 123.01554 
L 88.527544 122.751834 
L 88.606476 122.663932 
L 88.713034 122.576031 
L 88.752499 122.488129 
L 88.85511 122.283024 
L 88.937989 122.195122 
L 88.989294 122.10722 
L 89.194516 122.019318 
L 89.297127 121.814214 
L 89.356326 121.726312 
L 89.462883 121.550508 
L 89.565494 121.462606 
L 89.672052 121.316103 
L 89.707571 121.228201 
L 89.774663 121.081698 
L 89.822022 121.081698 
L 89.908847 120.847293 
L 90.078549 120.759391 
L 90.185107 120.495686 
L 90.275878 120.407784 
L 90.279825 120.319882 
L 90.473207 120.23198 
L 90.571872 120.085477 
L 90.674483 119.997575 
L 90.773147 119.821771 
L 90.816559 119.733869 
L 90.907331 119.499464 
L 91.195431 119.411562 
L 91.282256 119.118556 
L 91.475638 119.030654 
L 91.562462 118.85485 
L 91.850563 118.766948 
L 91.953174 118.532543 
L 92.122876 118.444641 
L 92.221541 118.210236 
L 92.335992 118.122334 
L 92.43071 117.91723 
L 92.529374 117.858629 
L 92.608306 117.712125 
L 92.817474 117.624223 
L 92.927978 117.419119 
L 93.074002 117.331217 
L 93.164773 117.272616 
L 93.279224 117.184714 
L 93.381835 116.97961 
L 93.452873 116.891708 
L 93.559431 116.715904 
L 93.768599 116.628002 
L 93.843584 116.422897 
L 93.993554 116.334996 
L 94.092219 116.07129 
L 94.155364 115.983388 
L 94.261921 115.866185 
L 94.348746 115.778284 
L 94.439517 115.661081 
L 94.569754 115.63178 
L 94.676312 115.426676 
L 94.774976 115.338774 
L 94.846015 115.192271 
L 95.003878 115.13367 
L 95.090703 114.752761 
L 95.232779 114.664859 
L 95.343284 114.430454 
L 95.453788 114.342553 
L 95.564292 114.137448 
L 95.698476 114.049546 
L 95.753728 113.932344 
L 96.08524 113.844442 
L 96.156279 113.727239 
L 96.341768 113.639337 
L 96.38518 113.551435 
L 96.566723 113.463534 
L 96.586456 113.404932 
L 96.732479 113.31703 
L 96.839036 113.141227 
L 96.894289 113.053325 
L 96.957434 112.84822 
L 97.087671 112.760318 
L 97.194228 112.584515 
L 97.348145 112.496613 
L 97.41129 112.320809 
L 97.521794 112.232907 
L 97.620459 112.057103 
L 97.671764 111.969201 
L 97.778322 111.793397 
L 97.896719 111.705496 
L 97.98749 111.529692 
L 98.074315 111.47109 
L 98.13746 111.324587 
L 98.275591 111.236685 
L 98.362415 111.119483 
L 98.445293 111.031581 
L 98.520278 110.797176 
L 98.630783 110.709274 
L 98.729447 110.621372 
L 98.938616 110.53347 
L 98.989921 110.416268 
L 99.218823 110.328366 
L 99.313541 110.181863 
L 99.431938 110.093961 
L 99.542442 109.830255 
L 99.716091 109.742353 
L 99.79897 109.537249 
L 99.976566 109.449347 
L 100.043657 109.244242 
L 100.276505 109.15634 
L 100.379116 109.039138 
L 100.438315 108.980537 
L 100.509353 108.863334 
L 100.600125 108.804733 
L 100.643537 108.68753 
L 100.809293 108.599628 
L 100.809293 108.541027 
L 101.050035 108.453125 
L 101.160539 108.394524 
L 101.421013 108.306622 
L 101.484158 108.21872 
L 101.543357 108.130818 
L 101.630181 108.013616 
L 101.736739 107.925714 
L 101.847243 107.720609 
L 101.965641 107.632707 
L 102.076145 107.2811 
L 102.159023 107.193198 
L 102.182702 107.134597 
L 102.380031 107.075995 
L 102.458963 106.782989 
L 102.561574 106.695087 
L 102.664185 106.607185 
L 102.798368 106.519283 
L 102.900979 106.34348 
L 103.074629 106.255578 
L 103.173293 106.109075 
L 103.331156 106.021173 
L 103.437714 105.845369 
L 103.595577 105.786768 
L 103.678455 105.581663 
L 103.75344 105.493761 
L 103.856051 105.376559 
L 103.91525 105.288657 
L 104.025754 105.054252 
L 104.116525 104.96635 
L 104.223083 104.761245 
L 104.270442 104.673344 
L 104.373053 104.52684 
L 104.499343 104.438938 
L 104.594061 104.292435 
L 104.732191 104.204533 
L 104.838749 104.116631 
L 104.901894 104.02873 
L 104.937413 103.882226 
L 105.040024 103.823625 
L 105.146582 103.501318 
L 105.446522 103.413416 
L 105.509667 103.325514 
L 105.616225 103.237612 
L 105.718836 103.061809 
L 105.963523 102.973907 
L 106.022722 102.886005 
L 106.180585 102.798103 
L 106.291089 102.622299 
L 106.425273 102.534397 
L 106.496311 102.358593 
L 106.666014 102.270692 
L 106.737053 102.065587 
L 106.890969 101.977685 
L 106.997527 101.772581 
L 107.104084 101.684679 
L 107.163283 101.538176 
L 107.329039 101.450274 
L 107.415864 101.333071 
L 107.530315 101.245169 
L 107.613193 101.127967 
L 107.802628 101.040065 
L 107.913133 100.80566 
L 107.996011 100.717758 
L 108.106515 100.512654 
L 108.280164 100.424752 
L 108.374882 100.36615 
L 108.627463 100.278249 
L 108.737967 100.131745 
L 108.816899 100.043843 
L 108.91951 99.89734 
L 109.041854 99.809438 
L 109.148411 99.692236 
L 109.278648 99.604334 
L 109.377313 99.457831 
L 109.428618 99.369929 
L 109.539123 99.194125 
L 109.606214 99.106223 
L 109.685146 98.989021 
L 109.882475 98.901119 
L 109.965353 98.783916 
L 110.064017 98.696014 
L 110.158735 98.49091 
L 110.391583 98.403008 
L 110.419209 98.344407 
L 110.715203 98.256505 
L 110.802027 97.992799 
L 110.916478 97.934198 
L 111.011196 97.787695 
L 111.188792 97.699793 
L 111.232204 97.58259 
L 111.429533 97.494688 
L 111.528198 97.377486 
L 111.666328 97.289584 
L 111.72158 97.201682 
L 111.824191 97.172381 
L 111.911016 97.025878 
L 112.053092 96.937976 
L 112.112291 96.820774 
L 112.44775 96.732872 
L 112.558254 96.64497 
L 112.696384 96.557068 
L 112.696384 96.527767 
L 113.043683 96.439866 
L 113.04763 96.381264 
L 113.244959 96.293362 
L 113.339677 96.146859 
L 113.513326 96.058957 
L 113.61199 95.883154 
L 113.655403 95.795252 
L 113.750121 95.590147 
L 113.888251 95.502245 
L 113.998755 95.385043 
L 114.109259 95.297141 
L 114.140832 95.209239 
L 114.334214 95.121337 
L 114.432878 95.062736 
L 114.51181 94.974834 
L 114.598635 94.769729 
L 114.859109 94.681828 
L 114.914361 94.623226 
L 115.052491 94.564625 
L 115.135369 94.388821 
L 115.23798 94.300919 
L 115.340591 94.037214 
L 115.447149 93.949312 
L 115.557653 93.773508 
L 115.664211 93.685606 
L 115.664211 93.656305 
L 115.814181 93.568403 
L 115.904952 93.333998 
L 115.97599 93.246097 
L 116.074655 93.040992 
L 116.244358 92.95309 
L 116.331182 92.865188 
L 116.418007 92.777286 
L 116.496938 92.660084 
L 116.757413 92.572182 
L 116.820558 92.454979 
L 116.927115 92.396378 
L 116.974474 92.249875 
L 117.092872 92.161973 
L 117.199429 92.044771 
L 117.487529 91.956869 
L 117.566461 91.868967 
L 117.755897 91.781065 
L 117.854561 91.722464 
L 117.965065 91.634562 
L 118.059783 91.429457 
L 118.284738 91.341555 
L 118.391296 91.224353 
L 118.718862 91.136451 
L 118.829366 91.019248 
L 118.856992 90.960647 
L 118.947763 90.696941 
L 119.078 90.60904 
L 119.156932 90.433236 
L 119.362154 90.345334 
L 119.448978 90.286733 
L 119.614735 90.198831 
L 119.626574 90.110929 
L 119.772598 90.023027 
L 119.875209 89.905824 
L 120.048858 89.817922 
L 120.076484 89.730021 
L 120.281706 89.642119 
L 120.368531 89.554217 
L 120.569806 89.466315 
L 120.625058 89.349112 
L 120.838174 89.26121 
L 120.909212 89.173308 
L 121.071022 89.085407 
L 121.181526 88.997505 
L 121.359122 88.909603 
L 121.461733 88.821701 
L 121.710367 88.733799 
L 121.805085 88.587296 
L 121.978734 88.499394 
L 122.061613 88.382191 
L 122.160277 88.352891 
L 122.247102 88.059884 
L 122.381285 87.971983 
L 122.47995 87.796179 
L 122.661492 87.708277 
L 122.736477 87.620375 
L 122.941699 87.532473 
L 123.004845 87.444571 
L 123.190334 87.356669 
L 123.296891 87.239467 
L 123.383716 87.151565 
L 123.474487 87.063663 
L 123.521846 86.975761 
L 123.620511 86.858558 
L 123.884931 86.770657 
L 123.959916 86.682755 
L 124.113833 86.594853 
L 124.212497 86.506951 
L 124.358521 86.419049 
L 124.445345 86.331147 
L 124.615048 86.243245 
L 124.721606 86.067441 
L 124.883415 85.979539 
L 124.986026 85.803736 
L 125.187302 85.715834 
L 125.246501 85.598631 
L 125.404364 85.510729 
L 125.447776 85.452128 
L 125.621425 85.364226 
L 125.704304 85.276324 
L 126.035816 85.188422 
L 126.14632 85.041919 
L 126.335756 84.954017 
L 126.442314 84.807514 
L 126.72252 84.719612 
L 126.769879 84.426606 
L 126.951422 84.368005 
L 126.979048 84.280103 
L 127.160591 84.192201 
L 127.259255 83.987096 
L 127.377652 83.899194 
L 127.409225 83.811293 
L 127.63418 83.723391 
L 127.693379 83.606188 
L 127.831509 83.518286 
L 127.902547 83.401084 
L 128.080143 83.313182 
L 128.166968 83.22528 
L 128.545839 83.137378 
L 128.628717 83.020175 
L 128.723435 82.932274 
L 128.833939 82.844372 
L 128.940497 82.75647 
L 129.039161 82.609967 
L 129.327262 82.522065 
L 129.433819 82.346261 
L 129.52459 82.258359 
L 129.599575 82.170457 
L 129.678507 82.111856 
L 129.789011 81.936052 
L 130.049485 81.877451 
L 130.104737 81.760248 
L 130.266547 81.672346 
L 130.365211 81.555144 
L 130.428357 81.467242 
L 130.534914 81.320739 
L 130.728297 81.232837 
L 130.732243 81.174236 
L 131.040076 81.086334 
L 131.079542 80.969131 
L 131.288711 80.881229 
L 131.288711 80.851929 
L 131.620223 80.764027 
L 131.718887 80.705425 
L 131.947789 80.617524 
L 132.058293 80.412419 
L 132.299034 80.324517 
L 132.405592 80.207315 
L 132.642387 80.119413 
L 132.685799 80.060812 
L 132.989685 80.00221 
L 133.068617 79.885008 
L 133.325144 79.797106 
L 133.384343 79.738505 
L 133.668497 79.650603 
L 133.755321 79.474799 
L 133.893452 79.386897 
L 133.893452 79.357596 
L 134.193392 79.269694 
L 134.292056 79.181793 
L 134.591996 79.093891 
L 134.591996 79.06459 
L 134.781432 78.976688 
L 134.884043 78.800884 
L 135.081371 78.742283 
L 135.160303 78.537179 
L 135.432617 78.449277 
L 135.495762 78.390675 
L 135.712824 78.302774 
L 135.803595 78.15627 
L 136.004871 78.068368 
L 136.103535 77.951166 
L 136.170627 77.863264 
L 136.221932 77.746061 
L 136.415315 77.65816 
L 136.521872 77.570258 
L 136.699468 77.482356 
L 136.806026 77.394454 
L 137.133592 77.306552 
L 137.224363 77.21865 
L 137.319081 77.130748 
L 137.421692 77.042846 
L 137.508517 76.954944 
L 137.619021 76.837742 
L 137.772937 76.74984 
L 137.840029 76.632637 
L 138.088663 76.544736 
L 138.183381 76.486134 
L 138.487268 76.398232 
L 138.550413 76.339631 
L 138.870086 76.251729 
L 138.976643 76.163827 
L 139.158186 76.075925 
L 139.260797 75.958723 
L 139.371301 75.870821 
L 139.446286 75.724318 
L 139.576523 75.636416 
L 139.671241 75.519213 
L 139.809371 75.431311 
L 139.904089 75.37271 
L 140.054059 75.284808 
L 140.125097 75.196906 
L 140.472396 75.109004 
L 140.563167 74.991802 
L 140.863107 74.9039 
L 140.942039 74.757397 
L 140.993344 74.698796 
L 141.092009 74.493691 
L 141.241979 74.405789 
L 141.297231 74.317887 
L 141.695835 74.229985 
L 141.719515 74.112783 
L 141.948416 74.024881 
L 142.054974 73.849077 
L 142.173371 73.761175 
L 142.272035 73.643973 
L 142.433845 73.556071 
L 142.528563 73.468169 
L 142.761411 73.380267 
L 142.800877 73.292365 
L 143.002152 73.204463 
L 143.10871 73.02866 
L 143.24684 72.940758 
L 143.357344 72.823555 
L 143.641498 72.735653 
L 143.748055 72.501248 
L 143.913812 72.413346 
L 144.008529 72.296144 
L 144.308469 72.208242 
L 144.391347 72.091039 
L 144.59657 72.003137 
L 144.695234 71.827334 
L 144.939922 71.739432 
L 145.007014 71.592928 
L 145.121464 71.505027 
L 145.192503 71.446425 
L 145.354312 71.358523 
L 145.445084 71.270622 
L 145.535855 71.18272 
L 145.595054 71.094818 
L 145.741077 71.006916 
L 145.827902 70.919014 
L 145.993658 70.831112 
L 146.104162 70.772511 
L 146.258079 70.713909 
L 146.305438 70.596707 
L 146.700095 70.508805 
L 146.731668 70.420903 
L 146.932943 70.333001 
L 147.000035 70.215799 
L 147.299975 70.127897 
L 147.382853 70.069296 
L 147.769618 69.981394 
L 147.880122 69.776289 
L 148.065611 69.688387 
L 148.168222 69.512584 
L 148.345818 69.424682 
L 148.420803 69.307479 
L 148.779942 69.219577 
L 148.886499 69.102375 
L 149.00095 69.043773 
L 149.040416 68.89727 
L 149.356142 68.809368 
L 149.415341 68.721466 
L 149.616616 68.633565 
L 149.691601 68.574963 
L 149.869197 68.516362 
L 149.916556 68.340558 
L 150.082312 68.281957 
L 150.169137 68.194055 
L 150.299374 68.106153 
L 150.390145 67.95965 
L 150.571688 67.871748 
L 150.642726 67.754546 
L 150.733497 67.666644 
L 150.792696 67.578742 
L 150.91504 67.49084 
L 150.997918 67.432239 
L 151.06501 67.344337 
L 151.132102 67.256435 
L 151.270232 67.168533 
L 151.376789 66.992729 
L 151.507027 66.904827 
L 151.617531 66.816925 
L 151.874058 66.729023 
L 151.960883 66.611821 
L 152.095067 66.523919 
L 152.134532 66.465318 
L 152.477885 66.377416 
L 152.509457 66.318814 
L 152.718626 66.230913 
L 152.730466 66.172311 
L 153.121177 66.084409 
L 153.196162 65.967207 
L 153.622392 65.879305 
L 153.669751 65.762102 
L 153.99337 65.674201 
L 154.103874 65.556998 
L 154.28147 65.469096 
L 154.380135 65.381194 
L 154.652449 65.293292 
L 154.660342 65.234691 
L 155.090519 65.146789 
L 155.19313 65.000286 
L 155.33126 64.912384 
L 155.406245 64.73658 
L 155.745651 64.648678 
L 155.848262 64.531476 
L 156.298171 64.443574 
L 156.404729 64.384973 
L 156.708616 64.297071 
L 156.716509 64.23847 
L 156.921731 64.150568 
L 157.016449 64.033365 
L 157.292709 63.945463 
L 157.363747 63.886862 
L 157.537397 63.79896 
L 157.636061 63.652457 
L 157.868909 63.564555 
L 157.975467 63.418052 
L 158.346445 63.33015 
L 158.441163 63.242248 
L 158.626652 63.154346 
L 158.737156 63.037144 
L 158.867393 62.949242 
L 158.970004 62.89064 
L 159.17128 62.802738 
L 159.254158 62.744137 
L 159.427807 62.656235 
L 159.538312 62.568333 
L 159.846145 62.480432 
L 159.952702 62.333928 
L 160.225016 62.246026 
L 160.307894 62.158125 
L 160.465757 62.070223 
L 160.568368 61.95302 
L 160.726231 61.865118 
L 160.832789 61.777216 
L 161.349791 61.689314 
L 161.397149 61.630713 
L 161.728662 61.542811 
L 161.787861 61.454909 
L 162.040442 61.367007 
L 162.150946 61.220504 
L 162.391687 61.132602 
L 162.391687 61.103302 
L 162.628482 61.0154 
L 162.727146 60.898197 
L 162.967887 60.810295 
L 163.070498 60.634492 
L 163.374385 60.54659 
L 163.374385 60.487988 
L 163.658538 60.400087 
L 163.71379 60.341485 
L 163.970318 60.253583 
L 163.970318 60.224283 
L 164.605717 60.136381 
L 164.605717 60.10708 
L 164.988535 60.019178 
L 164.988535 59.989878 
L 165.548949 59.901976 
L 165.560789 59.843375 
L 165.805476 59.784773 
L 165.908087 59.696871 
L 166.065951 59.608969 
L 166.10147 59.550368 
L 166.247493 59.462466 
L 166.354051 59.374564 
L 166.571112 59.286662 
L 166.571112 59.257362 
L 167.273603 59.16946 
L 167.352535 59.110859 
L 167.549864 59.022957 
L 167.609062 58.935055 
L 167.782712 58.847153 
L 167.885323 58.72995 
L 168.011613 58.642049 
L 168.114224 58.524846 
L 168.386538 58.436944 
L 168.386538 58.407643 
L 168.765409 58.349042 
L 168.871967 58.23184 
L 169.164014 58.143938 
L 169.207426 58.085337 
L 169.396862 58.026735 
L 169.416595 57.938833 
L 169.649443 57.850931 
L 169.724428 57.733729 
L 170.091459 57.645827 
L 170.111192 57.587226 
L 170.446651 57.499324 
L 170.446651 57.440723 
L 170.809737 57.352821 
L 170.813683 57.294219 
L 171.054424 57.206318 
L 171.085997 57.089115 
L 171.366204 57.030514 
L 171.366204 56.971912 
L 171.618785 56.913311 
L 171.662197 56.825409 
L 172.230504 56.766808 
L 172.317329 56.532403 
L 172.510711 56.444501 
L 172.538337 56.327299 
L 172.751453 56.239397 
L 172.84617 56.151495 
L 173.00798 56.063593 
L 173.043499 56.004992 
L 173.252668 55.91709 
L 173.260561 55.858488 
L 173.615753 55.799887 
L 173.639433 55.711985 
L 174.049877 55.624083 
L 174.089342 55.536181 
L 174.562932 55.44828 
L 174.590558 55.360378 
L 174.854978 55.272476 
L 174.957589 55.125973 
L 175.281209 55.038071 
L 175.38382 54.891567 
L 175.561416 54.803666 
L 175.561416 54.774365 
L 175.94818 54.686463 
L 176.058684 54.569261 
L 176.437556 54.481359 
L 176.54806 54.393457 
L 176.68619 54.305555 
L 176.69803 54.188352 
L 176.934825 54.10045 
L 176.994023 54.012548 
L 177.143993 53.924647 
L 177.151886 53.866045 
L 177.361055 53.778143 
L 177.471559 53.660941 
L 177.775446 53.573039 
L 177.854377 53.485137 
L 178.028027 53.397235 
L 178.122744 53.338634 
L 178.371379 53.250732 
L 178.458204 53.16283 
L 178.6358 53.074928 
L 178.679212 52.987026 
L 179.022564 52.899124 
L 179.046244 52.811223 
L 179.314611 52.723321 
L 179.322504 52.664719 
L 179.796093 52.576817 
L 179.796093 52.547517 
L 180.174965 52.459615 
L 180.273629 52.371713 
L 180.573569 52.283811 
L 180.660394 52.195909 
L 180.873509 52.108007 
L 180.97612 52.020105 
L 181.220808 51.932204 
L 181.307632 51.844302 
L 181.429976 51.7564 
L 181.508908 51.639197 
L 182.144307 51.551295 
L 182.246918 51.375491 
L 182.444247 51.28759 
L 182.471873 51.228988 
L 182.775759 51.141086 
L 182.842851 51.082485 
L 182.961248 50.994583 
L 183.0165 50.906681 
L 183.16647 50.818779 
L 183.16647 50.789479 
L 183.620327 50.701577 
L 183.632167 50.584374 
L 184.212313 50.496472 
L 184.322818 50.349969 
L 184.776674 50.262067 
L 184.81614 50.203466 
L 185.305515 50.144865 
L 185.40418 50.027662 
L 185.944861 49.93976 
L 185.944861 49.91046 
L 186.375038 49.822558 
L 186.450023 49.734656 
L 186.793375 49.646754 
L 186.793375 49.617453 
L 187.807645 49.529552 
L 187.815539 49.47095 
L 188.383846 49.383048 
L 188.383846 49.324447 
L 188.865328 49.236545 
L 188.94426 49.148643 
L 189.161321 49.060741 
L 189.23236 49.00214 
L 189.638857 48.914238 
L 189.698056 48.826336 
L 189.926957 48.738434 
L 189.954583 48.679833 
L 190.266363 48.591931 
L 190.266363 48.562631 
L 190.688647 48.474729 
L 190.783365 48.386827 
L 191.103038 48.328226 
L 191.174076 48.240324 
L 191.592413 48.152422 
L 191.671345 48.035219 
L 192.129148 47.947317 
L 192.219919 47.859415 
L 192.464607 47.771514 
L 192.4725 47.712912 
L 192.744814 47.62501 
L 192.819799 47.537109 
L 193.040807 47.449207 
L 193.064486 47.332004 
L 193.182884 47.244102 
L 193.285495 47.1562 
L 193.557809 47.068298 
L 193.644633 46.921795 
L 194.08665 46.833893 
L 194.169528 46.775292 
L 194.493147 46.68739 
L 194.493147 46.65809 
L 194.828607 46.570188 
L 194.828607 46.540887 
L 195.345608 46.452985 
L 195.365341 46.394384 
L 195.708693 46.306482 
L 195.736319 46.21858 
L 196.233588 46.130678 
L 196.292787 46.042776 
L 196.651925 45.954874 
L 196.695338 45.896273 
L 196.896613 45.808371 
L 196.896613 45.779071 
L 197.243912 45.691169 
L 197.287324 45.603267 
L 197.792486 45.515365 
L 197.792486 45.486064 
L 198.135838 45.398162 
L 198.246343 45.28096 
L 198.435778 45.193058 
L 198.483137 45.105156 
L 198.936994 45.017254 
L 199.015925 44.900052 
L 199.343491 44.81215 
L 199.438209 44.724248 
L 199.730256 44.636346 
L 199.730256 44.607045 
L 200.026249 44.519143 
L 200.026249 44.489843 
L 200.484052 44.401941 
L 200.574823 44.284738 
L 200.862923 44.226137 
L 200.862923 44.167536 
L 201.2931 44.079634 
L 201.372032 43.991732 
L 201.802209 43.90383 
L 201.802209 43.845229 
L 202.062683 43.786627 
L 202.062683 43.728026 
L 202.851998 43.640124 
L 202.851998 43.610824 
L 203.428199 43.522922 
L 203.475558 43.43502 
L 203.672887 43.347118 
L 203.755765 43.288517 
L 204.055705 43.200615 
L 204.158316 43.112713 
L 204.746356 43.024811 
L 204.746356 42.99551 
L 205.448846 42.907608 
L 205.555404 42.790406 
L 206.163177 42.702504 
L 206.273681 42.614602 
L 207.043264 42.5267 
L 207.066943 42.468099 
L 207.587891 42.380197 
L 207.678663 42.262995 
L 207.943083 42.175093 
L 207.943083 42.145792 
L 208.140412 42.05789 
L 208.187771 41.999289 
L 208.594269 41.911387 
L 208.637681 41.823485 
L 208.996819 41.735583 
L 209.107324 41.58908 
L 209.375691 41.501178 
L 209.375691 41.442577 
L 209.746669 41.354675 
L 209.746669 41.325374 
L 210.295243 41.237472 
L 210.358389 41.178871 
L 210.713581 41.090969 
L 210.76094 41.003067 
L 211.384499 40.915165 
L 211.455537 40.856564 
L 211.78705 40.768662 
L 211.873874 40.68076 
L 212.083043 40.592858 
L 212.189601 40.475656 
L 212.730282 40.387754 
L 212.730282 40.358453 
L 213.606422 40.270551 
L 213.606422 40.241251 
L 214.154996 40.153349 
L 214.210248 40.065447 
L 214.70357 39.977545 
L 214.814074 39.889643 
L 215.244251 39.801741 
L 215.283717 39.74314 
L 215.559978 39.655238 
L 215.559978 39.625938 
L 216.266415 39.538036 
L 216.266415 39.508735 
L 216.586088 39.420833 
L 216.601874 39.362232 
L 217.063624 39.27433 
L 217.138609 39.215729 
L 217.651664 39.127827 
L 217.683236 39.069225 
L 218.287063 38.981324 
L 218.287063 38.952023 
L 218.835637 38.864121 
L 218.835637 38.83482 
L 219.257921 38.746919 
L 219.325012 38.659017 
L 219.897266 38.571115 
L 219.952518 38.453912 
L 220.33139 38.36601 
L 220.406375 38.307409 
L 220.556345 38.248808 
L 220.651062 38.160906 
L 220.879964 38.073004 
L 220.951002 38.014403 
L 221.558775 37.926501 
L 221.6456 37.8679 
L 222.07183 37.779998 
L 222.07183 37.750697 
L 222.442809 37.662795 
L 222.442809 37.633494 
L 222.80984 37.545593 
L 222.80984 37.516292 
L 223.224231 37.42839 
L 223.263697 37.369789 
L 223.658354 37.281887 
L 223.693874 37.223286 
L 224.151677 37.135384 
L 224.250341 37.047482 
L 224.613426 36.95958 
L 224.72393 36.871678 
L 225.193573 36.813077 
L 225.221199 36.695874 
L 225.588231 36.607972 
L 225.671109 36.49077 
L 225.90001 36.402868 
L 225.939476 36.344267 
L 226.507783 36.256365 
L 226.535409 36.197763 
L 226.894548 36.109862 
L 226.993212 36.05126 
L 227.320778 35.963358 
L 227.431282 35.846156 
L 227.715436 35.758254 
L 227.758848 35.699653 
L 228.165346 35.611751 
L 228.165346 35.58245 
L 228.907302 35.494548 
L 228.950715 35.435947 
L 229.645312 35.348045 
L 229.75187 35.260143 
L 230.31623 35.172241 
L 230.407002 35.084339 
L 230.738514 34.996437 
L 230.738514 34.967137 
L 231.117386 34.879235 
L 231.22789 34.791333 
L 231.535723 34.703431 
L 231.567296 34.64483 
L 232.250053 34.556928 
L 232.250053 34.527627 
L 232.597352 34.439725 
L 232.668391 34.322523 
L 233.339309 34.234621 
L 233.378775 34.17602 
L 233.844471 34.088118 
L 233.887883 34.000216 
L 234.400938 33.912314 
L 234.471976 33.824412 
L 234.645626 33.73651 
L 234.645626 33.70721 
L 235.095536 33.619308 
L 235.095536 33.590007 
L 235.711202 33.502105 
L 235.774347 33.414203 
L 236.709686 33.326301 
L 236.733365 33.238399 
L 237.206955 33.150498 
L 237.266153 33.091896 
L 237.589773 33.003994 
L 237.621345 32.945393 
L 238.371195 32.857491 
L 238.438287 32.769589 
L 238.982914 32.681687 
L 239.014487 32.623086 
L 239.346 32.535184 
L 239.420985 32.476583 
L 239.594634 32.417982 
L 239.689352 32.212877 
L 239.930093 32.124975 
L 240.005078 31.949172 
L 240.695729 31.86127 
L 240.770714 31.773368 
L 241.216677 31.685466 
L 241.216677 31.656165 
L 241.946794 31.568263 
L 242.053352 31.451061 
L 242.199375 31.363159 
L 242.199375 31.333858 
L 242.582193 31.245956 
L 242.582193 31.216656 
L 243.470173 31.128754 
L 243.541211 31.070153 
L 243.742487 31.011551 
L 243.742487 30.95295 
L 244.05032 30.865048 
L 244.156877 30.806447 
L 244.46471 30.718545 
L 244.575215 30.572042 
L 244.859368 30.48414 
L 244.950139 30.425539 
L 245.104056 30.337637 
L 245.135629 30.279035 
L 245.577645 30.191134 
L 245.67631 30.132532 
L 246.031502 30.04463 
L 246.031502 30.01533 
L 246.647168 29.927428 
L 246.726099 29.868827 
L 246.994467 29.780925 
L 247.101024 29.663722 
L 247.357552 29.57582 
L 247.412804 29.517219 
L 247.57856 29.429317 
L 247.637759 29.341415 
L 248.150814 29.253513 
L 248.241585 29.165611 
L 248.825678 29.07771 
L 248.853305 29.019108 
L 249.240069 28.931206 
L 249.240069 28.901906 
L 249.749178 28.814004 
L 249.749178 28.784703 
L 250.534546 28.696801 
L 250.593745 28.6382 
L 251.071281 28.550298 
L 251.071281 28.520997 
L 251.367274 28.433096 
L 251.367274 28.374494 
L 251.951368 28.286592 
L 252.057925 28.198691 
L 252.389438 28.110789 
L 252.389438 28.081488 
L 252.843294 27.993586 
L 252.843294 27.964285 
L 253.297151 27.876384 
L 253.407655 27.788482 
L 254.039107 27.70058 
L 254.039107 27.671279 
L 254.548216 27.583377 
L 254.548216 27.554077 
L 255.242813 27.466175 
L 255.325691 27.407573 
L 255.724296 27.319672 
L 255.826907 27.26107 
L 256.332069 27.173168 
L 256.332069 27.143868 
L 257.070079 27.055966 
L 257.164796 26.938763 
L 257.717317 26.850861 
L 257.717317 26.821561 
L 258.47506 26.733659 
L 258.506633 26.675058 
L 258.944703 26.587156 
L 259.039421 26.469953 
L 259.820843 26.382051 
L 259.911614 26.32345 
L 261.024549 26.235548 
L 261.024549 26.206247 
L 261.794132 26.118346 
L 261.837544 26.059744 
L 262.717631 25.971842 
L 262.717631 25.942542 
L 262.993891 25.85464 
L 263.076769 25.796039 
L 263.617451 25.708137 
L 263.680596 25.649535 
L 264.075254 25.561634 
L 264.075254 25.532333 
L 264.686973 25.444431 
L 264.694866 25.38583 
L 265.709137 25.297928 
L 265.709137 25.268627 
L 266.541864 25.180725 
L 266.541864 25.151425 
L 267.118065 25.063523 
L 267.185156 24.975621 
L 268.006045 24.887719 
L 268.006045 24.858418 
L 268.309931 24.770516 
L 268.309931 24.741216 
L 268.842719 24.653314 
L 268.846666 24.594713 
L 269.426812 24.506811 
L 269.426812 24.47751 
L 269.900402 24.389608 
L 269.900402 24.360308 
L 270.472655 24.272406 
L 270.472655 24.243105 
L 271.013337 24.155203 
L 271.048856 24.067301 
L 271.534285 23.979399 
L 271.534285 23.950099 
L 272.157844 23.862197 
L 272.157844 23.832896 
L 272.907694 23.744994 
L 272.915587 23.686393 
L 273.669383 23.598491 
L 273.775941 23.481289 
L 274.162705 23.393387 
L 274.162705 23.364086 
L 275.121724 23.276184 
L 275.145403 23.217583 
L 275.926825 23.129681 
L 275.926825 23.10038 
L 276.321483 23.012478 
L 276.341216 22.953877 
L 277.06344 22.865975 
L 277.114745 22.807374 
L 277.537029 22.719472 
L 277.647533 22.63157 
L 278.204001 22.543668 
L 278.310558 22.485067 
L 278.444742 22.397165 
L 278.444742 22.367864 
L 279.013049 22.279963 
L 279.080141 22.221361 
L 279.514264 22.133459 
L 279.514264 22.104159 
L 280.098358 22.016257 
L 280.181236 21.928355 
L 280.875833 21.840453 
L 280.911353 21.781852 
L 281.708561 21.69395 
L 281.799333 21.606048 
L 282.501823 21.518146 
L 282.564969 21.459545 
L 282.860962 21.371643 
L 282.860962 21.342342 
L 283.283246 21.25444 
L 283.283246 21.22514 
L 284.585616 21.137238 
L 284.585616 21.107937 
L 285.453863 21.020035 
L 285.453863 20.990735 
L 285.947185 20.902833 
L 286.010331 20.814931 
L 286.7957 20.727029 
L 286.874631 20.668428 
L 287.22193 20.580526 
L 287.22193 20.551225 
L 287.723145 20.463323 
L 287.723145 20.434023 
L 288.508514 20.346121 
L 288.516407 20.258219 
L 288.978157 20.170317 
L 288.978157 20.141016 
L 290.114771 20.053114 
L 290.114771 20.023814 
L 290.915926 19.935912 
L 291.010644 19.789409 
L 291.227706 19.701507 
L 291.322424 19.642906 
L 291.894678 19.555004 
L 291.894678 19.525703 
L 292.861589 19.437801 
L 292.861589 19.408501 
L 293.761409 19.320599 
L 293.761409 19.291298 
L 294.554671 19.203396 
L 294.59019 19.115494 
L 295.292681 19.027592 
L 295.292681 18.998292 
L 295.845202 18.91039 
L 295.880721 18.851788 
L 296.745021 18.763887 
L 296.745021 18.734586 
L 297.305435 18.646684 
L 297.305435 18.617383 
L 297.976353 18.529482 
L 297.976353 18.500181 
L 298.99457 18.412279 
L 299.01825 18.353678 
L 300.071986 18.265776 
L 300.099612 18.207175 
L 301.433555 18.119273 
L 301.480914 18.031371 
L 302.333375 17.943469 
L 302.333375 17.914168 
L 303.813341 17.826266 
L 303.8607 17.738364 
L 305.316987 17.650463 
L 305.368293 17.591861 
L 305.944493 17.503959 
L 305.987905 17.445358 
L 306.840366 17.357456 
L 306.840366 17.298855 
L 307.811224 17.210953 
L 307.811224 17.181652 
L 308.387424 17.09375 
L 308.466356 17.035149 
L 309.567451 16.947247 
L 309.567451 16.917947 
L 310.19101 16.830045 
L 310.19101 16.800744 
L 310.684333 16.712842 
L 310.711959 16.654241 
L 311.363144 16.566339 
L 311.363144 16.537038 
L 312.732606 16.449137 
L 312.732606 16.419836 
L 313.462723 16.331934 
L 313.462723 16.302633 
L 314.800613 16.214731 
L 314.800613 16.185431 
L 316.193755 16.097529 
L 316.292419 16.038928 
L 317.752653 15.951026 
L 317.752653 15.921725 
L 318.502502 15.833823 
L 318.601167 15.716621 
L 319.954843 15.628719 
L 319.978522 15.570118 
L 320.562616 15.482216 
L 320.562616 15.452915 
L 321.801841 15.365013 
L 321.801841 15.335712 
L 322.677981 15.247811 
L 322.677981 15.21851 
L 323.329166 15.130608 
L 323.396258 15.042706 
L 324.106642 14.954804 
L 324.165841 14.896203 
L 325.641861 14.808301 
L 325.716846 14.7497 
L 327.323103 14.661798 
L 327.421767 14.603197 
L 327.847998 14.515295 
L 327.847998 14.485994 
L 329.008291 14.398092 
L 329.008291 14.368792 
L 330.342235 14.28089 
L 330.342235 14.251589 
L 332.311577 14.163687 
L 332.311577 14.134387 
L 333.310061 14.046485 
L 333.310061 14.017184 
L 333.617894 13.929282 
L 333.617894 13.899981 
L 335.034715 13.81208 
L 335.062341 13.753478 
L 336.384445 13.665576 
L 336.384445 13.636276 
L 337.59999 13.548374 
L 337.59999 13.519073 
L 338.657673 13.431171 
L 338.748444 13.37257 
L 339.462775 13.284668 
L 339.462775 13.255368 
L 341.743897 13.167466 
L 341.743897 13.138165 
L 343.701399 13.050263 
L 343.701399 13.020962 
L 344.834067 12.933061 
L 344.834067 12.90376 
L 345.927269 12.815858 
L 346.01804 12.757257 
L 346.550828 12.669355 
L 346.645546 12.581453 
L 347.636137 12.493551 
L 347.651923 12.43495 
L 348.97008 12.347048 
L 348.97008 12.317747 
L 349.99619 12.229845 
L 349.99619 12.200545 
L 354.408463 12.112643 
L 354.408463 12.083342 
L 356.077866 11.99544 
L 356.077866 11.96614 
L 357.818306 11.878238 
L 357.909078 11.819636 
L 360.288864 11.731735 
L 360.288864 11.702434 
L 361.595181 11.614532 
L 361.595181 11.585231 
L 363.236957 11.49733 
L 363.236957 11.468029 
L 364.665618 11.380127 
L 364.665618 11.350826 
L 367.518994 11.262924 
L 367.518994 11.233624 
L 369.440977 11.145722 
L 369.440977 11.116421 
L 374.007167 11.028519 
L 374.007167 10.999219 
L 378.58125 10.999219 
L 378.58125 10.999219 
" style="fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;"></path>
   </g>
   <g id="line2d_18">
    <path clip-path="url(#p0fe17390f5)" d="M 43.78125 228.439219 
L 378.58125 10.999219 
" style="fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;"></path>
   </g>
   <g id="patch_3">
    <path d="M 43.78125 228.439219 
L 43.78125 10.999219 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path>
   </g>
   <g id="patch_4">
    <path d="M 378.58125 228.439219 
L 378.58125 10.999219 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path>
   </g>
   <g id="patch_5">
    <path d="M 43.78125 228.439219 
L 378.58125 228.439219 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path>
   </g>
   <g id="patch_6">
    <path d="M 43.78125 10.999219 
L 378.58125 10.999219 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path>
   </g>
   <g id="legend_1">
    <g id="patch_7">
     <path d="M 50.78125 48.355469 
L 124.178125 48.355469 
Q 126.178125 48.355469 126.178125 46.355469 
L 126.178125 17.999219 
Q 126.178125 15.999219 124.178125 15.999219 
L 50.78125 15.999219 
Q 48.78125 15.999219 48.78125 17.999219 
L 48.78125 46.355469 
Q 48.78125 48.355469 50.78125 48.355469 
z
" style="fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;"></path>
    </g>
    <g id="line2d_19">
     <path d="M 52.78125 24.097656 
L 72.78125 24.097656 
" style="fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;"></path>
    </g>
    <g id="line2d_20"></g>
    <g id="text_19">
     <!-- ROC -->
     <g transform="translate(80.78125 27.597656)scale(0.1 -0.1)">
      <defs>
       <path d="M 39.40625 66.21875 
Q 28.65625 66.21875 22.328125 58.203125 
Q 16.015625 50.203125 16.015625 36.375 
Q 16.015625 22.609375 22.328125 14.59375 
Q 28.65625 6.59375 39.40625 6.59375 
Q 50.140625 6.59375 56.421875 14.59375 
Q 62.703125 22.609375 62.703125 36.375 
Q 62.703125 50.203125 56.421875 58.203125 
Q 50.140625 66.21875 39.40625 66.21875 
z
M 39.40625 74.21875 
Q 54.734375 74.21875 63.90625 63.9375 
Q 73.09375 53.65625 73.09375 36.375 
Q 73.09375 19.140625 63.90625 8.859375 
Q 54.734375 -1.421875 39.40625 -1.421875 
Q 24.03125 -1.421875 14.8125 8.828125 
Q 5.609375 19.09375 5.609375 36.375 
Q 5.609375 53.65625 14.8125 63.9375 
Q 24.03125 74.21875 39.40625 74.21875 
z
" id="DejaVuSans-79"></path>
       <path d="M 64.40625 67.28125 
L 64.40625 56.890625 
Q 59.421875 61.53125 53.78125 63.8125 
Q 48.140625 66.109375 41.796875 66.109375 
Q 29.296875 66.109375 22.65625 58.46875 
Q 16.015625 50.828125 16.015625 36.375 
Q 16.015625 21.96875 22.65625 14.328125 
Q 29.296875 6.6875 41.796875 6.6875 
Q 48.140625 6.6875 53.78125 8.984375 
Q 59.421875 11.28125 64.40625 15.921875 
L 64.40625 5.609375 
Q 59.234375 2.09375 53.4375 0.328125 
Q 47.65625 -1.421875 41.21875 -1.421875 
Q 24.65625 -1.421875 15.125 8.703125 
Q 5.609375 18.84375 5.609375 36.375 
Q 5.609375 53.953125 15.125 64.078125 
Q 24.65625 74.21875 41.21875 74.21875 
Q 47.75 74.21875 53.53125 72.484375 
Q 59.328125 70.75 64.40625 67.28125 
z
" id="DejaVuSans-67"></path>
      </defs>
      <use xlink:href="#DejaVuSans-82"></use>
      <use x="69.482422" xlink:href="#DejaVuSans-79"></use>
      <use x="148.193359" xlink:href="#DejaVuSans-67"></use>
     </g>
    </g>
    <g id="line2d_21">
     <path d="M 52.78125 38.775781 
L 72.78125 38.775781 
" style="fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;"></path>
    </g>
    <g id="line2d_22"></g>
    <g id="text_20">
     <!-- Random -->
     <g transform="translate(80.78125 42.275781)scale(0.1 -0.1)">
      <defs>
       <path d="M 45.40625 46.390625 
L 45.40625 75.984375 
L 54.390625 75.984375 
L 54.390625 0 
L 45.40625 0 
L 45.40625 8.203125 
Q 42.578125 3.328125 38.25 0.953125 
Q 33.9375 -1.421875 27.875 -1.421875 
Q 17.96875 -1.421875 11.734375 6.484375 
Q 5.515625 14.40625 5.515625 27.296875 
Q 5.515625 40.1875 11.734375 48.09375 
Q 17.96875 56 27.875 56 
Q 33.9375 56 38.25 53.625 
Q 42.578125 51.265625 45.40625 46.390625 
z
M 14.796875 27.296875 
Q 14.796875 17.390625 18.875 11.75 
Q 22.953125 6.109375 30.078125 6.109375 
Q 37.203125 6.109375 41.296875 11.75 
Q 45.40625 17.390625 45.40625 27.296875 
Q 45.40625 37.203125 41.296875 42.84375 
Q 37.203125 48.484375 30.078125 48.484375 
Q 22.953125 48.484375 18.875 42.84375 
Q 14.796875 37.203125 14.796875 27.296875 
z
" id="DejaVuSans-100"></path>
       <path d="M 30.609375 48.390625 
Q 23.390625 48.390625 19.1875 42.75 
Q 14.984375 37.109375 14.984375 27.296875 
Q 14.984375 17.484375 19.15625 11.84375 
Q 23.34375 6.203125 30.609375 6.203125 
Q 37.796875 6.203125 41.984375 11.859375 
Q 46.1875 17.53125 46.1875 27.296875 
Q 46.1875 37.015625 41.984375 42.703125 
Q 37.796875 48.390625 30.609375 48.390625 
z
M 30.609375 56 
Q 42.328125 56 49.015625 48.375 
Q 55.71875 40.765625 55.71875 27.296875 
Q 55.71875 13.875 49.015625 6.21875 
Q 42.328125 -1.421875 30.609375 -1.421875 
Q 18.84375 -1.421875 12.171875 6.21875 
Q 5.515625 13.875 5.515625 27.296875 
Q 5.515625 40.765625 12.171875 48.375 
Q 18.84375 56 30.609375 56 
z
" id="DejaVuSans-111"></path>
       <path d="M 52 44.1875 
Q 55.375 50.25 60.0625 53.125 
Q 64.75 56 71.09375 56 
Q 79.640625 56 84.28125 50.015625 
Q 88.921875 44.046875 88.921875 33.015625 
L 88.921875 0 
L 79.890625 0 
L 79.890625 32.71875 
Q 79.890625 40.578125 77.09375 44.375 
Q 74.3125 48.1875 68.609375 48.1875 
Q 61.625 48.1875 57.5625 43.546875 
Q 53.515625 38.921875 53.515625 30.90625 
L 53.515625 0 
L 44.484375 0 
L 44.484375 32.71875 
Q 44.484375 40.625 41.703125 44.40625 
Q 38.921875 48.1875 33.109375 48.1875 
Q 26.21875 48.1875 22.15625 43.53125 
Q 18.109375 38.875 18.109375 30.90625 
L 18.109375 0 
L 9.078125 0 
L 9.078125 54.6875 
L 18.109375 54.6875 
L 18.109375 46.1875 
Q 21.1875 51.21875 25.484375 53.609375 
Q 29.78125 56 35.6875 56 
Q 41.65625 56 45.828125 52.96875 
Q 50 49.953125 52 44.1875 
z
" id="DejaVuSans-109"></path>
      </defs>
      <use xlink:href="#DejaVuSans-82"></use>
      <use x="67.232422" xlink:href="#DejaVuSans-97"></use>
      <use x="128.511719" xlink:href="#DejaVuSans-110"></use>
      <use x="191.890625" xlink:href="#DejaVuSans-100"></use>
      <use x="255.367188" xlink:href="#DejaVuSans-111"></use>
      <use x="316.548828" xlink:href="#DejaVuSans-109"></use>
     </g>
    </g>
   </g>
  </g>
 </g>
 <defs>
  <clippath id="p0fe17390f5">
   <rect height="217.44" width="334.8" x="43.78125" y="10.999219"></rect>
  </clippath>
 </defs>
</svg>

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="cross-validation-으로-hyper-parameter-재-tuning">
<a class="anchor" href="#cross-validation-%EC%9C%BC%EB%A1%9C-hyper-parameter-%EC%9E%AC-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>cross validation 으로 hyper parameter 재 tuning<a class="anchor-link" href="#cross-validation-%EC%9C%BC%EB%A1%9C-hyper-parameter-%EC%9E%AC-tuning"> </a>
</h5>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bayesian_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> 
    <span class="s1">'num_leaves'</span><span class="p">:</span> <span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> 
    <span class="s1">'min_data_in_leaf'</span><span class="p">:</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="c1"># min_child_samples</span>
    <span class="s1">'min_child_weight'</span><span class="p">:(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="s1">'bagging_fraction'</span><span class="p">:(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="c1"># subsample</span>
    <span class="s1">'feature_fraction'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="c1"># colsample_bytree</span>
    <span class="s1">'max_bin'</span><span class="p">:(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
    <span class="s1">'lambda_l2'</span><span class="p">:(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="c1"># reg_lambda</span>
    <span class="s1">'lambda_l1'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span> <span class="c1"># reg_alpha</span>
<span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ftr_app</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">target_app</span><span class="p">,</span> <span class="n">free_raw_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">lgb_roc_eval_cv</span><span class="p">(</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">num_leaves</span><span class="p">,</span> <span class="n">min_data_in_leaf</span><span class="p">,</span> <span class="n">min_child_weight</span><span class="p">,</span> <span class="n">bagging_fraction</span><span class="p">,</span> 
                 <span class="n">feature_fraction</span><span class="p">,</span>  <span class="n">max_bin</span><span class="p">,</span> <span class="n">lambda_l2</span><span class="p">,</span> <span class="n">lambda_l1</span><span class="p">):</span>   
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"num_iterations"</span><span class="p">:</span><span class="mi">500</span><span class="p">,</span> <span class="s2">"learning_rate"</span><span class="p">:</span><span class="mf">0.02</span><span class="p">,</span>
        <span class="s1">'early_stopping_rounds'</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="s1">'metric'</span><span class="p">:</span><span class="s1">'auc'</span><span class="p">,</span>
        <span class="s1">'max_depth'</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">max_depth</span><span class="p">)),</span> <span class="c1">#  호출 시 실수형 값이 들어오므로 실수형 하이퍼 파라미터는 정수형으로 변경 </span>
        <span class="s1">'num_leaves'</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">num_leaves</span><span class="p">)),</span> 
        <span class="s1">'min_data_in_leaf'</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">min_data_in_leaf</span><span class="p">)),</span>
        <span class="s1">'min_child_weight'</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">min_child_weight</span><span class="p">)),</span>
        <span class="s1">'bagging_fraction'</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">bagging_fraction</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> 
        <span class="s1">'feature_fraction'</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">feature_fraction</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">'max_bin'</span><span class="p">:</span>  <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">max_bin</span><span class="p">)),</span><span class="mi">10</span><span class="p">),</span>
        <span class="s1">'lambda_l2'</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="n">lambda_l2</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
        <span class="s1">'lambda_l1'</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="n">lambda_l1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="c1"># 파이썬 lightgbm의 cv 메소드를 사용. </span>
    
    <span class="n">cv_result</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="n">verbose_eval</span> <span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'auc'</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">cv_result</span><span class="p">[</span><span class="s1">'auc-mean'</span><span class="p">])</span>   
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lgbBO</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span><span class="n">lgb_roc_eval_cv</span><span class="p">,</span><span class="n">bayesian_params</span> <span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># 함수 반환값이 최대가 되는 입력값 유추를 위한 iteration 수행. </span>
<span class="n">lgbBO</span><span class="o">.</span><span class="n">maximize</span><span class="p">(</span><span class="n">init_points</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 |  max_bin  | max_depth | min_ch... | min_da... | num_le... |
-------------------------------------------------------------------------------------------------------------------------------------
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163181 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 20217
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229890 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 20217
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170975 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 20217
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.752413 + 0.00209345
[200]	cv_agg's auc: 0.761932 + 0.00172974
[300]	cv_agg's auc: 0.766578 + 0.00170894
[400]	cv_agg's auc: 0.768997 + 0.00156106
[500]	cv_agg's auc: 0.770413 + 0.0014655
|  1        |  0.7704   |  0.7744   |  0.8576   |  30.14    |  5.449    |  217.6    |  12.46    |  22.44    |  179.4    |  62.55    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150128 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 39937
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269771 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 39937
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220449 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 39937
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.752486 + 0.00223274
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[200]	cv_agg's auc: 0.762232 + 0.00215819
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[300]	cv_agg's auc: 0.767065 + 0.00202812
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[400]	cv_agg's auc: 0.76932 + 0.0018439
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[500]	cv_agg's auc: 0.770732 + 0.00167646
| <span class="ansi-magenta-intense-fg"> 2       </span> | <span class="ansi-magenta-intense-fg"> 0.7707  </span> | <span class="ansi-magenta-intense-fg"> 0.6917  </span> | <span class="ansi-magenta-intense-fg"> 0.8959  </span> | <span class="ansi-magenta-intense-fg"> 26.45   </span> | <span class="ansi-magenta-intense-fg"> 5.681   </span> | <span class="ansi-magenta-intense-fg"> 463.5   </span> | <span class="ansi-magenta-intense-fg"> 6.71    </span> | <span class="ansi-magenta-intense-fg"> 5.269   </span> | <span class="ansi-magenta-intense-fg"> 13.84   </span> | <span class="ansi-magenta-intense-fg"> 57.3    </span> |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031859 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 21674
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145476 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 21674
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217018 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 21674
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.747033 + 0.00241826
[200]	cv_agg's auc: 0.757622 + 0.00238554
[300]	cv_agg's auc: 0.763026 + 0.00240661
[400]	cv_agg's auc: 0.765727 + 0.00246971
[500]	cv_agg's auc: 0.767406 + 0.00235341
|  3        |  0.7674   |  0.8891   |  0.935    |  48.93    |  7.992    |  236.1    |  13.81    |  6.795    |  131.6    |  29.73    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216285 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 34163
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147815 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 34163
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213175 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 34163
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.751849 + 0.00212158
[200]	cv_agg's auc: 0.762101 + 0.00196294
[300]	cv_agg's auc: 0.766926 + 0.00203444
[400]	cv_agg's auc: 0.769328 + 0.00189603
[500]	cv_agg's auc: 0.770617 + 0.00193616
|  4        |  0.7706   |  0.9723   |  0.7609   |  20.74    |  2.646    |  389.4    |  10.56    |  28.85    |  13.57    |  48.71    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223108 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 17490
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163106 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 17490
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019565 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 17490
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.750094 + 0.00231875
[200]	cv_agg's auc: 0.760001 + 0.00216013
[300]	cv_agg's auc: 0.765002 + 0.00214597
[400]	cv_agg's auc: 0.767619 + 0.00204746
[500]	cv_agg's auc: 0.769264 + 0.00198502
|  5        |  0.7693   |  0.806    |  0.8085   |  47.19    |  6.819    |  186.2    |  10.37    |  35.18    |  21.44    |  50.67    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.231705 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 20299
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018600 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 20299
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017201 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 20299
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.75385 + 0.00200506
[200]	cv_agg's auc: 0.762312 + 0.00169187
[300]	cv_agg's auc: 0.766827 + 0.00187033
[400]	cv_agg's auc: 0.769265 + 0.00182896
[500]	cv_agg's auc: 0.770744 + 0.00172514
| <span class="ansi-magenta-intense-fg"> 6       </span> | <span class="ansi-magenta-intense-fg"> 0.7707  </span> | <span class="ansi-magenta-intense-fg"> 0.8999  </span> | <span class="ansi-magenta-intense-fg"> 0.5994  </span> | <span class="ansi-magenta-intense-fg"> 31.07   </span> | <span class="ansi-magenta-intense-fg"> 4.228   </span> | <span class="ansi-magenta-intense-fg"> 219.1   </span> | <span class="ansi-magenta-intense-fg"> 11.43   </span> | <span class="ansi-magenta-intense-fg"> 15.67   </span> | <span class="ansi-magenta-intense-fg"> 183.4   </span> | <span class="ansi-magenta-intense-fg"> 63.65   </span> |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292215 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 37203
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 169
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169085 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 37203
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 169
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222382 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 37203
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 169
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.751501 + 0.00221409
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[200]	cv_agg's auc: 0.761105 + 0.0020891
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[300]	cv_agg's auc: 0.766057 + 0.00201994
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[400]	cv_agg's auc: 0.768684 + 0.00192914
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[500]	cv_agg's auc: 0.77023 + 0.00185669
|  7        |  0.7702   |  0.7404   |  0.7668   |  40.5     |  6.329    |  428.5    |  7.581    |  18.72    |  36.92    |  49.96    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166676 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 17390
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228066 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 17390
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020179 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 17390
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.753772 + 0.00271324
[200]	cv_agg's auc: 0.764642 + 0.00241158
[300]	cv_agg's auc: 0.768816 + 0.00221752
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[400]	cv_agg's auc: 0.770854 + 0.00201828
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[500]	cv_agg's auc: 0.77191 + 0.0020515
| <span class="ansi-magenta-intense-fg"> 8       </span> | <span class="ansi-magenta-intense-fg"> 0.7719  </span> | <span class="ansi-magenta-intense-fg"> 0.8143  </span> | <span class="ansi-magenta-intense-fg"> 0.8885  </span> | <span class="ansi-magenta-intense-fg"> 3.55    </span> | <span class="ansi-magenta-intense-fg"> 4.612   </span> | <span class="ansi-magenta-intense-fg"> 184.9   </span> | <span class="ansi-magenta-intense-fg"> 9.342   </span> | <span class="ansi-magenta-intense-fg"> 4.441   </span> | <span class="ansi-magenta-intense-fg"> 198.5   </span> | <span class="ansi-magenta-intense-fg"> 57.95   </span> |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021471 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 12896
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024637 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 12896
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021677 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 12896
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.750032 + 0.00214251
[200]	cv_agg's auc: 0.760098 + 0.00198386
[300]	cv_agg's auc: 0.765089 + 0.00206596
[400]	cv_agg's auc: 0.76748 + 0.00205654
[500]	cv_agg's auc: 0.769124 + 0.00185619
|  9        |  0.7691   |  0.8068   |  0.9537   |  32.45    |  0.03474  |  133.3    |  14.63    |  4.227    |  196.7    |  46.35    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017705 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 19499
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019306 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 19499
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258748 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 19499
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.752892 + 0.00239651
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[200]	cv_agg's auc: 0.762432 + 0.00238515
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[300]	cv_agg's auc: 0.766805 + 0.0024799
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[400]	cv_agg's auc: 0.769205 + 0.00226882
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[500]	cv_agg's auc: 0.770527 + 0.00217943
|  10       |  0.7705   |  0.5315   |  0.6123   |  0.7358   |  7.801    |  209.7    |  6.136    |  16.85    |  187.7    |  39.45    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021861 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 19007
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025572 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 19007
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023354 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 19007
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.751214 + 0.002638
[200]	cv_agg's auc: 0.762887 + 0.00256294
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[300]	cv_agg's auc: 0.76752 + 0.00242286
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[400]	cv_agg's auc: 0.769755 + 0.00229775
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[500]	cv_agg's auc: 0.771039 + 0.00218057
|  11       |  0.771    |  0.5941   |  0.8808   |  3.598    |  6.108    |  203.7    |  7.426    |  11.11    |  183.6    |  38.16    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264825 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 16402
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224450 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 16402
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222576 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 16402
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 165
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.753336 + 0.00242392
[200]	cv_agg's auc: 0.764042 + 0.0019804
[300]	cv_agg's auc: 0.768487 + 0.00181753
[400]	cv_agg's auc: 0.770666 + 0.00166117
[500]	cv_agg's auc: 0.771877 + 0.00164313
|  12       |  0.7719   |  0.6576   |  0.6812   |  4.784    |  0.958    |  173.0    |  9.47     |  8.141    |  167.8    |  49.13    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220884 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 19579
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166716 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 19579
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153352 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 19579
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.752887 + 0.0024333
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[200]	cv_agg's auc: 0.762859 + 0.00250737
[300]	cv_agg's auc: 0.767329 + 0.00227619
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[400]	cv_agg's auc: 0.7697 + 0.00213193
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[500]	cv_agg's auc: 0.77114 + 0.00205365
|  13       |  0.7711   |  0.8786   |  0.6958   |  7.338    |  2.273    |  211.3    |  7.188    |  17.38    |  192.5    |  39.91    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170049 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 16894
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144410 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 16894
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235024 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 16894
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.749206 + 0.00268119
[200]	cv_agg's auc: 0.760618 + 0.00261398
[300]	cv_agg's auc: 0.765772 + 0.00265961
[400]	cv_agg's auc: 0.76807 + 0.00249413
[500]	cv_agg's auc: 0.769569 + 0.0023889
|  14       |  0.7696   |  0.6999   |  0.9419   |  16.83    |  4.11     |  179.0    |  12.26    |  10.37    |  197.7    |  31.16    |
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019978 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 16816
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022800 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 16816
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 165
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221621 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 16816
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 165
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.755504 + 0.00217257
[200]	cv_agg's auc: 0.764391 + 0.00204597
[300]	cv_agg's auc: 0.768847 + 0.00201129
[400]	cv_agg's auc: 0.771043 + 0.00189954
[500]	cv_agg's auc: 0.772446 + 0.00191014
| <span class="ansi-magenta-intense-fg"> 15      </span> | <span class="ansi-magenta-intense-fg"> 0.7724  </span> | <span class="ansi-magenta-intense-fg"> 0.9299  </span> | <span class="ansi-magenta-intense-fg"> 0.5747  </span> | <span class="ansi-magenta-intense-fg"> 7.543   </span> | <span class="ansi-magenta-intense-fg"> 3.292   </span> | <span class="ansi-magenta-intense-fg"> 177.9   </span> | <span class="ansi-magenta-intense-fg"> 10.62   </span> | <span class="ansi-magenta-intense-fg"> 1.398   </span> | <span class="ansi-magenta-intense-fg"> 167.0   </span> | <span class="ansi-magenta-intense-fg"> 54.19   </span> |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161626 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 34319
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036407 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 34319
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 170
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275589 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 34319
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 170
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.751498 + 0.00213142
[200]	cv_agg's auc: 0.76134 + 0.00190147
[300]	cv_agg's auc: 0.766449 + 0.00197641
[400]	cv_agg's auc: 0.768736 + 0.00198658
[500]	cv_agg's auc: 0.770273 + 0.00196995
|  16       |  0.7703   |  0.8845   |  0.727    |  23.44    |  0.6011   |  390.9    |  13.53    |  29.49    |  16.74    |  42.66    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222346 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 36283
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226372 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 36283
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266766 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 36283
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.750135 + 0.00234214
[200]	cv_agg's auc: 0.760181 + 0.00219461
[300]	cv_agg's auc: 0.765124 + 0.00222087
[400]	cv_agg's auc: 0.767708 + 0.00229372
[500]	cv_agg's auc: 0.769155 + 0.0020001
|  17       |  0.7692   |  0.5286   |  0.9555   |  37.34    |  4.565    |  415.6    |  15.38    |  30.31    |  77.81    |  51.73    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020385 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 17800
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017535 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 17800
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014516 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 17800
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.756489 + 0.00222134
[200]	cv_agg's auc: 0.764369 + 0.00182719
[300]	cv_agg's auc: 0.768363 + 0.00176986
[400]	cv_agg's auc: 0.770592 + 0.00173656
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[500]	cv_agg's auc: 0.771856 + 0.00183132
|  18       |  0.7719   |  0.6943   |  0.5035   |  14.91    |  3.789    |  190.4    |  9.341    |  7.834    |  183.6    |  61.8     |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221313 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 27374
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219466 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 27374
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153893 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 27374
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.751297 + 0.00232016
[200]	cv_agg's auc: 0.761509 + 0.00220102
[300]	cv_agg's auc: 0.76659 + 0.0021112
[400]	cv_agg's auc: 0.769188 + 0.00192386
[500]	cv_agg's auc: 0.77086 + 0.00163407
|  19       |  0.7709   |  0.9644   |  0.8266   |  23.37    |  0.6431   |  304.5    |  10.6     |  41.3     |  144.9    |  44.4     |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028651 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 16818
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018467 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 16818
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017772 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 16818
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.755812 + 0.00217874
[200]	cv_agg's auc: 0.764044 + 0.0019529
[300]	cv_agg's auc: 0.768441 + 0.00186193
[400]	cv_agg's auc: 0.770725 + 0.00169801
[500]	cv_agg's auc: 0.77197 + 0.00163797
|  20       |  0.772    |  0.959    |  0.5257   |  10.8     |  1.577    |  177.5    |  15.64    |  3.17     |  144.9    |  56.8     |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169408 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 41759
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215377 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 41759
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221710 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 41759
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.751072 + 0.00218472
[200]	cv_agg's auc: 0.761439 + 0.00212142
[300]	cv_agg's auc: 0.766515 + 0.00204473
[400]	cv_agg's auc: 0.769194 + 0.00183068
[500]	cv_agg's auc: 0.77054 + 0.00173042
|  21       |  0.7705   |  0.6587   |  0.7454   |  19.94    |  4.78     |  487.5    |  9.144    |  41.32    |  192.7    |  33.83    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021007 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 5318
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020654 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 5318
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020216 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 5318
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.750244 + 0.00221396
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[200]	cv_agg's auc: 0.759543 + 0.0021569
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[300]	cv_agg's auc: 0.764436 + 0.00205619
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[400]	cv_agg's auc: 0.766954 + 0.0020125
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[500]	cv_agg's auc: 0.768461 + 0.00193603
|  22       |  0.7685   |  0.9983   |  0.749    |  48.25    |  8.54     |  47.5     |  9.0      |  34.54    |  164.5    |  49.58    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163681 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 33376
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270704 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 33376
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225304 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 33376
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.748988 + 0.00258915
[200]	cv_agg's auc: 0.758788 + 0.00222831
[300]	cv_agg's auc: 0.764116 + 0.00232042
[400]	cv_agg's auc: 0.76702 + 0.00218378
[500]	cv_agg's auc: 0.768711 + 0.00216329
|  23       |  0.7687   |  0.9707   |  0.6446   |  36.9     |  1.905    |  378.9    |  9.577    |  47.94    |  108.4    |  25.81    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261242 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 17478
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221486 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 17478
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022389 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 17478
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.754393 + 0.00249219
[200]	cv_agg's auc: 0.764248 + 0.00225796
[300]	cv_agg's auc: 0.768461 + 0.00200145
[400]	cv_agg's auc: 0.77067 + 0.00182478
[500]	cv_agg's auc: 0.771811 + 0.00171913
|  24       |  0.7718   |  0.5754   |  0.7482   |  6.943    |  9.527    |  185.9    |  11.16    |  2.699    |  195.7    |  54.58    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264848 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 14077
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217247 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 14077
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223460 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 14077
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.754153 + 0.00222357
[200]	cv_agg's auc: 0.764278 + 0.00232934
[300]	cv_agg's auc: 0.768641 + 0.00214161
[400]	cv_agg's auc: 0.770555 + 0.00209644
[500]	cv_agg's auc: 0.771691 + 0.00197203
|  25       |  0.7717   |  0.7917   |  0.6605   |  3.41     |  8.116    |  146.5    |  10.42    |  1.418    |  102.0    |  56.14    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.267075 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 11051
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221536 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 11051
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169461 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 11051
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.750838 + 0.00256294
[200]	cv_agg's auc: 0.761322 + 0.00237721
[300]	cv_agg's auc: 0.766301 + 0.00235791
[400]	cv_agg's auc: 0.768735 + 0.00221622
[500]	cv_agg's auc: 0.77019 + 0.00218553
|  26       |  0.7702   |  0.5176   |  0.6145   |  4.366    |  2.571    |  111.4    |  6.679    |  6.432    |  86.05    |  29.8     |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023574 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 18842
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024873 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 18842
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 164
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024455 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 18842
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 164
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.752761 + 0.00253328
[200]	cv_agg's auc: 0.764295 + 0.00214627
[300]	cv_agg's auc: 0.768268 + 0.00157287
[400]	cv_agg's auc: 0.769987 + 0.00150729
[500]	cv_agg's auc: 0.770816 + 0.00144921
|  27       |  0.7708   |  1.0      |  1.0      |  0.01     |  0.001    |  201.9    |  16.0     |  1.0      |  189.9    |  64.0     |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022653 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 14825
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024379 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 14825
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022400 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 14825
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.753842 + 0.00219322
[200]	cv_agg's auc: 0.764597 + 0.00227203
[300]	cv_agg's auc: 0.768912 + 0.00188056
[400]	cv_agg's auc: 0.770975 + 0.00181401
[500]	cv_agg's auc: 0.772 + 0.001822
|  28       |  0.772    |  0.6115   |  0.9252   |  6.227    |  9.844    |  153.7    |  9.391    |  4.001    |  131.8    |  61.4     |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018247 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 15329
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018926 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 15329
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 166
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021225 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 15329
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 166
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.755952 + 0.00217505
[200]	cv_agg's auc: 0.764695 + 0.00210075
[300]	cv_agg's auc: 0.769001 + 0.00207877
[400]	cv_agg's auc: 0.770975 + 0.00221316
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[500]	cv_agg's auc: 0.772213 + 0.00214666
|  29       |  0.7722   |  0.6475   |  0.5026   |  0.8679   |  2.108    |  159.8    |  9.651    |  45.07    |  121.8    |  60.14    |
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:527: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
C:\Users\channee\anaconda3\lib\site-packages\lightgbm\engine.py:532: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument
  _log_warning("Found `{}` in params. Will use it instead of argument".format(alias))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020594 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 15248
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020154 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 15248
[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 167
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016801 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 15248
[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 167
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[LightGBM] [Info] Start training from score 0.080729
[100]	cv_agg's auc: 0.755049 + 0.00200925
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[200]	cv_agg's auc: 0.762787 + 0.00190172
[300]	cv_agg's auc: 0.767045 + 0.00186358
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[400]	cv_agg's auc: 0.769573 + 0.00165805
[500]	cv_agg's auc: 0.770932 + 0.00161966
|  30       |  0.7709   |  0.9025   |  0.5039   |  26.8     |  6.035    |  159.5    |  9.297    |  36.25    |  103.7    |  61.16    |
=====================================================================================================================================
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lgbBO</span><span class="o">.</span><span class="n">res</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[{'target': 0.7704130128940919,
  'params': {'bagging_fraction': 0.7744067519636624,
   'feature_fraction': 0.8575946831862098,
   'lambda_l1': 30.142141169821482,
   'lambda_l2': 5.449286946785972,
   'max_bin': 217.5908516760633,
   'max_depth': 12.458941130666561,
   'min_child_weight': 22.441773351871934,
   'min_data_in_leaf': 179.43687014859515,
   'num_leaves': 62.54651042004117}},
 {'target': 0.7707320615580647,
  'params': {'bagging_fraction': 0.6917207594128889,
   'feature_fraction': 0.8958625190413323,
   'lambda_l1': 26.449457038447697,
   'lambda_l2': 5.680877566378229,
   'max_bin': 463.5423527634039,
   'max_depth': 6.710360581978869,
   'min_child_weight': 5.269335685375495,
   'min_data_in_leaf': 13.841495513661886,
   'num_leaves': 57.30479382191752}},
 {'target': 0.7674060125927115,
  'params': {'bagging_fraction': 0.8890783754749252,
   'feature_fraction': 0.9350060741234096,
   'lambda_l1': 48.93113092821587,
   'lambda_l2': 7.99178648360302,
   'max_bin': 236.1248875039366,
   'max_depth': 13.805291762864554,
   'min_child_weight': 6.795446867577728,
   'min_data_in_leaf': 131.58499405222955,
   'num_leaves': 29.734131496361854}},
 {'target': 0.7706174479634317,
  'params': {'bagging_fraction': 0.972334458524792,
   'feature_fraction': 0.7609241608750359,
   'lambda_l1': 20.738950380126276,
   'lambda_l2': 2.646291565434165,
   'max_bin': 389.3745078227662,
   'max_depth': 10.561503322165485,
   'min_child_weight': 28.853263494563777,
   'min_data_in_leaf': 13.570062082907477,
   'num_leaves': 48.705419883035084}},
 {'target': 0.7692640265604144,
  'params': {'bagging_fraction': 0.8060478613612108,
   'feature_fraction': 0.8084669984373785,
   'lambda_l1': 47.18796644494606,
   'lambda_l2': 6.818521170735731,
   'max_bin': 186.15887128115514,
   'max_depth': 10.370319537993414,
   'min_child_weight': 35.183928600435976,
   'min_data_in_leaf': 21.44283960956127,
   'num_leaves': 50.670668617826706}},
 {'target': 0.7707441174483378,
  'params': {'bagging_fraction': 0.8999235532397036,
   'feature_fraction': 0.5994369290340391,
   'lambda_l1': 31.074749286460722,
   'lambda_l2': 4.228074498468137,
   'max_bin': 219.10229236574725,
   'max_depth': 11.427010587833749,
   'min_child_weight': 15.667080789571736,
   'min_data_in_leaf': 183.41711356263576,
   'num_leaves': 63.64711067026588}},
 {'target': 0.770229710649176,
  'params': {'bagging_fraction': 0.7404321772522139,
   'feature_fraction': 0.7668315626022122,
   'lambda_l1': 40.496397840921915,
   'lambda_l2': 6.329038251811815,
   'max_bin': 428.47974892655657,
   'max_depth': 7.581108392237857,
   'min_child_weight': 18.721900794608175,
   'min_data_in_leaf': 36.91555073964587,
   'num_leaves': 49.96227208627961}},
 {'target': 0.7719096310120473,
  'params': {'bagging_fraction': 0.8143033572205088,
   'feature_fraction': 0.8884974616228736,
   'lambda_l1': 3.5504085208024487,
   'lambda_l2': 4.611830458102663,
   'max_bin': 184.9398381966357,
   'max_depth': 9.34172001478695,
   'min_child_weight': 4.440733035446478,
   'min_data_in_leaf': 198.5155850624426,
   'num_leaves': 57.94763994734239}},
 {'target': 0.7691251381694691,
  'params': {'bagging_fraction': 0.8067814387656114,
   'feature_fraction': 0.9536571212037808,
   'lambda_l1': 32.4521675349507,
   'lambda_l2': 0.034735067421668264,
   'max_bin': 133.30349225834965,
   'max_depth': 14.62576290350622,
   'min_child_weight': 4.227256072865901,
   'min_data_in_leaf': 196.70462703040053,
   'num_leaves': 46.351397650073146}},
 {'target': 0.7705267701934897,
  'params': {'bagging_fraction': 0.5315401804588187,
   'feature_fraction': 0.612303641148811,
   'lambda_l1': 0.7358255076011369,
   'lambda_l2': 7.800929367645439,
   'max_bin': 209.6908259343996,
   'max_depth': 6.135772933697576,
   'min_child_weight': 16.849896162166722,
   'min_data_in_leaf': 187.67343516011584,
   'num_leaves': 39.44552475790631}},
 {'target': 0.7710389366063817,
  'params': {'bagging_fraction': 0.5940850954373089,
   'feature_fraction': 0.8807926141911452,
   'lambda_l1': 3.5979735041387433,
   'lambda_l2': 6.108496637444428,
   'max_bin': 203.72096015715022,
   'max_depth': 7.425928691480058,
   'min_child_weight': 11.111565381111847,
   'min_data_in_leaf': 183.58886753504038,
   'num_leaves': 38.16261823863786}},
 {'target': 0.7718773672753726,
  'params': {'bagging_fraction': 0.657614492893178,
   'feature_fraction': 0.6811813427672369,
   'lambda_l1': 4.783749362922549,
   'lambda_l2': 0.957997706400652,
   'max_bin': 173.03662669577741,
   'max_depth': 9.470274418940324,
   'min_child_weight': 8.141437429408178,
   'min_data_in_leaf': 167.75970713753094,
   'num_leaves': 49.13015644124545}},
 {'target': 0.7711454074315656,
  'params': {'bagging_fraction': 0.8786355335852942,
   'feature_fraction': 0.6958187117248162,
   'lambda_l1': 7.338172390732913,
   'lambda_l2': 2.272677319027393,
   'max_bin': 211.27555567684178,
   'max_depth': 7.187955460578591,
   'min_child_weight': 17.382076324484828,
   'min_data_in_leaf': 192.54729357889843,
   'num_leaves': 39.9125263869441}},
 {'target': 0.7695691149218623,
  'params': {'bagging_fraction': 0.6998973669239097,
   'feature_fraction': 0.9419379137870297,
   'lambda_l1': 16.828686125736517,
   'lambda_l2': 4.109532131663239,
   'max_bin': 178.95633418021208,
   'max_depth': 12.259221410586013,
   'min_child_weight': 10.36621015246892,
   'min_data_in_leaf': 197.6663134909162,
   'num_leaves': 31.1594353080567}},
 {'target': 0.7724457191365559,
  'params': {'bagging_fraction': 0.9299169469599182,
   'feature_fraction': 0.5746809011453164,
   'lambda_l1': 7.543441207233135,
   'lambda_l2': 3.292195078359306,
   'max_bin': 177.94521484058205,
   'max_depth': 10.61966768561966,
   'min_child_weight': 1.397663097770165,
   'min_data_in_leaf': 166.95618212603793,
   'num_leaves': 54.18985368611588}},
 {'target': 0.7702728801817355,
  'params': {'bagging_fraction': 0.8844562637903797,
   'feature_fraction': 0.7269989688983634,
   'lambda_l1': 23.43573646623273,
   'lambda_l2': 0.601050164341131,
   'max_bin': 390.8777598211938,
   'max_depth': 13.531193778598972,
   'min_child_weight': 29.486064774397775,
   'min_data_in_leaf': 16.738983494464513,
   'num_leaves': 42.66429176206504}},
 {'target': 0.7691546772195966,
  'params': {'bagging_fraction': 0.5285839690801153,
   'feature_fraction': 0.9554784176155467,
   'lambda_l1': 37.33730516793895,
   'lambda_l2': 4.564719396185809,
   'max_bin': 415.6458449914903,
   'max_depth': 15.378391843299484,
   'min_child_weight': 30.31421249555068,
   'min_data_in_leaf': 77.81456840599282,
   'num_leaves': 51.72987878906692}},
 {'target': 0.7718560657970831,
  'params': {'bagging_fraction': 0.6943325913373878,
   'feature_fraction': 0.5034545166040366,
   'lambda_l1': 14.911628058528853,
   'lambda_l2': 3.7888209125712415,
   'max_bin': 190.37970682243528,
   'max_depth': 9.340892785972903,
   'min_child_weight': 7.833820755151955,
   'min_data_in_leaf': 183.59628053956956,
   'num_leaves': 61.80346524240569}},
 {'target': 0.7708603583870698,
  'params': {'bagging_fraction': 0.9644030062523238,
   'feature_fraction': 0.8266253858261036,
   'lambda_l1': 23.3655535588359,
   'lambda_l2': 0.6430770731307863,
   'max_bin': 304.49321845606374,
   'max_depth': 10.603479817877357,
   'min_child_weight': 41.299919645601506,
   'min_data_in_leaf': 144.92832924801024,
   'num_leaves': 44.39976983311924}},
 {'target': 0.7719703571205899,
  'params': {'bagging_fraction': 0.958982625395157,
   'feature_fraction': 0.5256535956136048,
   'lambda_l1': 10.796315561196417,
   'lambda_l2': 1.576964383196764,
   'max_bin': 177.52508011858072,
   'max_depth': 15.636191927411867,
   'min_child_weight': 3.1704301617341315,
   'min_data_in_leaf': 144.90957631393655,
   'num_leaves': 56.801621269683345}},
 {'target': 0.77054013792762,
  'params': {'bagging_fraction': 0.6587283396471605,
   'feature_fraction': 0.7454373863647559,
   'lambda_l1': 19.94077731253563,
   'lambda_l2': 4.780423949040658,
   'max_bin': 487.5445679344283,
   'max_depth': 9.144439579692584,
   'min_child_weight': 41.323096983850455,
   'min_data_in_leaf': 192.74672138313088,
   'num_leaves': 33.83493453930372}},
 {'target': 0.7684607098023677,
  'params': {'bagging_fraction': 0.9982660890256111,
   'feature_fraction': 0.7490195259582262,
   'lambda_l1': 48.254390571730376,
   'lambda_l2': 8.539953109394675,
   'max_bin': 47.49826121182029,
   'max_depth': 8.999627090062027,
   'min_child_weight': 34.5372274617776,
   'min_data_in_leaf': 164.45701046555462,
   'num_leaves': 49.57867373971419}},
 {'target': 0.7687114634910718,
  'params': {'bagging_fraction': 0.9707101482347569,
   'feature_fraction': 0.6446304084751359,
   'lambda_l1': 36.895675572499385,
   'lambda_l2': 1.9051562879071906,
   'max_bin': 378.88812663146865,
   'max_depth': 9.576771523517493,
   'min_child_weight': 47.93924951072747,
   'min_data_in_leaf': 108.42458883918025,
   'num_leaves': 25.81430654629507}},
 {'target': 0.7718111662970083,
  'params': {'bagging_fraction': 0.5754069027269129,
   'feature_fraction': 0.7482186706912217,
   'lambda_l1': 6.943057799845263,
   'lambda_l2': 9.52747316232688,
   'max_bin': 185.8683083359433,
   'max_depth': 11.15539259574,
   'min_child_weight': 2.698545324998308,
   'min_data_in_leaf': 195.68310244680038,
   'num_leaves': 54.58265366779938}},
 {'target': 0.7716905284261153,
  'params': {'bagging_fraction': 0.791661982638599,
   'feature_fraction': 0.6605051402067922,
   'lambda_l1': 3.4104212745018185,
   'lambda_l2': 8.115945070358096,
   'max_bin': 146.54012416035053,
   'max_depth': 10.417648175230251,
   'min_child_weight': 1.4180074553193076,
   'min_data_in_leaf': 102.01504347366142,
   'num_leaves': 56.140917586018034}},
 {'target': 0.7701900669922322,
  'params': {'bagging_fraction': 0.5176386777761051,
   'feature_fraction': 0.6145074911288368,
   'lambda_l1': 4.366000163341913,
   'lambda_l2': 2.5707335917583762,
   'max_bin': 111.39585114915086,
   'max_depth': 6.678620073549007,
   'min_child_weight': 6.431766588419492,
   'min_data_in_leaf': 86.05285708510192,
   'num_leaves': 29.799311383594773}},
 {'target': 0.7708307863985207,
  'params': {'bagging_fraction': 1.0,
   'feature_fraction': 1.0,
   'lambda_l1': 0.01,
   'lambda_l2': 0.001,
   'max_bin': 201.87175711924215,
   'max_depth': 16.0,
   'min_child_weight': 1.0,
   'min_data_in_leaf': 189.8553861436057,
   'num_leaves': 64.0}},
 {'target': 0.7719997542250262,
  'params': {'bagging_fraction': 0.6114748246676402,
   'feature_fraction': 0.9252070326413618,
   'lambda_l1': 6.227021904304498,
   'lambda_l2': 9.844477214259657,
   'max_bin': 153.69685844257907,
   'max_depth': 9.391343257415311,
   'min_child_weight': 4.001203263458588,
   'min_data_in_leaf': 131.7722947221485,
   'num_leaves': 61.40198317680489}},
 {'target': 0.7722128809560068,
  'params': {'bagging_fraction': 0.6475157210105189,
   'feature_fraction': 0.5026014053686318,
   'lambda_l1': 0.8678952276657708,
   'lambda_l2': 2.108427218491435,
   'max_bin': 159.7514426083027,
   'max_depth': 9.651405177349304,
   'min_child_weight': 45.072924694401834,
   'min_data_in_leaf': 121.77348343784082,
   'num_leaves': 60.13997800642714}},
 {'target': 0.7709315329107215,
  'params': {'bagging_fraction': 0.9024808966462303,
   'feature_fraction': 0.5038753407648007,
   'lambda_l1': 26.801718133436975,
   'lambda_l2': 6.034542815576746,
   'max_bin': 159.45233398204377,
   'max_depth': 9.296591640023367,
   'min_child_weight': 36.24576576521278,
   'min_data_in_leaf': 103.70752753815785,
   'num_leaves': 61.159466432090326}}]</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">target_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">lgbBO</span><span class="o">.</span><span class="n">res</span><span class="p">:</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">'target'</span><span class="p">]</span>
    <span class="n">target_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_list</span><span class="p">)</span>
<span class="c1"># 가장 큰 target 값을 가지는 순번(index)를 추출</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'maximum target index:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target_list</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.7704130128940919, 0.7707320615580647, 0.7674060125927115, 0.7706174479634317, 0.7692640265604144, 0.7707441174483378, 0.770229710649176, 0.7719096310120473, 0.7691251381694691, 0.7705267701934897, 0.7710389366063817, 0.7718773672753726, 0.7711454074315656, 0.7695691149218623, 0.7724457191365559, 0.7702728801817355, 0.7691546772195966, 0.7718560657970831, 0.7708603583870698, 0.7719703571205899, 0.77054013792762, 0.7684607098023677, 0.7687114634910718, 0.7718111662970083, 0.7716905284261153, 0.7701900669922322, 0.7708307863985207, 0.7719997542250262, 0.7722128809560068, 0.7709315329107215]
maximum target index: 14
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_dict</span> <span class="o">=</span> <span class="n">lgbBO</span><span class="o">.</span><span class="n">res</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target_list</span><span class="p">))]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">max_dict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{'target': 0.7724457191365559, 'params': {'bagging_fraction': 0.9299169469599182, 'feature_fraction': 0.5746809011453164, 'lambda_l1': 7.543441207233135, 'lambda_l2': 3.292195078359306, 'max_bin': 177.94521484058205, 'max_depth': 10.61966768561966, 'min_child_weight': 1.397663097770165, 'min_data_in_leaf': 166.95618212603793, 'num_leaves': 54.18985368611588}}
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ftr_app</span> <span class="o">=</span> <span class="n">apps_all_train</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'SK_ID_CURR'</span><span class="p">,</span> <span class="s1">'TARGET'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">target_app</span> <span class="o">=</span> <span class="n">apps_all_train</span><span class="p">[</span><span class="s1">'TARGET'</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">ftr_app</span><span class="p">,</span> <span class="n">target_app</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2020</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'train shape:'</span><span class="p">,</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">'valid shape:'</span><span class="p">,</span> <span class="n">valid_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">lgbm_wrapper</span> <span class="o">=</span> <span class="n">LGBMClassifier</span><span class="p">(</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span>
    <span class="n">num_leaves</span><span class="o">=</span><span class="mi">54</span><span class="p">,</span>
    <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.574</span><span class="p">,</span>
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.930</span><span class="p">,</span>
    <span class="n">max_bin</span><span class="o">=</span><span class="mi">403</span><span class="p">,</span>
    <span class="n">reg_alpha</span><span class="o">=</span><span class="mf">7.543</span><span class="p">,</span>
    <span class="n">reg_lambda</span><span class="o">=</span><span class="mf">3.292</span><span class="p">,</span>
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">min_child_samples</span><span class="o">=</span><span class="mi">167</span><span class="p">,</span>
    <span class="n">silent</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>



<span class="n">evals</span> <span class="o">=</span> <span class="p">[(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)]</span>
<span class="n">lgbm_wrapper</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">eval_metric</span><span class="o">=</span><span class="s2">"auc"</span><span class="p">,</span> <span class="n">eval_set</span><span class="o">=</span><span class="n">evals</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">lgbm_wrapper</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">pred_proba</span> <span class="o">=</span> <span class="n">lgbm_wrapper</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>train shape: (215257, 174) valid shape: (92254, 174)
Training until validation scores don't improve for 100 rounds
[100]	valid_0's auc: 0.759222	valid_0's binary_logloss: 0.248113
[200]	valid_0's auc: 0.769117	valid_0's binary_logloss: 0.24319
[300]	valid_0's auc: 0.773816	valid_0's binary_logloss: 0.241396
[400]	valid_0's auc: 0.77635	valid_0's binary_logloss: 0.24053
[500]	valid_0's auc: 0.777994	valid_0's binary_logloss: 0.239917
[600]	valid_0's auc: 0.77869	valid_0's binary_logloss: 0.239651
[700]	valid_0's auc: 0.779292	valid_0's binary_logloss: 0.239452
[800]	valid_0's auc: 0.779592	valid_0's binary_logloss: 0.239369
[900]	valid_0's auc: 0.77968	valid_0's binary_logloss: 0.239334
[1000]	valid_0's auc: 0.779873	valid_0's binary_logloss: 0.239287
Did not meet early stopping. Best iteration is:
[998]	valid_0's auc: 0.779896	valid_0's binary_logloss: 0.239281
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">get_clf_eval</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">pred_proba</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>오차 행렬
[[84646   187]
 [ 7200   221]]
정확도: 0.9199, 정밀도: 0.5417, 재현율: 0.0298,          F1: 0.0565, AUC:0.7799
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">roc_curve_plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_proba</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_svg output_subarea ">
<?xml version="1.0" encoding="utf-8" standalone="no"?>
&lt;!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"&gt;
<!-- Created with matplotlib (https://matplotlib.org/) -->
<svg height="265.995469pt" version="1.1" viewbox="0 0 385.78125 265.995469" width="385.78125pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
 <metadata>
  <rdf xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <work>
    <type rdf:resource="http://purl.org/dc/dcmitype/StillImage"></type>
    <date>2021-09-01T10:27:44.518246</date>
    <format>image/svg+xml</format>
    <creator>
     <agent>
      <title>Matplotlib v3.3.4, https://matplotlib.org/</title>
     </agent>
    </creator>
   </work>
  </rdf>
 </metadata>
 <defs>
  <style type="text/css">*{stroke-linecap:butt;stroke-linejoin:round;}</style>
 </defs>
 <g id="figure_1">
  <g id="patch_1">
   <path d="M 0 265.995469 
L 385.78125 265.995469 
L 385.78125 0 
L 0 0 
z
" style="fill:none;"></path>
  </g>
  <g id="axes_1">
   <g id="patch_2">
    <path d="M 43.78125 228.439219 
L 378.58125 228.439219 
L 378.58125 10.999219 
L 43.78125 10.999219 
z
" style="fill:#ffffff;"></path>
   </g>
   <g id="matplotlib.axis_1">
    <g id="xtick_1">
     <g id="line2d_1">
      <defs>
       <path d="M 0 0 
L 0 3.5 
" id="md928ba9a49" style="stroke:#000000;stroke-width:0.8;"></path>
      </defs>
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="60.52125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_1">
      <!-- 0.05 -->
      <g transform="translate(49.388438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 31.78125 66.40625 
Q 24.171875 66.40625 20.328125 58.90625 
Q 16.5 51.421875 16.5 36.375 
Q 16.5 21.390625 20.328125 13.890625 
Q 24.171875 6.390625 31.78125 6.390625 
Q 39.453125 6.390625 43.28125 13.890625 
Q 47.125 21.390625 47.125 36.375 
Q 47.125 51.421875 43.28125 58.90625 
Q 39.453125 66.40625 31.78125 66.40625 
z
M 31.78125 74.21875 
Q 44.046875 74.21875 50.515625 64.515625 
Q 56.984375 54.828125 56.984375 36.375 
Q 56.984375 17.96875 50.515625 8.265625 
Q 44.046875 -1.421875 31.78125 -1.421875 
Q 19.53125 -1.421875 13.0625 8.265625 
Q 6.59375 17.96875 6.59375 36.375 
Q 6.59375 54.828125 13.0625 64.515625 
Q 19.53125 74.21875 31.78125 74.21875 
z
" id="DejaVuSans-48"></path>
        <path d="M 10.6875 12.40625 
L 21 12.40625 
L 21 0 
L 10.6875 0 
z
" id="DejaVuSans-46"></path>
        <path d="M 10.796875 72.90625 
L 49.515625 72.90625 
L 49.515625 64.59375 
L 19.828125 64.59375 
L 19.828125 46.734375 
Q 21.96875 47.46875 24.109375 47.828125 
Q 26.265625 48.1875 28.421875 48.1875 
Q 40.625 48.1875 47.75 41.5 
Q 54.890625 34.8125 54.890625 23.390625 
Q 54.890625 11.625 47.5625 5.09375 
Q 40.234375 -1.421875 26.90625 -1.421875 
Q 22.3125 -1.421875 17.546875 -0.640625 
Q 12.796875 0.140625 7.71875 1.703125 
L 7.71875 11.625 
Q 12.109375 9.234375 16.796875 8.0625 
Q 21.484375 6.890625 26.703125 6.890625 
Q 35.15625 6.890625 40.078125 11.328125 
Q 45.015625 15.765625 45.015625 23.390625 
Q 45.015625 31 40.078125 35.4375 
Q 35.15625 39.890625 26.703125 39.890625 
Q 22.75 39.890625 18.8125 39.015625 
Q 14.890625 38.140625 10.796875 36.28125 
z
" id="DejaVuSans-53"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-48"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_2">
     <g id="line2d_2">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="94.00125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_2">
      <!-- 0.15 -->
      <g transform="translate(82.868437 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 12.40625 8.296875 
L 28.515625 8.296875 
L 28.515625 63.921875 
L 10.984375 60.40625 
L 10.984375 69.390625 
L 28.421875 72.90625 
L 38.28125 72.90625 
L 38.28125 8.296875 
L 54.390625 8.296875 
L 54.390625 0 
L 12.40625 0 
z
" id="DejaVuSans-49"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-49"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_3">
     <g id="line2d_3">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="127.48125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_3">
      <!-- 0.25 -->
      <g transform="translate(116.348438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 19.1875 8.296875 
L 53.609375 8.296875 
L 53.609375 0 
L 7.328125 0 
L 7.328125 8.296875 
Q 12.9375 14.109375 22.625 23.890625 
Q 32.328125 33.6875 34.8125 36.53125 
Q 39.546875 41.84375 41.421875 45.53125 
Q 43.3125 49.21875 43.3125 52.78125 
Q 43.3125 58.59375 39.234375 62.25 
Q 35.15625 65.921875 28.609375 65.921875 
Q 23.96875 65.921875 18.8125 64.3125 
Q 13.671875 62.703125 7.8125 59.421875 
L 7.8125 69.390625 
Q 13.765625 71.78125 18.9375 73 
Q 24.125 74.21875 28.421875 74.21875 
Q 39.75 74.21875 46.484375 68.546875 
Q 53.21875 62.890625 53.21875 53.421875 
Q 53.21875 48.921875 51.53125 44.890625 
Q 49.859375 40.875 45.40625 35.40625 
Q 44.1875 33.984375 37.640625 27.21875 
Q 31.109375 20.453125 19.1875 8.296875 
z
" id="DejaVuSans-50"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-50"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_4">
     <g id="line2d_4">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="160.96125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_4">
      <!-- 0.35 -->
      <g transform="translate(149.828438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 40.578125 39.3125 
Q 47.65625 37.796875 51.625 33 
Q 55.609375 28.21875 55.609375 21.1875 
Q 55.609375 10.40625 48.1875 4.484375 
Q 40.765625 -1.421875 27.09375 -1.421875 
Q 22.515625 -1.421875 17.65625 -0.515625 
Q 12.796875 0.390625 7.625 2.203125 
L 7.625 11.71875 
Q 11.71875 9.328125 16.59375 8.109375 
Q 21.484375 6.890625 26.8125 6.890625 
Q 36.078125 6.890625 40.9375 10.546875 
Q 45.796875 14.203125 45.796875 21.1875 
Q 45.796875 27.640625 41.28125 31.265625 
Q 36.765625 34.90625 28.71875 34.90625 
L 20.21875 34.90625 
L 20.21875 43.015625 
L 29.109375 43.015625 
Q 36.375 43.015625 40.234375 45.921875 
Q 44.09375 48.828125 44.09375 54.296875 
Q 44.09375 59.90625 40.109375 62.90625 
Q 36.140625 65.921875 28.71875 65.921875 
Q 24.65625 65.921875 20.015625 65.03125 
Q 15.375 64.15625 9.8125 62.3125 
L 9.8125 71.09375 
Q 15.4375 72.65625 20.34375 73.4375 
Q 25.25 74.21875 29.59375 74.21875 
Q 40.828125 74.21875 47.359375 69.109375 
Q 53.90625 64.015625 53.90625 55.328125 
Q 53.90625 49.265625 50.4375 45.09375 
Q 46.96875 40.921875 40.578125 39.3125 
z
" id="DejaVuSans-51"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-51"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_5">
     <g id="line2d_5">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="194.44125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_5">
      <!-- 0.45 -->
      <g transform="translate(183.308437 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 37.796875 64.3125 
L 12.890625 25.390625 
L 37.796875 25.390625 
z
M 35.203125 72.90625 
L 47.609375 72.90625 
L 47.609375 25.390625 
L 58.015625 25.390625 
L 58.015625 17.1875 
L 47.609375 17.1875 
L 47.609375 0 
L 37.796875 0 
L 37.796875 17.1875 
L 4.890625 17.1875 
L 4.890625 26.703125 
z
" id="DejaVuSans-52"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-52"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_6">
     <g id="line2d_6">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="227.92125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_6">
      <!-- 0.55 -->
      <g transform="translate(216.788438 243.037656)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-53"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_7">
     <g id="line2d_7">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="261.40125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_7">
      <!-- 0.65 -->
      <g transform="translate(250.268438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 33.015625 40.375 
Q 26.375 40.375 22.484375 35.828125 
Q 18.609375 31.296875 18.609375 23.390625 
Q 18.609375 15.53125 22.484375 10.953125 
Q 26.375 6.390625 33.015625 6.390625 
Q 39.65625 6.390625 43.53125 10.953125 
Q 47.40625 15.53125 47.40625 23.390625 
Q 47.40625 31.296875 43.53125 35.828125 
Q 39.65625 40.375 33.015625 40.375 
z
M 52.59375 71.296875 
L 52.59375 62.3125 
Q 48.875 64.0625 45.09375 64.984375 
Q 41.3125 65.921875 37.59375 65.921875 
Q 27.828125 65.921875 22.671875 59.328125 
Q 17.53125 52.734375 16.796875 39.40625 
Q 19.671875 43.65625 24.015625 45.921875 
Q 28.375 48.1875 33.59375 48.1875 
Q 44.578125 48.1875 50.953125 41.515625 
Q 57.328125 34.859375 57.328125 23.390625 
Q 57.328125 12.15625 50.6875 5.359375 
Q 44.046875 -1.421875 33.015625 -1.421875 
Q 20.359375 -1.421875 13.671875 8.265625 
Q 6.984375 17.96875 6.984375 36.375 
Q 6.984375 53.65625 15.1875 63.9375 
Q 23.390625 74.21875 37.203125 74.21875 
Q 40.921875 74.21875 44.703125 73.484375 
Q 48.484375 72.75 52.59375 71.296875 
z
" id="DejaVuSans-54"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-54"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_8">
     <g id="line2d_8">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="294.88125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_8">
      <!-- 0.75 -->
      <g transform="translate(283.748438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 8.203125 72.90625 
L 55.078125 72.90625 
L 55.078125 68.703125 
L 28.609375 0 
L 18.3125 0 
L 43.21875 64.59375 
L 8.203125 64.59375 
z
" id="DejaVuSans-55"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-55"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_9">
     <g id="line2d_9">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="328.36125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_9">
      <!-- 0.85 -->
      <g transform="translate(317.228437 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 31.78125 34.625 
Q 24.75 34.625 20.71875 30.859375 
Q 16.703125 27.09375 16.703125 20.515625 
Q 16.703125 13.921875 20.71875 10.15625 
Q 24.75 6.390625 31.78125 6.390625 
Q 38.8125 6.390625 42.859375 10.171875 
Q 46.921875 13.96875 46.921875 20.515625 
Q 46.921875 27.09375 42.890625 30.859375 
Q 38.875 34.625 31.78125 34.625 
z
M 21.921875 38.8125 
Q 15.578125 40.375 12.03125 44.71875 
Q 8.5 49.078125 8.5 55.328125 
Q 8.5 64.0625 14.71875 69.140625 
Q 20.953125 74.21875 31.78125 74.21875 
Q 42.671875 74.21875 48.875 69.140625 
Q 55.078125 64.0625 55.078125 55.328125 
Q 55.078125 49.078125 51.53125 44.71875 
Q 48 40.375 41.703125 38.8125 
Q 48.828125 37.15625 52.796875 32.3125 
Q 56.78125 27.484375 56.78125 20.515625 
Q 56.78125 9.90625 50.3125 4.234375 
Q 43.84375 -1.421875 31.78125 -1.421875 
Q 19.734375 -1.421875 13.25 4.234375 
Q 6.78125 9.90625 6.78125 20.515625 
Q 6.78125 27.484375 10.78125 32.3125 
Q 14.796875 37.15625 21.921875 38.8125 
z
M 18.3125 54.390625 
Q 18.3125 48.734375 21.84375 45.5625 
Q 25.390625 42.390625 31.78125 42.390625 
Q 38.140625 42.390625 41.71875 45.5625 
Q 45.3125 48.734375 45.3125 54.390625 
Q 45.3125 60.0625 41.71875 63.234375 
Q 38.140625 66.40625 31.78125 66.40625 
Q 25.390625 66.40625 21.84375 63.234375 
Q 18.3125 60.0625 18.3125 54.390625 
z
" id="DejaVuSans-56"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-56"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="xtick_10">
     <g id="line2d_10">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="361.84125" xlink:href="#md928ba9a49" y="228.439219"></use>
      </g>
     </g>
     <g id="text_10">
      <!-- 0.95 -->
      <g transform="translate(350.708438 243.037656)scale(0.1 -0.1)">
       <defs>
        <path d="M 10.984375 1.515625 
L 10.984375 10.5 
Q 14.703125 8.734375 18.5 7.8125 
Q 22.3125 6.890625 25.984375 6.890625 
Q 35.75 6.890625 40.890625 13.453125 
Q 46.046875 20.015625 46.78125 33.40625 
Q 43.953125 29.203125 39.59375 26.953125 
Q 35.25 24.703125 29.984375 24.703125 
Q 19.046875 24.703125 12.671875 31.3125 
Q 6.296875 37.9375 6.296875 49.421875 
Q 6.296875 60.640625 12.9375 67.421875 
Q 19.578125 74.21875 30.609375 74.21875 
Q 43.265625 74.21875 49.921875 64.515625 
Q 56.59375 54.828125 56.59375 36.375 
Q 56.59375 19.140625 48.40625 8.859375 
Q 40.234375 -1.421875 26.421875 -1.421875 
Q 22.703125 -1.421875 18.890625 -0.6875 
Q 15.09375 0.046875 10.984375 1.515625 
z
M 30.609375 32.421875 
Q 37.25 32.421875 41.125 36.953125 
Q 45.015625 41.5 45.015625 49.421875 
Q 45.015625 57.28125 41.125 61.84375 
Q 37.25 66.40625 30.609375 66.40625 
Q 23.96875 66.40625 20.09375 61.84375 
Q 16.21875 57.28125 16.21875 49.421875 
Q 16.21875 41.5 20.09375 36.953125 
Q 23.96875 32.421875 30.609375 32.421875 
z
" id="DejaVuSans-57"></path>
       </defs>
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-57"></use>
       <use x="159.033203" xlink:href="#DejaVuSans-53"></use>
      </g>
     </g>
    </g>
    <g id="text_11">
     <!-- FPR( 1 - Sensitivity ) -->
     <g transform="translate(160.542969 256.715781)scale(0.1 -0.1)">
      <defs>
       <path d="M 9.8125 72.90625 
L 51.703125 72.90625 
L 51.703125 64.59375 
L 19.671875 64.59375 
L 19.671875 43.109375 
L 48.578125 43.109375 
L 48.578125 34.8125 
L 19.671875 34.8125 
L 19.671875 0 
L 9.8125 0 
z
" id="DejaVuSans-70"></path>
       <path d="M 19.671875 64.796875 
L 19.671875 37.40625 
L 32.078125 37.40625 
Q 38.96875 37.40625 42.71875 40.96875 
Q 46.484375 44.53125 46.484375 51.125 
Q 46.484375 57.671875 42.71875 61.234375 
Q 38.96875 64.796875 32.078125 64.796875 
z
M 9.8125 72.90625 
L 32.078125 72.90625 
Q 44.34375 72.90625 50.609375 67.359375 
Q 56.890625 61.8125 56.890625 51.125 
Q 56.890625 40.328125 50.609375 34.8125 
Q 44.34375 29.296875 32.078125 29.296875 
L 19.671875 29.296875 
L 19.671875 0 
L 9.8125 0 
z
" id="DejaVuSans-80"></path>
       <path d="M 44.390625 34.1875 
Q 47.5625 33.109375 50.5625 29.59375 
Q 53.5625 26.078125 56.59375 19.921875 
L 66.609375 0 
L 56 0 
L 46.6875 18.703125 
Q 43.0625 26.03125 39.671875 28.421875 
Q 36.28125 30.8125 30.421875 30.8125 
L 19.671875 30.8125 
L 19.671875 0 
L 9.8125 0 
L 9.8125 72.90625 
L 32.078125 72.90625 
Q 44.578125 72.90625 50.734375 67.671875 
Q 56.890625 62.453125 56.890625 51.90625 
Q 56.890625 45.015625 53.6875 40.46875 
Q 50.484375 35.9375 44.390625 34.1875 
z
M 19.671875 64.796875 
L 19.671875 38.921875 
L 32.078125 38.921875 
Q 39.203125 38.921875 42.84375 42.21875 
Q 46.484375 45.515625 46.484375 51.90625 
Q 46.484375 58.296875 42.84375 61.546875 
Q 39.203125 64.796875 32.078125 64.796875 
z
" id="DejaVuSans-82"></path>
       <path d="M 31 75.875 
Q 24.46875 64.65625 21.28125 53.65625 
Q 18.109375 42.671875 18.109375 31.390625 
Q 18.109375 20.125 21.3125 9.0625 
Q 24.515625 -2 31 -13.1875 
L 23.1875 -13.1875 
Q 15.875 -1.703125 12.234375 9.375 
Q 8.59375 20.453125 8.59375 31.390625 
Q 8.59375 42.28125 12.203125 53.3125 
Q 15.828125 64.359375 23.1875 75.875 
z
" id="DejaVuSans-40"></path>
       <path id="DejaVuSans-32"></path>
       <path d="M 4.890625 31.390625 
L 31.203125 31.390625 
L 31.203125 23.390625 
L 4.890625 23.390625 
z
" id="DejaVuSans-45"></path>
       <path d="M 53.515625 70.515625 
L 53.515625 60.890625 
Q 47.90625 63.578125 42.921875 64.890625 
Q 37.9375 66.21875 33.296875 66.21875 
Q 25.25 66.21875 20.875 63.09375 
Q 16.5 59.96875 16.5 54.203125 
Q 16.5 49.359375 19.40625 46.890625 
Q 22.3125 44.4375 30.421875 42.921875 
L 36.375 41.703125 
Q 47.40625 39.59375 52.65625 34.296875 
Q 57.90625 29 57.90625 20.125 
Q 57.90625 9.515625 50.796875 4.046875 
Q 43.703125 -1.421875 29.984375 -1.421875 
Q 24.8125 -1.421875 18.96875 -0.25 
Q 13.140625 0.921875 6.890625 3.21875 
L 6.890625 13.375 
Q 12.890625 10.015625 18.65625 8.296875 
Q 24.421875 6.59375 29.984375 6.59375 
Q 38.421875 6.59375 43.015625 9.90625 
Q 47.609375 13.234375 47.609375 19.390625 
Q 47.609375 24.75 44.3125 27.78125 
Q 41.015625 30.8125 33.5 32.328125 
L 27.484375 33.5 
Q 16.453125 35.6875 11.515625 40.375 
Q 6.59375 45.0625 6.59375 53.421875 
Q 6.59375 63.09375 13.40625 68.65625 
Q 20.21875 74.21875 32.171875 74.21875 
Q 37.3125 74.21875 42.625 73.28125 
Q 47.953125 72.359375 53.515625 70.515625 
z
" id="DejaVuSans-83"></path>
       <path d="M 56.203125 29.59375 
L 56.203125 25.203125 
L 14.890625 25.203125 
Q 15.484375 15.921875 20.484375 11.0625 
Q 25.484375 6.203125 34.421875 6.203125 
Q 39.59375 6.203125 44.453125 7.46875 
Q 49.3125 8.734375 54.109375 11.28125 
L 54.109375 2.78125 
Q 49.265625 0.734375 44.1875 -0.34375 
Q 39.109375 -1.421875 33.890625 -1.421875 
Q 20.796875 -1.421875 13.15625 6.1875 
Q 5.515625 13.8125 5.515625 26.8125 
Q 5.515625 40.234375 12.765625 48.109375 
Q 20.015625 56 32.328125 56 
Q 43.359375 56 49.78125 48.890625 
Q 56.203125 41.796875 56.203125 29.59375 
z
M 47.21875 32.234375 
Q 47.125 39.59375 43.09375 43.984375 
Q 39.0625 48.390625 32.421875 48.390625 
Q 24.90625 48.390625 20.390625 44.140625 
Q 15.875 39.890625 15.1875 32.171875 
z
" id="DejaVuSans-101"></path>
       <path d="M 54.890625 33.015625 
L 54.890625 0 
L 45.90625 0 
L 45.90625 32.71875 
Q 45.90625 40.484375 42.875 44.328125 
Q 39.84375 48.1875 33.796875 48.1875 
Q 26.515625 48.1875 22.3125 43.546875 
Q 18.109375 38.921875 18.109375 30.90625 
L 18.109375 0 
L 9.078125 0 
L 9.078125 54.6875 
L 18.109375 54.6875 
L 18.109375 46.1875 
Q 21.34375 51.125 25.703125 53.5625 
Q 30.078125 56 35.796875 56 
Q 45.21875 56 50.046875 50.171875 
Q 54.890625 44.34375 54.890625 33.015625 
z
" id="DejaVuSans-110"></path>
       <path d="M 44.28125 53.078125 
L 44.28125 44.578125 
Q 40.484375 46.53125 36.375 47.5 
Q 32.28125 48.484375 27.875 48.484375 
Q 21.1875 48.484375 17.84375 46.4375 
Q 14.5 44.390625 14.5 40.28125 
Q 14.5 37.15625 16.890625 35.375 
Q 19.28125 33.59375 26.515625 31.984375 
L 29.59375 31.296875 
Q 39.15625 29.25 43.1875 25.515625 
Q 47.21875 21.78125 47.21875 15.09375 
Q 47.21875 7.46875 41.1875 3.015625 
Q 35.15625 -1.421875 24.609375 -1.421875 
Q 20.21875 -1.421875 15.453125 -0.5625 
Q 10.6875 0.296875 5.421875 2 
L 5.421875 11.28125 
Q 10.40625 8.6875 15.234375 7.390625 
Q 20.0625 6.109375 24.8125 6.109375 
Q 31.15625 6.109375 34.5625 8.28125 
Q 37.984375 10.453125 37.984375 14.40625 
Q 37.984375 18.0625 35.515625 20.015625 
Q 33.0625 21.96875 24.703125 23.78125 
L 21.578125 24.515625 
Q 13.234375 26.265625 9.515625 29.90625 
Q 5.8125 33.546875 5.8125 39.890625 
Q 5.8125 47.609375 11.28125 51.796875 
Q 16.75 56 26.8125 56 
Q 31.78125 56 36.171875 55.265625 
Q 40.578125 54.546875 44.28125 53.078125 
z
" id="DejaVuSans-115"></path>
       <path d="M 9.421875 54.6875 
L 18.40625 54.6875 
L 18.40625 0 
L 9.421875 0 
z
M 9.421875 75.984375 
L 18.40625 75.984375 
L 18.40625 64.59375 
L 9.421875 64.59375 
z
" id="DejaVuSans-105"></path>
       <path d="M 18.3125 70.21875 
L 18.3125 54.6875 
L 36.8125 54.6875 
L 36.8125 47.703125 
L 18.3125 47.703125 
L 18.3125 18.015625 
Q 18.3125 11.328125 20.140625 9.421875 
Q 21.96875 7.515625 27.59375 7.515625 
L 36.8125 7.515625 
L 36.8125 0 
L 27.59375 0 
Q 17.1875 0 13.234375 3.875 
Q 9.28125 7.765625 9.28125 18.015625 
L 9.28125 47.703125 
L 2.6875 47.703125 
L 2.6875 54.6875 
L 9.28125 54.6875 
L 9.28125 70.21875 
z
" id="DejaVuSans-116"></path>
       <path d="M 2.984375 54.6875 
L 12.5 54.6875 
L 29.59375 8.796875 
L 46.6875 54.6875 
L 56.203125 54.6875 
L 35.6875 0 
L 23.484375 0 
z
" id="DejaVuSans-118"></path>
       <path d="M 32.171875 -5.078125 
Q 28.375 -14.84375 24.75 -17.8125 
Q 21.140625 -20.796875 15.09375 -20.796875 
L 7.90625 -20.796875 
L 7.90625 -13.28125 
L 13.1875 -13.28125 
Q 16.890625 -13.28125 18.9375 -11.515625 
Q 21 -9.765625 23.484375 -3.21875 
L 25.09375 0.875 
L 2.984375 54.6875 
L 12.5 54.6875 
L 29.59375 11.921875 
L 46.6875 54.6875 
L 56.203125 54.6875 
z
" id="DejaVuSans-121"></path>
       <path d="M 8.015625 75.875 
L 15.828125 75.875 
Q 23.140625 64.359375 26.78125 53.3125 
Q 30.421875 42.28125 30.421875 31.390625 
Q 30.421875 20.453125 26.78125 9.375 
Q 23.140625 -1.703125 15.828125 -13.1875 
L 8.015625 -13.1875 
Q 14.5 -2 17.703125 9.0625 
Q 20.90625 20.125 20.90625 31.390625 
Q 20.90625 42.671875 17.703125 53.65625 
Q 14.5 64.65625 8.015625 75.875 
z
" id="DejaVuSans-41"></path>
      </defs>
      <use xlink:href="#DejaVuSans-70"></use>
      <use x="57.519531" xlink:href="#DejaVuSans-80"></use>
      <use x="117.822266" xlink:href="#DejaVuSans-82"></use>
      <use x="187.304688" xlink:href="#DejaVuSans-40"></use>
      <use x="226.318359" xlink:href="#DejaVuSans-32"></use>
      <use x="258.105469" xlink:href="#DejaVuSans-49"></use>
      <use x="321.728516" xlink:href="#DejaVuSans-32"></use>
      <use x="353.515625" xlink:href="#DejaVuSans-45"></use>
      <use x="389.599609" xlink:href="#DejaVuSans-32"></use>
      <use x="421.386719" xlink:href="#DejaVuSans-83"></use>
      <use x="484.863281" xlink:href="#DejaVuSans-101"></use>
      <use x="546.386719" xlink:href="#DejaVuSans-110"></use>
      <use x="609.765625" xlink:href="#DejaVuSans-115"></use>
      <use x="661.865234" xlink:href="#DejaVuSans-105"></use>
      <use x="689.648438" xlink:href="#DejaVuSans-116"></use>
      <use x="728.857422" xlink:href="#DejaVuSans-105"></use>
      <use x="756.640625" xlink:href="#DejaVuSans-118"></use>
      <use x="815.820312" xlink:href="#DejaVuSans-105"></use>
      <use x="843.603516" xlink:href="#DejaVuSans-116"></use>
      <use x="882.8125" xlink:href="#DejaVuSans-121"></use>
      <use x="941.992188" xlink:href="#DejaVuSans-32"></use>
      <use x="973.779297" xlink:href="#DejaVuSans-41"></use>
     </g>
    </g>
   </g>
   <g id="matplotlib.axis_2">
    <g id="ytick_1">
     <g id="line2d_11">
      <defs>
       <path d="M 0 0 
L -3.5 0 
" id="m06e0c079f7" style="stroke:#000000;stroke-width:0.8;"></path>
      </defs>
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m06e0c079f7" y="228.439219"></use>
      </g>
     </g>
     <g id="text_12">
      <!-- 0.0 -->
      <g transform="translate(20.878125 232.238437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-48"></use>
      </g>
     </g>
    </g>
    <g id="ytick_2">
     <g id="line2d_12">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m06e0c079f7" y="184.951219"></use>
      </g>
     </g>
     <g id="text_13">
      <!-- 0.2 -->
      <g transform="translate(20.878125 188.750437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-50"></use>
      </g>
     </g>
    </g>
    <g id="ytick_3">
     <g id="line2d_13">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m06e0c079f7" y="141.463219"></use>
      </g>
     </g>
     <g id="text_14">
      <!-- 0.4 -->
      <g transform="translate(20.878125 145.262437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-52"></use>
      </g>
     </g>
    </g>
    <g id="ytick_4">
     <g id="line2d_14">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m06e0c079f7" y="97.975219"></use>
      </g>
     </g>
     <g id="text_15">
      <!-- 0.6 -->
      <g transform="translate(20.878125 101.774437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-54"></use>
      </g>
     </g>
    </g>
    <g id="ytick_5">
     <g id="line2d_15">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m06e0c079f7" y="54.487219"></use>
      </g>
     </g>
     <g id="text_16">
      <!-- 0.8 -->
      <g transform="translate(20.878125 58.286437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-48"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-56"></use>
      </g>
     </g>
    </g>
    <g id="ytick_6">
     <g id="line2d_16">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="43.78125" xlink:href="#m06e0c079f7" y="10.999219"></use>
      </g>
     </g>
     <g id="text_17">
      <!-- 1.0 -->
      <g transform="translate(20.878125 14.798437)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-49"></use>
       <use x="63.623047" xlink:href="#DejaVuSans-46"></use>
       <use x="95.410156" xlink:href="#DejaVuSans-48"></use>
      </g>
     </g>
    </g>
    <g id="text_18">
     <!-- TPR( Recall ) -->
     <g transform="translate(14.798438 151.259062)rotate(-90)scale(0.1 -0.1)">
      <defs>
       <path d="M -0.296875 72.90625 
L 61.375 72.90625 
L 61.375 64.59375 
L 35.5 64.59375 
L 35.5 0 
L 25.59375 0 
L 25.59375 64.59375 
L -0.296875 64.59375 
z
" id="DejaVuSans-84"></path>
       <path d="M 48.78125 52.59375 
L 48.78125 44.1875 
Q 44.96875 46.296875 41.140625 47.34375 
Q 37.3125 48.390625 33.40625 48.390625 
Q 24.65625 48.390625 19.8125 42.84375 
Q 14.984375 37.3125 14.984375 27.296875 
Q 14.984375 17.28125 19.8125 11.734375 
Q 24.65625 6.203125 33.40625 6.203125 
Q 37.3125 6.203125 41.140625 7.25 
Q 44.96875 8.296875 48.78125 10.40625 
L 48.78125 2.09375 
Q 45.015625 0.34375 40.984375 -0.53125 
Q 36.96875 -1.421875 32.421875 -1.421875 
Q 20.0625 -1.421875 12.78125 6.34375 
Q 5.515625 14.109375 5.515625 27.296875 
Q 5.515625 40.671875 12.859375 48.328125 
Q 20.21875 56 33.015625 56 
Q 37.15625 56 41.109375 55.140625 
Q 45.0625 54.296875 48.78125 52.59375 
z
" id="DejaVuSans-99"></path>
       <path d="M 34.28125 27.484375 
Q 23.390625 27.484375 19.1875 25 
Q 14.984375 22.515625 14.984375 16.5 
Q 14.984375 11.71875 18.140625 8.90625 
Q 21.296875 6.109375 26.703125 6.109375 
Q 34.1875 6.109375 38.703125 11.40625 
Q 43.21875 16.703125 43.21875 25.484375 
L 43.21875 27.484375 
z
M 52.203125 31.203125 
L 52.203125 0 
L 43.21875 0 
L 43.21875 8.296875 
Q 40.140625 3.328125 35.546875 0.953125 
Q 30.953125 -1.421875 24.3125 -1.421875 
Q 15.921875 -1.421875 10.953125 3.296875 
Q 6 8.015625 6 15.921875 
Q 6 25.140625 12.171875 29.828125 
Q 18.359375 34.515625 30.609375 34.515625 
L 43.21875 34.515625 
L 43.21875 35.40625 
Q 43.21875 41.609375 39.140625 45 
Q 35.0625 48.390625 27.6875 48.390625 
Q 23 48.390625 18.546875 47.265625 
Q 14.109375 46.140625 10.015625 43.890625 
L 10.015625 52.203125 
Q 14.9375 54.109375 19.578125 55.046875 
Q 24.21875 56 28.609375 56 
Q 40.484375 56 46.34375 49.84375 
Q 52.203125 43.703125 52.203125 31.203125 
z
" id="DejaVuSans-97"></path>
       <path d="M 9.421875 75.984375 
L 18.40625 75.984375 
L 18.40625 0 
L 9.421875 0 
z
" id="DejaVuSans-108"></path>
      </defs>
      <use xlink:href="#DejaVuSans-84"></use>
      <use x="61.083984" xlink:href="#DejaVuSans-80"></use>
      <use x="121.386719" xlink:href="#DejaVuSans-82"></use>
      <use x="190.869141" xlink:href="#DejaVuSans-40"></use>
      <use x="229.882812" xlink:href="#DejaVuSans-32"></use>
      <use x="261.669922" xlink:href="#DejaVuSans-82"></use>
      <use x="326.652344" xlink:href="#DejaVuSans-101"></use>
      <use x="388.175781" xlink:href="#DejaVuSans-99"></use>
      <use x="443.15625" xlink:href="#DejaVuSans-97"></use>
      <use x="504.435547" xlink:href="#DejaVuSans-108"></use>
      <use x="532.21875" xlink:href="#DejaVuSans-108"></use>
      <use x="560.001953" xlink:href="#DejaVuSans-32"></use>
      <use x="591.789062" xlink:href="#DejaVuSans-41"></use>
     </g>
    </g>
   </g>
   <g id="line2d_17">
    <path clip-path="url(#p14b7ad9762)" d="M 43.78125 228.439219 
L 43.891754 227.325795 
L 43.899647 227.237893 
L 44.006205 225.860763 
L 44.025938 225.772861 
L 44.136442 224.747339 
L 44.152228 224.659437 
L 44.262732 223.956222 
L 44.290358 223.86832 
L 44.400863 222.842798 
L 44.420596 222.813497 
L 44.523207 221.641472 
L 44.546886 221.55357 
L 44.65739 220.879655 
L 44.692909 220.791753 
L 44.799467 220.000636 
L 44.823146 219.912734 
L 44.933651 219.561127 
L 44.941544 219.473225 
L 45.048101 219.033715 
L 45.091514 218.945814 
L 45.202018 217.890991 
L 45.221751 217.86169 
L 45.332255 217.187776 
L 45.355934 217.099874 
L 45.446706 216.894769 
L 45.478278 216.836168 
L 45.561156 216.250155 
L 45.628248 216.162253 
L 45.738752 215.312535 
L 45.778218 215.253934 
L 45.880829 214.63862 
L 45.912402 214.550719 
L 46.022906 214.16981 
L 46.042639 214.111209 
L 46.14525 213.525196 
L 46.164983 213.525196 
L 46.263647 212.997785 
L 46.29522 212.909883 
L 46.401777 212.206668 
L 46.42151 212.118766 
L 46.532014 211.474152 
L 46.55964 211.38625 
L 46.622786 210.946741 
L 46.697771 210.858839 
L 46.808275 209.833317 
L 46.828008 209.745415 
L 46.938512 209.305905 
L 46.946405 209.218003 
L 47.049016 208.866396 
L 47.072696 208.778494 
L 47.175307 208.221782 
L 47.206879 208.16318 
L 47.313437 207.606468 
L 47.33317 207.518567 
L 47.435781 207.137658 
L 47.48314 207.108358 
L 47.577857 206.375842 
L 47.60943 206.317241 
L 47.712041 205.936332 
L 47.723881 205.936332 
L 47.818599 204.969411 
L 47.865958 204.91081 
L 47.976462 204.442 
L 48.015928 204.354098 
L 48.126432 203.592282 
L 48.18563 203.50438 
L 48.296135 203.06487 
L 48.323761 203.03557 
L 48.430318 202.56676 
L 48.465837 202.478858 
L 48.576342 202.010047 
L 48.627647 201.922146 
L 48.738151 201.570538 
L 48.761831 201.511937 
L 48.872335 200.896623 
L 48.892068 200.808722 
L 48.978892 200.339911 
L 49.042038 200.252009 
L 49.124916 199.959003 
L 49.199901 199.871101 
L 49.306458 199.402291 
L 49.353817 199.314389 
L 49.452482 198.845579 
L 49.491947 198.757677 
L 49.598505 198.40607 
L 49.630078 198.347468 
L 49.728742 197.96656 
L 49.795834 197.907959 
L 49.898445 197.439149 
L 49.957644 197.351247 
L 50.052361 196.823835 
L 50.115507 196.765234 
L 50.222064 196.12062 
L 50.300996 196.032718 
L 50.407553 195.65181 
L 50.435179 195.563908 
L 50.522004 195.212301 
L 50.577256 195.124399 
L 50.675921 194.831392 
L 50.743013 194.772791 
L 50.845624 194.216079 
L 50.87325 194.128177 
L 50.956128 193.835171 
L 51.019273 193.747269 
L 51.121884 193.395661 
L 51.14951 193.33706 
L 51.252121 192.985452 
L 51.28764 192.897551 
L 51.390251 192.282237 
L 51.445503 192.194335 
L 51.500755 191.989231 
L 51.595473 191.901329 
L 51.694138 191.608323 
L 51.729657 191.520421 
L 51.836214 190.993009 
L 51.859894 190.993009 
L 51.958558 190.494899 
L 52.001971 190.465598 
L 52.084849 189.820984 
L 52.136154 189.733082 
L 52.222979 189.234971 
L 52.32559 189.147069 
L 52.436094 188.736861 
L 52.518972 188.648959 
L 52.62553 188.180149 
L 52.716301 188.092247 
L 52.818912 187.682038 
L 52.870218 187.594136 
L 52.976775 187.125326 
L 53.035974 187.066724 
L 53.142532 186.510012 
L 53.253036 186.422111 
L 53.33986 186.217006 
L 53.418792 186.129104 
L 53.52535 185.865399 
L 53.560869 185.777497 
L 53.655587 185.308686 
L 53.730572 185.220785 
L 53.809503 184.927778 
L 53.888435 184.839876 
L 53.987099 184.664073 
L 54.054191 184.576171 
L 54.144962 184.195262 
L 54.196268 184.107361 
L 54.306772 183.755753 
L 54.322558 183.697152 
L 54.425169 183.023237 
L 54.476475 182.935335 
L 54.583032 182.583728 
L 54.622498 182.495826 
L 54.725109 182.056316 
L 54.780361 181.968414 
L 54.890865 181.558205 
L 54.906652 181.528905 
L 55.00137 181.235898 
L 55.048728 181.147997 
L 55.155286 180.679186 
L 55.210538 180.591285 
L 55.313149 180.474082 
L 55.368401 180.38618 
L 55.411814 180.181076 
L 55.545997 180.093174 
L 55.652555 179.653664 
L 55.668341 179.624364 
L 55.743326 179.331357 
L 55.818311 179.243455 
L 55.920922 178.97975 
L 55.972228 178.891848 
L 56.070892 178.481639 
L 56.134037 178.393737 
L 56.228755 178.012829 
L 56.272167 177.954228 
L 56.382672 177.631921 
L 56.430031 177.544019 
L 56.536588 177.368215 
L 56.607627 177.280313 
L 56.702344 176.870104 
L 56.75365 176.782202 
L 56.836528 176.577098 
L 56.954925 176.489196 
L 57.049643 176.254791 
L 57.116735 176.166889 
L 57.219346 175.932484 
L 57.250919 175.844582 
L 57.34169 175.463674 
L 57.416675 175.375772 
L 57.523232 175.082765 
L 57.606111 174.994864 
L 57.688989 174.701857 
L 57.783707 174.613955 
L 57.894211 174.35025 
L 57.925783 174.262348 
L 58.036288 173.91074 
L 58.099433 173.852139 
L 58.209937 173.383329 
L 58.221777 173.324727 
L 58.316495 172.855917 
L 58.39148 172.768015 
L 58.498037 172.240604 
L 58.545396 172.152702 
L 58.648007 171.801095 
L 58.722992 171.713193 
L 58.81771 171.449487 
L 58.849282 171.420186 
L 58.959787 170.980677 
L 59.015039 170.892775 
L 59.125543 170.629069 
L 59.161062 170.570468 
L 59.247887 170.365363 
L 59.318925 170.277462 
L 59.318925 170.21886 
L 59.480735 170.130958 
L 59.591239 169.837952 
L 59.674117 169.75005 
L 59.784621 169.486344 
L 59.847767 169.398443 
L 59.938538 168.958933 
L 59.997737 168.871031 
L 60.080615 168.519424 
L 60.12008 168.490123 
L 60.226638 168.138515 
L 60.266104 168.050613 
L 60.368715 167.786908 
L 60.423967 167.757607 
L 60.522631 167.347398 
L 60.59367 167.259496 
L 60.696281 167.054392 
L 60.723907 166.96649 
L 60.818625 166.585582 
L 60.850197 166.49768 
L 60.960701 166.204674 
L 61.035686 166.116772 
L 61.146191 165.823765 
L 61.221176 165.735863 
L 61.327733 165.208452 
L 61.367199 165.12055 
L 61.46981 164.856844 
L 61.576367 164.768943 
L 61.686872 164.446636 
L 61.702658 164.446636 
L 61.805269 164.007126 
L 61.899987 163.948525 
L 61.998651 163.655518 
L 62.120995 163.596917 
L 62.223606 163.303911 
L 62.365683 163.216009 
L 62.468294 162.893702 
L 62.677463 162.8058 
L 62.776127 162.542094 
L 62.847165 162.454192 
L 62.95767 162.131886 
L 63.052387 162.043984 
L 63.162892 161.809579 
L 63.206304 161.721677 
L 63.304968 161.457971 
L 63.435205 161.370069 
L 63.541763 161.164965 
L 63.600962 161.077063 
L 63.711466 160.901259 
L 63.908795 160.813357 
L 64.007459 160.549651 
L 64.062711 160.49105 
L 64.09823 160.344547 
L 64.232414 160.285946 
L 64.335025 159.934338 
L 64.374491 159.846436 
L 64.473155 159.670632 
L 64.55998 159.58273 
L 64.603392 159.260423 
L 64.729683 159.172522 
L 64.812561 159.026018 
L 64.974371 158.938116 
L 65.080928 158.61581 
L 65.13618 158.527908 
L 65.203272 158.440006 
L 65.28615 158.352104 
L 65.372975 158.029797 
L 65.483479 157.941895 
L 65.593983 157.560987 
L 65.641342 157.473085 
L 65.7479 157.209379 
L 65.826831 157.121477 
L 65.937335 156.740569 
L 66.004427 156.652667 
L 66.095199 156.418262 
L 66.253062 156.33036 
L 66.34778 156.125256 
L 66.48591 156.037354 
L 66.572734 155.802949 
L 66.659559 155.715047 
L 66.746384 155.42204 
L 66.805583 155.334139 
L 66.916087 155.041132 
L 66.995018 154.982531 
L 67.089736 154.836028 
L 67.129202 154.777427 
L 67.216027 154.543021 
L 67.298905 154.45512 
L 67.401516 154.191414 
L 67.484394 154.103512 
L 67.594898 153.722604 
L 67.701456 153.634702 
L 67.808013 153.283094 
L 67.855372 153.253794 
L 67.965876 152.872885 
L 68.032968 152.784983 
L 68.143472 152.404075 
L 68.25003 152.374775 
L 68.352641 152.16967 
L 68.447359 152.111069 
L 68.534183 151.905964 
L 68.695993 151.818063 
L 68.806497 151.349252 
L 68.83807 151.290651 
L 68.948574 151.114847 
L 69.015666 151.026945 
L 69.12617 150.733939 
L 69.185369 150.646037 
L 69.284033 150.265129 
L 69.410324 150.177227 
L 69.516881 149.884221 
L 69.599759 149.796319 
L 69.686584 149.620515 
L 69.804981 149.532613 
L 69.915485 149.268907 
L 69.998364 149.181006 
L 70.065455 149.063803 
L 70.235158 148.975901 
L 70.321983 148.712195 
L 70.408808 148.624294 
L 70.475899 148.419189 
L 70.582457 148.331287 
L 70.669282 148.096882 
L 70.866611 148.038281 
L 70.953435 147.833176 
L 71.079726 147.745275 
L 71.166551 147.54017 
L 71.217856 147.510869 
L 71.32836 147.247164 
L 71.438864 147.159262 
L 71.529636 146.954157 
L 71.64014 146.895556 
L 71.750644 146.514648 
L 71.79011 146.426746 
L 71.884828 146.104439 
L 71.959813 146.045838 
L 72.022958 145.899335 
L 72.097943 145.811433 
L 72.2045 145.518426 
L 72.322898 145.459825 
L 72.413669 145.166819 
L 72.512333 145.078917 
L 72.595212 144.903113 
L 72.682036 144.815211 
L 72.768861 144.610107 
L 72.899098 144.522205 
L 73.009602 144.3171 
L 73.037228 144.229199 
L 73.143786 144.170597 
L 73.218771 144.082695 
L 73.313489 143.84829 
L 73.39242 143.760388 
L 73.495031 143.37948 
L 73.558176 143.291578 
L 73.605535 143.145075 
L 73.71604 143.057173 
L 73.779185 142.881369 
L 73.877849 142.793468 
L 73.937048 142.705566 
L 74.11859 142.617664 
L 74.229095 142.295357 
L 74.276454 142.207455 
L 74.379065 141.914449 
L 74.434317 141.826547 
L 74.532981 141.562841 
L 74.59218 141.50424 
L 74.698737 141.240534 
L 74.769776 141.152632 
L 74.781615 141.094031 
L 75.03025 141.006129 
L 75.121021 140.801024 
L 75.267044 140.713123 
L 75.365709 140.449417 
L 75.405175 140.361515 
L 75.495946 140.156411 
L 75.574877 140.097809 
L 75.685382 139.922005 
L 75.760367 139.863404 
L 75.870871 139.599698 
L 75.926123 139.511797 
L 76.001108 139.160189 
L 76.095826 139.072287 
L 76.162918 138.867183 
L 76.308941 138.779281 
L 76.407605 138.515575 
L 76.49443 138.427673 
L 76.577308 138.222569 
L 76.743064 138.134667 
L 76.841729 137.958863 
L 76.893034 137.870961 
L 77.003538 137.607255 
L 77.10615 137.519354 
L 77.204814 137.402151 
L 77.342944 137.34355 
L 77.38241 137.197047 
L 77.524487 137.138445 
L 77.634991 136.816138 
L 77.678403 136.728236 
L 77.788907 136.523132 
L 77.816533 136.43523 
L 77.903358 136.259426 
L 77.986236 136.171524 
L 78.080954 136.025021 
L 78.317749 135.937119 
L 78.341428 135.819917 
L 78.590063 135.732015 
L 78.69662 135.409708 
L 78.811071 135.321806 
L 78.878163 135.146002 
L 79.059705 135.0581 
L 79.150477 134.882297 
L 79.229408 134.794395 
L 79.320179 134.55999 
L 79.387271 134.472088 
L 79.478043 134.325584 
L 79.545134 134.237683 
L 79.620119 134.091179 
L 79.718784 134.003278 
L 79.821395 133.768872 
L 79.947685 133.680971 
L 80.02267 133.563768 
L 80.168694 133.475866 
L 80.259465 133.21216 
L 80.354183 133.124259 
L 80.444954 132.919154 
L 80.571244 132.831252 
L 80.650176 132.596847 
L 80.740947 132.508945 
L 80.808039 132.333141 
L 80.91065 132.24524 
L 81.009315 131.952233 
L 81.068513 131.864331 
L 81.163231 131.542024 
L 81.250056 131.454122 
L 81.34872 131.307619 
L 81.411865 131.219717 
L 81.52237 130.86811 
L 81.632874 130.780208 
L 81.735485 130.487202 
L 81.893348 130.3993 
L 81.976226 130.223496 
L 82.102516 130.135594 
L 82.165662 129.930489 
L 82.37483 129.842588 
L 82.469548 129.608183 
L 82.587946 129.520281 
L 82.66293 129.432379 
L 82.733969 129.344477 
L 82.820794 129.168673 
L 83.002336 129.080771 
L 83.108894 128.846366 
L 83.168092 128.758464 
L 83.254917 128.641262 
L 83.432513 128.55336 
L 83.515391 128.318955 
L 83.578536 128.231053 
L 83.677201 127.996648 
L 83.748239 127.908746 
L 83.842957 127.674341 
L 84.063965 127.586439 
L 84.15079 127.527838 
L 84.340226 127.439936 
L 84.430997 127.234831 
L 84.553341 127.146929 
L 84.655952 127.029727 
L 84.76251 126.941825 
L 84.853281 126.824622 
L 84.940106 126.73672 
L 85.046663 126.502315 
L 85.208473 126.414413 
L 85.311084 126.23861 
L 85.417641 126.150708 
L 85.48868 125.974904 
L 85.622864 125.887002 
L 85.670222 125.7698 
L 85.859658 125.681898 
L 85.970162 125.506094 
L 86.135919 125.418192 
L 86.20301 125.300989 
L 86.317461 125.213088 
L 86.3885 125.037284 
L 86.581882 124.978682 
L 86.656867 124.832179 
L 86.948914 124.744277 
L 87.055471 124.656375 
L 87.106777 124.568474 
L 87.169922 124.480572 
L 87.268586 124.39267 
L 87.323838 124.275467 
L 87.505381 124.187565 
L 87.600099 124.041062 
L 87.765855 123.95316 
L 87.856626 123.660154 
L 87.975024 123.601553 
L 88.081581 123.425749 
L 88.14078 123.337847 
L 88.239444 123.162043 
L 88.357842 123.074141 
L 88.448613 122.869037 
L 88.56701 122.781135 
L 88.669621 122.634632 
L 88.720927 122.54673 
L 88.819591 122.312325 
L 88.922202 122.224423 
L 89.012974 122.07792 
L 89.257661 121.990018 
L 89.340539 121.755613 
L 89.482616 121.667711 
L 89.581281 121.579809 
L 89.74309 121.491907 
L 89.798342 121.316103 
L 89.975938 121.228201 
L 90.086443 121.110999 
L 90.224573 121.023097 
L 90.335077 120.935195 
L 90.437688 120.847293 
L 90.540299 120.583587 
L 90.702109 120.524986 
L 90.812613 120.319882 
L 90.899437 120.23198 
L 91.002049 120.085477 
L 91.124392 119.997575 
L 91.163858 119.880372 
L 91.396706 119.79247 
L 91.499317 119.587366 
L 91.68086 119.499464 
L 91.775578 119.32366 
L 91.81899 119.265059 
L 91.901868 119.089255 
L 92.028159 119.001353 
L 92.11893 118.913451 
L 92.24522 118.825549 
L 92.332045 118.532543 
L 92.438603 118.444641 
L 92.541214 118.239537 
L 92.639878 118.151635 
L 92.742489 117.91723 
L 92.888513 117.829328 
L 92.999017 117.741426 
L 93.117414 117.653524 
L 93.180559 117.536322 
L 93.413407 117.47772 
L 93.523911 117.331217 
L 93.563377 117.243315 
L 93.669935 116.833106 
L 93.705454 116.745204 
L 93.800172 116.569401 
L 93.859371 116.481499 
L 93.88305 116.393597 
L 94.03302 116.305695 
L 94.123791 116.159192 
L 94.202723 116.07129 
L 94.30928 115.924787 
L 94.392158 115.895486 
L 94.431624 115.778284 
L 94.553968 115.690382 
L 94.652633 115.485277 
L 94.814442 115.397375 
L 94.913107 115.250872 
L 95.063077 115.16297 
L 95.157795 114.987166 
L 95.280138 114.899265 
L 95.386696 114.664859 
L 95.631384 114.576958 
L 95.726102 114.459755 
L 95.864232 114.371853 
L 95.887911 114.313252 
L 96.136546 114.22535 
L 96.211531 114.078847 
L 96.274676 113.990945 
L 96.329928 113.932344 
L 96.479898 113.844442 
L 96.479898 113.78584 
L 96.740372 113.697939 
L 96.82325 113.492834 
L 96.925861 113.404932 
L 97.032419 113.258429 
L 97.063991 113.170527 
L 97.166602 113.024024 
L 97.269213 112.936122 
L 97.375771 112.81892 
L 97.431023 112.731018 
L 97.521794 112.584515 
L 97.71123 112.496613 
L 97.798055 112.320809 
L 97.896719 112.291508 
L 97.995384 112.086404 
L 98.208499 111.998502 
L 98.295324 111.822698 
L 98.500546 111.734796 
L 98.595263 111.617594 
L 98.7255 111.529692 
L 98.832058 111.47109 
L 99.017547 111.383189 
L 99.128051 111.060882 
L 99.167517 110.97298 
L 99.266182 110.621372 
L 99.416152 110.562771 
L 99.514816 110.474869 
L 99.629267 110.386967 
L 99.700305 110.269764 
L 99.779237 110.211163 
L 99.806863 110.123261 
L 99.984459 110.035359 
L 100.07523 109.830255 
L 100.169948 109.742353 
L 100.209414 109.683752 
L 100.410689 109.59585 
L 100.5133 109.420046 
L 100.619858 109.332144 
L 100.722469 109.185641 
L 100.840866 109.097739 
L 100.900065 109.039138 
L 101.038195 108.951236 
L 101.121073 108.746132 
L 101.294722 108.65823 
L 101.397333 108.335923 
L 101.448639 108.248021 
L 101.535464 108.101518 
L 101.630181 108.013616 
L 101.740686 107.808511 
L 101.823564 107.720609 
L 101.914335 107.544806 
L 102.052465 107.456904 
L 102.15113 107.251799 
L 102.281367 107.163897 
L 102.391871 106.900192 
L 102.573413 106.81229 
L 102.601039 106.753688 
L 102.794422 106.665787 
L 102.881246 106.519283 
L 103.011484 106.431382 
L 103.114095 106.34348 
L 103.196973 106.255578 
L 103.307477 106.079774 
L 103.398248 105.991872 
L 103.508752 105.786768 
L 103.62715 105.698866 
L 103.733707 105.493761 
L 103.859998 105.405859 
L 103.966555 105.288657 
L 104.06522 105.230056 
L 104.148098 104.937049 
L 104.234923 104.849147 
L 104.333587 104.673344 
L 104.412518 104.585442 
L 104.50329 104.49754 
L 104.566435 104.438938 
L 104.669046 104.263135 
L 104.724298 104.175233 
L 104.724298 104.145932 
L 104.921627 104.05803 
L 104.968986 103.911527 
L 105.059757 103.823625 
L 105.118956 103.735723 
L 105.37943 103.647821 
L 105.458362 103.559919 
L 105.545186 103.472018 
L 105.545186 103.413416 
L 105.714889 103.325514 
L 105.797767 103.237612 
L 105.963523 103.149711 
L 106.034562 103.061809 
L 106.227944 102.973907 
L 106.338448 102.856704 
L 106.370021 102.768802 
L 106.480525 102.622299 
L 106.602869 102.534397 
L 106.713373 102.417195 
L 106.800198 102.329293 
L 106.906755 102.124188 
L 107.021206 102.036287 
L 107.025153 101.977685 
L 107.265894 101.889783 
L 107.372452 101.772581 
L 107.526368 101.684679 
L 107.632926 101.538176 
L 107.735537 101.450274 
L 107.771056 101.333071 
L 107.960492 101.245169 
L 108.035477 101.069366 
L 108.106515 101.010764 
L 108.185446 100.717758 
L 108.323577 100.629856 
L 108.414348 100.424752 
L 108.576158 100.33685 
L 108.670875 100.102445 
L 108.805059 100.014543 
L 108.915563 99.926641 
L 108.978709 99.838739 
L 109.010281 99.692236 
L 109.235236 99.604334 
L 109.34574 99.516432 
L 109.381259 99.457831 
L 109.487817 99.311328 
L 109.590428 99.252726 
L 109.689092 99.076923 
L 109.80749 98.989021 
L 109.894314 98.871818 
L 110.119269 98.813217 
L 110.229774 98.666714 
L 110.269239 98.578812 
L 110.379744 98.432309 
L 110.612592 98.373707 
L 110.691523 98.227204 
L 110.8336 98.139302 
L 110.932264 97.758394 
L 111.13354 97.670492 
L 111.240097 97.523989 
L 111.326922 97.436087 
L 111.429533 97.289584 
L 111.662381 97.201682 
L 111.749206 97.084479 
L 111.883389 96.996578 
L 111.978107 96.820774 
L 112.084665 96.732872 
L 112.183329 96.527767 
L 112.368819 96.469166 
L 112.467483 96.234761 
L 112.593773 96.146859 
L 112.684545 95.883154 
L 112.767423 95.795252 
L 112.822675 95.70735 
L 113.02395 95.619448 
L 113.134455 95.531546 
L 113.260745 95.443644 
L 113.371249 95.23854 
L 113.398875 95.179938 
L 113.493593 95.004135 
L 113.663296 94.916233 
L 113.698815 94.857631 
L 113.876411 94.769729 
L 113.982969 94.593926 
L 114.065847 94.506024 
L 114.164511 94.359521 
L 114.413146 94.271619 
L 114.456558 94.183717 
L 114.697299 94.095815 
L 114.79991 93.949312 
L 114.847269 93.86141 
L 114.847269 93.832109 
L 115.024865 93.773508 
L 115.091957 93.539103 
L 115.24982 93.451201 
L 115.309019 93.304698 
L 115.557653 93.216796 
L 115.668157 93.128894 
L 115.786555 93.040992 
L 115.885219 92.865188 
L 116.019403 92.777286 
L 116.110174 92.542881 
L 116.500885 92.454979 
L 116.603496 92.308476 
L 116.974474 92.220574 
L 117.021833 92.161973 
L 117.140231 92.103372 
L 117.199429 91.839666 
L 117.392812 91.751764 
L 117.412544 91.663862 
L 117.645392 91.57596 
L 117.755897 91.370856 
L 117.953226 91.282954 
L 118.059783 91.048549 
L 118.134768 90.989948 
L 118.241326 90.843445 
L 118.375509 90.755543 
L 118.379456 90.696941 
L 118.596518 90.60904 
L 118.667556 90.550438 
L 118.896458 90.462536 
L 118.983282 90.286733 
L 119.074054 90.198831 
L 119.156932 90.081628 
L 119.362154 89.993726 
L 119.468711 89.876524 
L 119.595002 89.788622 
L 119.701559 89.70072 
L 119.733132 89.612818 
L 119.83969 89.466315 
L 120.013339 89.378413 
L 120.119897 89.290511 
L 120.297492 89.202609 
L 120.372477 89.114707 
L 120.538234 89.026805 
L 120.577699 88.968204 
L 120.905265 88.880302 
L 120.984197 88.704498 
L 121.28019 88.616596 
L 121.378855 88.470093 
L 121.540664 88.382191 
L 121.615649 88.264989 
L 121.734047 88.177087 
L 121.793245 88.030584 
L 121.970841 87.942682 
L 121.970841 87.884081 
L 122.247102 87.796179 
L 122.254995 87.737577 
L 122.641759 87.649676 
L 122.685172 87.591074 
L 122.874608 87.503172 
L 122.981165 87.268767 
L 123.087723 87.210166 
L 123.186387 87.092964 
L 123.253479 87.005062 
L 123.340304 86.770657 
L 123.446861 86.712055 
L 123.490274 86.624153 
L 123.734961 86.536251 
L 123.841519 86.419049 
L 123.93229 86.331147 
L 123.975703 86.272546 
L 124.169085 86.184644 
L 124.279589 86.00884 
L 124.476918 85.920938 
L 124.476918 85.891638 
L 124.642674 85.833036 
L 124.733445 85.715834 
L 125.147836 85.627932 
L 125.222821 85.510729 
L 125.483295 85.422827 
L 125.562227 85.276324 
L 125.763502 85.188422 
L 125.862167 85.012619 
L 125.99635 84.924717 
L 126.102908 84.778213 
L 126.252878 84.690312 
L 126.339702 84.60241 
L 126.643589 84.514508 
L 126.730414 84.338704 
L 126.856704 84.250802 
L 126.947475 84.074998 
L 127.152697 83.987096 
L 127.20795 83.811293 
L 127.47237 83.723391 
L 127.563141 83.576888 
L 127.681539 83.488986 
L 127.776257 83.283881 
L 128.009105 83.195979 
L 128.091983 83.108077 
L 128.174861 83.049476 
L 128.277472 82.815071 
L 128.439282 82.727169 
L 128.51032 82.639267 
L 128.841833 82.551365 
L 128.94839 82.346261 
L 129.256223 82.258359 
L 129.358834 82.082555 
L 129.441712 81.994653 
L 129.552216 81.936052 
L 129.733759 81.84815 
L 129.785065 81.730948 
L 130.156043 81.643046 
L 130.2626 81.467242 
L 130.467822 81.37934 
L 130.558594 81.262137 
L 130.637525 81.203536 
L 130.748029 81.086334 
L 130.771709 81.057033 
L 130.870373 80.91053 
L 130.984824 80.822628 
L 131.087435 80.588223 
L 131.229512 80.500321 
L 131.340016 80.353818 
L 131.58865 80.265916 
L 131.679422 80.207315 
L 131.778086 80.119413 
L 131.864911 80.00221 
L 131.995148 79.914308 
L 132.105652 79.797106 
L 132.279301 79.709204 
L 132.322714 79.592001 
L 132.681852 79.504099 
L 132.780517 79.416198 
L 132.922594 79.328296 
L 132.985739 79.211093 
L 133.112029 79.123191 
L 133.171228 79.06459 
L 133.415916 78.976688 
L 133.415916 78.947387 
L 133.842146 78.859486 
L 133.865826 78.800884 
L 134.130246 78.712982 
L 134.130246 78.683682 
L 134.351255 78.59578 
L 134.418346 78.390675 
L 134.639355 78.302774 
L 134.682767 78.185571 
L 134.844577 78.097669 
L 134.923508 77.980467 
L 135.037959 77.892565 
L 135.097158 77.804663 
L 135.337899 77.716761 
L 135.436563 77.599558 
L 135.503655 77.511656 
L 135.614159 77.394454 
L 135.783862 77.306552 
L 135.894366 77.247951 
L 136.174573 77.160049 
L 136.273238 77.042846 
L 136.379795 76.954944 
L 136.45478 76.867042 
L 136.644216 76.779141 
L 136.703415 76.661938 
L 136.932316 76.574036 
L 136.987568 76.486134 
L 137.330921 76.398232 
L 137.425638 76.31033 
L 137.678219 76.222429 
L 137.678219 76.193128 
L 137.903174 76.134527 
L 137.997892 76.017324 
L 138.10445 75.929422 
L 138.187328 75.753618 
L 138.400443 75.665717 
L 138.455695 75.548514 
L 138.751688 75.460612 
L 138.787208 75.402011 
L 139.193705 75.314109 
L 139.296316 75.226207 
L 139.55679 75.138305 
L 139.639668 75.079704 
L 140.085632 74.991802 
L 140.152723 74.9039 
L 140.413197 74.815998 
L 140.519755 74.669495 
L 140.728924 74.581593 
L 140.839428 74.317887 
L 140.922306 74.229985 
L 141.024917 73.96628 
L 141.324857 73.878378 
L 141.431414 73.643973 
L 141.553758 73.556071 
L 141.636636 73.468169 
L 141.861591 73.380267 
L 141.901057 73.233764 
L 142.216783 73.204463 
L 142.327288 73.05796 
L 142.390433 72.970058 
L 142.49699 72.882156 
L 142.662747 72.794254 
L 142.706159 72.735653 
L 142.907434 72.647751 
L 142.990313 72.530549 
L 143.08503 72.442647 
L 143.112656 72.354745 
L 143.321825 72.266843 
L 143.321825 72.237542 
L 143.53494 72.149641 
L 143.574406 72.061739 
L 143.787521 71.973837 
L 143.886186 71.739432 
L 143.929598 71.68083 
L 143.992743 71.505027 
L 144.395294 71.417125 
L 144.399241 71.358523 
L 144.639982 71.270622 
L 144.639982 71.241321 
L 145.089892 71.153419 
L 145.117518 71.065517 
L 145.421404 70.977615 
L 145.480603 70.919014 
L 145.630573 70.860413 
L 145.725291 70.655308 
L 146.005498 70.567406 
L 146.116002 70.391603 
L 146.431728 70.303701 
L 146.502766 70.186498 
L 146.790867 70.127897 
L 146.846119 69.981394 
L 147.059234 69.893492 
L 147.157898 69.717688 
L 147.303922 69.659087 
L 147.406533 69.512584 
L 147.497304 69.424682 
L 147.599915 69.36608 
L 147.947214 69.278178 
L 148.022199 69.190277 
L 148.195848 69.102375 
L 148.302406 68.985172 
L 148.472109 68.89727 
L 148.519467 68.809368 
L 148.953591 68.721466 
L 149.044362 68.545663 
L 149.344302 68.457761 
L 149.419287 68.399159 
L 149.620563 68.311258 
L 149.620563 68.281957 
L 150.050739 68.194055 
L 150.157297 68.047552 
L 150.244122 67.95965 
L 150.307267 67.842447 
L 150.555901 67.754546 
L 150.555901 67.695944 
L 150.800589 67.608042 
L 150.816375 67.549441 
L 151.06501 67.461539 
L 151.147888 67.315036 
L 151.345217 67.227134 
L 151.432042 67.109932 
L 151.633317 67.02203 
L 151.716195 66.904827 
L 151.949043 66.816925 
L 151.968776 66.729023 
L 152.339754 66.641121 
L 152.343701 66.58252 
L 152.671267 66.494618 
L 152.762038 66.406716 
L 152.868596 66.318814 
L 152.971207 66.260213 
L 153.109337 66.201612 
L 153.211948 66.11371 
L 153.373758 66.025808 
L 153.452689 65.908606 
L 153.689484 65.850004 
L 153.764469 65.703501 
L 154.095981 65.615599 
L 154.119661 65.556998 
L 154.368295 65.469096 
L 154.46696 65.410495 
L 154.88135 65.322593 
L 154.991854 65.20539 
L 155.157611 65.117489 
L 155.185237 65.058887 
L 155.406245 64.970985 
L 155.500963 64.795182 
L 155.666719 64.70728 
L 155.666719 64.677979 
L 156.108736 64.590077 
L 156.215293 64.472875 
L 156.35737 64.414273 
L 156.396836 64.326371 
L 156.661257 64.23847 
L 156.712562 64.150568 
L 157.071701 64.062666 
L 157.146686 63.974764 
L 157.217724 63.886862 
L 157.284816 63.769659 
L 157.474252 63.681757 
L 157.474252 63.652457 
L 157.742619 63.564555 
L 157.793924 63.505954 
L 158.12149 63.418052 
L 158.231994 63.300849 
L 158.303033 63.242248 
L 158.334605 63.125045 
L 158.887126 63.037144 
L 158.973951 62.978542 
L 159.435701 62.89064 
L 159.518579 62.832039 
L 159.889557 62.744137 
L 159.948756 62.685536 
L 160.122405 62.597634 
L 160.205283 62.509732 
L 160.355253 62.42183 
L 160.418398 62.275327 
L 160.603887 62.187425 
L 160.603887 62.158125 
L 161.144569 62.070223 
L 161.255073 61.894419 
L 161.436615 61.806517 
L 161.539226 61.660014 
L 161.819433 61.572112 
L 161.918098 61.396308 
L 162.202251 61.308406 
L 162.304862 61.103302 
L 162.427206 61.0447 
L 162.514031 60.927498 
L 162.723199 60.839596 
L 162.770558 60.722394 
L 162.995513 60.634492 
L 163.086285 60.54659 
L 163.512515 60.458688 
L 163.57566 60.400087 
L 163.690111 60.312185 
L 163.690111 60.282884 
L 163.922959 60.194982 
L 163.938745 60.136381 
L 164.218952 60.048479 
L 164.31367 59.931276 
L 164.732007 59.843375 
L 164.826725 59.755473 
L 165.170077 59.667571 
L 165.22533 59.550368 
L 165.422658 59.462466 
L 165.422658 59.433166 
L 165.77785 59.374564 
L 165.876515 59.228061 
L 166.042271 59.140159 
L 166.148829 59.022957 
L 166.338264 58.935055 
L 166.405356 58.847153 
L 166.942091 58.759251 
L 167.052595 58.642049 
L 167.368321 58.554147 
L 167.368321 58.524846 
L 167.55381 58.466245 
L 167.660368 58.319742 
L 167.794551 58.23184 
L 167.84191 58.143938 
L 168.126064 58.056036 
L 168.181316 57.938833 
L 168.30366 57.850931 
L 168.30366 57.821631 
L 168.57992 57.733729 
L 168.690424 57.557925 
L 168.986418 57.470023 
L 169.092975 57.352821 
L 169.452114 57.264919 
L 169.511313 57.177017 
L 169.759947 57.089115 
L 169.815199 57.030514 
L 170.044101 56.942612 
L 170.150658 56.825409 
L 170.426919 56.737507 
L 170.450598 56.678906 
L 170.770271 56.591004 
L 170.872882 56.473802 
L 171.437242 56.3859 
L 171.445136 56.327299 
L 171.677984 56.239397 
L 171.689823 56.180795 
L 172.060802 56.092893 
L 172.060802 56.063593 
L 172.368635 55.975691 
L 172.455459 55.858488 
L 172.585696 55.770586 
L 172.585696 55.741286 
L 172.932995 55.653384 
L 172.932995 55.624083 
L 173.260561 55.536181 
L 173.260561 55.506881 
L 173.517089 55.418979 
L 173.611807 55.272476 
L 174.053823 55.184574 
L 174.053823 55.155273 
L 174.286671 55.067371 
L 174.33403 54.979469 
L 174.464267 54.891567 
L 174.539252 54.774365 
L 174.882604 54.686463 
L 174.929963 54.627862 
L 175.340407 54.53996 
L 175.431179 54.364156 
L 175.849516 54.276254 
L 175.900821 54.188352 
L 176.630938 54.10045 
L 176.630938 54.07115 
L 176.982184 53.983248 
L 176.982184 53.953947 
L 177.420254 53.866045 
L 177.491292 53.778143 
L 177.751766 53.690242 
L 177.795179 53.60234 
L 177.964881 53.514438 
L 178.039866 53.426536 
L 178.142477 53.338634 
L 178.213516 53.16283 
L 178.33586 53.074928 
L 178.367432 53.016327 
L 178.746304 52.928425 
L 178.746304 52.899124 
L 178.975205 52.811223 
L 178.975205 52.781922 
L 179.251466 52.723321 
L 179.251466 52.664719 
L 179.693482 52.576817 
L 179.784254 52.488916 
L 179.922384 52.401014 
L 180.028941 52.313112 
L 180.301255 52.22521 
L 180.328881 52.137308 
L 180.494637 52.049406 
L 180.54989 51.990805 
L 181.110304 51.902903 
L 181.220808 51.7857 
L 181.433923 51.697798 
L 181.520748 51.639197 
L 181.658878 51.551295 
L 181.658878 51.521995 
L 181.923298 51.434093 
L 182.00223 51.375491 
L 182.120627 51.28759 
L 182.187719 51.199688 
L 182.756026 51.111786 
L 182.858637 50.994583 
L 183.225669 50.906681 
L 183.332227 50.760178 
L 183.517716 50.701577 
L 183.549288 50.613675 
L 183.868961 50.525773 
L 183.963679 50.408571 
L 184.417535 50.320669 
L 184.520146 50.262067 
L 184.646437 50.174166 
L 184.756941 50.115564 
L 184.863499 50.027662 
L 184.863499 49.998362 
L 185.333141 49.91046 
L 185.423913 49.793257 
L 185.790944 49.705355 
L 185.806731 49.617453 
L 186.075098 49.529552 
L 186.075098 49.500251 
L 186.517115 49.412349 
L 186.525008 49.353748 
L 186.734176 49.265846 
L 186.809161 49.177944 
L 187.270911 49.090042 
L 187.270911 49.060741 
L 187.487973 49.00214 
L 187.543225 48.914238 
L 187.898417 48.826336 
L 188.004974 48.738434 
L 188.170731 48.650533 
L 188.229929 48.591931 
L 188.900847 48.504029 
L 189.003458 48.386827 
L 189.338917 48.298925 
L 189.441528 48.211023 
L 189.780934 48.123121 
L 189.844079 48.005919 
L 190.072981 47.918017 
L 190.183485 47.771514 
L 190.424226 47.683612 
L 190.518944 47.62501 
L 191.016213 47.537109 
L 191.075411 47.478507 
L 191.74633 47.390605 
L 191.74633 47.361305 
L 192.002857 47.273403 
L 192.002857 47.244102 
L 192.48434 47.1562 
L 192.583004 47.097599 
L 192.950036 47.009697 
L 192.965822 46.951096 
L 193.605168 46.863194 
L 193.605168 46.833893 
L 193.889321 46.745991 
L 193.909054 46.68739 
L 194.272139 46.599488 
L 194.37475 46.482286 
L 194.757568 46.394384 
L 194.844393 46.335783 
L 195.219318 46.247881 
L 195.278516 46.159979 
L 195.677121 46.072077 
L 195.783678 46.013476 
L 196.217802 45.925574 
L 196.217802 45.896273 
L 196.616406 45.808371 
L 196.616406 45.779071 
L 197.149194 45.691169 
L 197.216286 45.632567 
L 198.139785 45.544665 
L 198.191091 45.486064 
L 198.597588 45.398162 
L 198.597588 45.368862 
L 199.071177 45.28096 
L 199.118536 45.222358 
L 199.367171 45.134457 
L 199.41453 45.075855 
L 199.836813 44.987953 
L 199.836813 44.958653 
L 200.12886 44.870751 
L 200.239364 44.81215 
L 200.487999 44.724248 
L 200.503785 44.636346 
L 200.886603 44.548444 
L 200.930015 44.431241 
L 201.462803 44.343339 
L 201.462803 44.284738 
L 201.908766 44.196836 
L 202.007431 44.021033 
L 202.417875 43.933131 
L 202.5047 43.815928 
L 202.836212 43.728026 
L 202.836212 43.698726 
L 203.203244 43.610824 
L 203.313748 43.493621 
L 203.50713 43.405719 
L 203.526863 43.347118 
L 203.929414 43.259216 
L 203.98072 43.200615 
L 204.24514 43.112713 
L 204.284606 43.054112 
L 204.631905 42.96621 
L 204.651638 42.907608 
L 205.322556 42.819707 
L 205.429113 42.673203 
L 205.819825 42.585301 
L 205.879023 42.468099 
L 206.167123 42.380197 
L 206.269734 42.204393 
L 206.838042 42.116491 
L 206.838042 42.087191 
L 207.327417 41.999289 
L 207.437921 41.882086 
L 207.737861 41.794184 
L 207.737861 41.764884 
L 208.033855 41.676982 
L 208.033855 41.647681 
L 208.763971 41.559779 
L 208.799491 41.471877 
L 209.797975 41.383976 
L 209.904532 41.266773 
L 210.224205 41.178871 
L 210.334709 41.090969 
L 211.226636 41.003067 
L 211.333193 40.944466 
L 211.47527 40.856564 
L 211.562095 40.768662 
L 211.960699 40.68076 
L 211.988325 40.592858 
L 212.398769 40.504957 
L 212.442181 40.446355 
L 212.986809 40.358453 
L 212.986809 40.329153 
L 213.409093 40.241251 
L 213.488024 40.153349 
L 213.941881 40.065447 
L 214.040545 40.006846 
L 214.537814 39.918944 
L 214.636479 39.831042 
L 214.73909 39.74314 
L 214.766716 39.684539 
L 214.904846 39.596637 
L 214.904846 39.567336 
L 215.362649 39.479434 
L 215.390275 39.362232 
L 215.646802 39.27433 
L 215.733627 39.157127 
L 216.140124 39.069225 
L 216.195377 39.010624 
L 216.657126 38.922722 
L 216.747897 38.83482 
L 217.099143 38.746919 
L 217.174128 38.688317 
L 217.51748 38.600415 
L 217.51748 38.571115 
L 217.872672 38.483213 
L 217.931871 38.395311 
L 218.476498 38.307409 
L 218.571216 38.248808 
L 218.914568 38.160906 
L 218.973767 38.043703 
L 219.372371 37.955801 
L 219.411837 37.8972 
L 219.984091 37.809298 
L 220.086702 37.750697 
L 220.694475 37.662795 
L 220.710261 37.604194 
L 220.89575 37.516292 
L 220.89575 37.486991 
L 221.365393 37.399089 
L 221.365393 37.369789 
L 221.866608 37.281887 
L 221.866608 37.252586 
L 222.174441 37.164684 
L 222.174441 37.135384 
L 222.549366 37.047482 
L 222.608565 36.988881 
L 222.940077 36.900979 
L 222.98349 36.813077 
L 223.405773 36.725175 
L 223.492598 36.607972 
L 224.360845 36.52007 
L 224.360845 36.49077 
L 224.660785 36.402868 
L 224.743663 36.314966 
L 225.61191 36.227064 
L 225.61191 36.197763 
L 226.385439 36.109862 
L 226.48805 36.02196 
L 226.843242 35.934058 
L 226.886655 35.875456 
L 227.735169 35.787555 
L 227.818047 35.728953 
L 228.165346 35.641051 
L 228.165346 35.611751 
L 228.508698 35.553149 
L 228.55211 35.465248 
L 228.832317 35.377346 
L 228.832317 35.348045 
L 229.511129 35.260143 
L 229.578221 35.172241 
L 230.257032 35.084339 
L 230.257032 35.055039 
L 230.805606 34.967137 
L 230.805606 34.937836 
L 231.535723 34.879235 
L 231.535723 34.820634 
L 232.131656 34.732732 
L 232.131656 34.703431 
L 232.451329 34.615529 
L 232.475008 34.556928 
L 232.684177 34.469026 
L 232.763108 34.381124 
L 233.41824 34.293222 
L 233.512958 34.234621 
L 233.808951 34.146719 
L 233.808951 34.117418 
L 234.132571 34.029517 
L 234.148357 33.970915 
L 234.633786 33.883013 
L 234.633786 33.824412 
L 235.004764 33.73651 
L 235.063963 33.590007 
L 235.573072 33.502105 
L 235.573072 33.472805 
L 236.145325 33.384903 
L 236.145325 33.355602 
L 236.583395 33.2677 
L 236.662327 33.209099 
L 237.092504 33.121197 
L 237.092504 33.091896 
L 237.550307 33.003994 
L 237.562147 32.945393 
L 237.8463 32.857491 
L 237.8463 32.828191 
L 238.094935 32.740289 
L 238.181759 32.681687 
L 238.631669 32.593786 
L 238.659295 32.535184 
L 238.828998 32.447282 
L 238.828998 32.417982 
L 239.105258 32.33008 
L 239.105258 32.300779 
L 239.389412 32.212877 
L 239.47229 32.124975 
L 239.618313 32.037073 
L 239.630153 31.978472 
L 240.22214 31.89057 
L 240.320804 31.831969 
L 240.573385 31.744067 
L 240.573385 31.714767 
L 240.849646 31.626865 
L 240.94831 31.509662 
L 241.189051 31.42176 
L 241.252196 31.333858 
L 241.512671 31.245956 
L 241.512671 31.216656 
L 241.82445 31.128754 
L 241.840236 31.070153 
L 242.337505 30.982251 
L 242.392757 30.865048 
L 242.791362 30.777146 
L 242.830827 30.718545 
L 243.280737 30.630643 
L 243.296524 30.572042 
L 243.920083 30.48414 
L 243.975335 30.425539 
L 244.598894 30.337637 
L 244.598894 30.308336 
L 245.313225 30.220434 
L 245.415836 30.132532 
L 246.197258 30.04463 
L 246.288029 29.986029 
L 246.623488 29.898127 
L 246.730046 29.810225 
L 246.923428 29.722323 
L 247.026039 29.663722 
L 247.349659 29.57582 
L 247.42859 29.517219 
L 248.103455 29.429317 
L 248.103455 29.400016 
L 248.624403 29.312115 
L 248.624403 29.282814 
L 248.975648 29.194912 
L 248.995381 29.136311 
L 249.914934 29.048409 
L 249.914934 29.019108 
L 250.175408 28.960507 
L 250.258286 28.872605 
L 250.743715 28.784703 
L 250.767395 28.696801 
L 251.1068 28.608899 
L 251.142319 28.550298 
L 251.923742 28.462396 
L 251.923742 28.433096 
L 252.28288 28.345194 
L 252.389438 28.257292 
L 252.902493 28.16939 
L 252.902493 28.140089 
L 253.245845 28.052187 
L 253.245845 28.022887 
L 253.514212 27.934985 
L 253.549732 27.876384 
L 254.173291 27.788482 
L 254.173291 27.759181 
L 254.587681 27.671279 
L 254.67056 27.612678 
L 255.033645 27.524776 
L 255.132309 27.436874 
L 255.582219 27.348972 
L 255.661151 27.26107 
L 256.115007 27.173168 
L 256.217618 27.085266 
L 256.742513 26.997365 
L 256.785925 26.938763 
L 257.283194 26.850861 
L 257.370019 26.79226 
L 257.851501 26.704358 
L 257.851501 26.675058 
L 258.609244 26.587156 
L 258.640816 26.528554 
L 259.264376 26.440653 
L 259.335414 26.352751 
L 259.923454 26.264849 
L 259.923454 26.235548 
L 260.894312 26.147646 
L 260.969297 26.089045 
L 261.45078 26.001143 
L 261.486299 25.88394 
L 262.082232 25.796039 
L 262.169057 25.737437 
L 262.409798 25.649535 
L 262.484783 25.532333 
L 262.792616 25.444431 
L 262.863654 25.356529 
L 263.692436 25.268627 
L 263.692436 25.239327 
L 264.027895 25.151425 
L 264.027895 25.122124 
L 264.572522 25.034222 
L 264.572522 25.004921 
L 265.022432 24.91702 
L 265.109257 24.829118 
L 265.661778 24.741216 
L 265.661778 24.711915 
L 266.111687 24.624013 
L 266.111687 24.594713 
L 266.857591 24.506811 
L 266.857591 24.47751 
L 267.733731 24.389608 
L 267.733731 24.360308 
L 268.503313 24.272406 
L 268.526993 24.213804 
L 269.10714 24.125902 
L 269.10714 24.096602 
L 269.880669 24.0087 
L 269.916188 23.950099 
L 270.334525 23.862197 
L 270.334525 23.832896 
L 271.415887 23.744994 
L 271.415887 23.715694 
L 272.003927 23.627792 
L 272.031554 23.56919 
L 273.393123 23.481289 
L 273.393123 23.451988 
L 274.24953 23.364086 
L 274.312675 23.305485 
L 274.644188 23.217583 
L 274.719173 23.10038 
L 275.240121 23.012478 
L 275.350625 22.924577 
L 276.329376 22.836675 
L 276.329376 22.807374 
L 276.767446 22.719472 
L 276.767446 22.690171 
L 277.020027 22.60227 
L 277.087119 22.543668 
L 277.50151 22.455766 
L 277.544922 22.397165 
L 278.042191 22.309263 
L 278.073764 22.192061 
L 278.559193 22.104159 
L 278.598658 22.045558 
L 279.447172 21.957656 
L 279.474799 21.899054 
L 280.307526 21.811152 
L 280.386458 21.752551 
L 281.337583 21.664649 
L 281.337583 21.635349 
L 282.241349 21.547447 
L 282.284762 21.459545 
L 283.010932 21.371643 
L 283.010932 21.342342 
L 283.401643 21.25444 
L 283.413483 21.195839 
L 283.962057 21.107937 
L 284.040989 21.020035 
L 284.550097 20.932133 
L 284.613242 20.844232 
L 285.296 20.75633 
L 285.363092 20.697728 
L 286.199766 20.609826 
L 286.199766 20.580526 
L 287.000922 20.492624 
L 287.000922 20.463323 
L 287.802077 20.375421 
L 287.829703 20.31682 
L 288.12175 20.228918 
L 288.145429 20.170317 
L 289.384654 20.082415 
L 289.43596 20.023814 
L 290.963285 19.935912 
L 291.034324 19.877311 
L 291.503966 19.789409 
L 291.503966 19.760108 
L 292.427466 19.672206 
L 292.427466 19.642906 
L 292.81423 19.555004 
L 292.81423 19.525703 
L 293.481202 19.437801 
L 293.481202 19.408501 
L 294.558617 19.320599 
L 294.558617 19.291298 
L 295.119031 19.203396 
L 295.16639 19.086194 
L 296.04253 18.998292 
L 296.121462 18.93969 
L 297.198878 18.851788 
L 297.305435 18.793187 
L 298.288133 18.705285 
L 298.311812 18.646684 
L 299.164273 18.558782 
L 299.255044 18.47088 
L 301.476967 18.382978 
L 301.476967 18.353678 
L 302.222871 18.265776 
L 302.222871 18.236475 
L 303.008239 18.148573 
L 303.031919 18.089972 
L 303.975151 18.00207 
L 304.006724 17.943469 
L 304.646069 17.855567 
L 304.646069 17.826266 
L 305.407759 17.738364 
L 305.514316 17.679763 
L 305.782683 17.591861 
L 305.782683 17.562561 
L 306.102356 17.474659 
L 306.102356 17.445358 
L 307.061374 17.357456 
L 307.061374 17.328156 
L 307.594162 17.240254 
L 307.594162 17.210953 
L 308.040126 17.123051 
L 308.040126 17.09375 
L 308.853121 17.005849 
L 308.951785 16.947247 
L 309.30303 16.888646 
L 309.30303 16.830045 
L 309.887124 16.742143 
L 309.993681 16.683542 
L 311.055311 16.59564 
L 311.07899 16.537038 
L 312.014329 16.449137 
L 312.014329 16.419836 
L 312.310322 16.331934 
L 312.40504 16.273333 
L 313.73109 16.185431 
L 313.73109 16.15613 
L 314.386222 16.068228 
L 314.496726 15.980326 
L 315.637287 15.892425 
L 315.720165 15.804523 
L 317.930249 15.716621 
L 317.930249 15.68732 
L 319.453627 15.628719 
L 319.453627 15.570118 
L 320.302142 15.482216 
L 320.302142 15.452915 
L 321.032258 15.365013 
L 321.032258 15.335712 
L 322.058369 15.247811 
L 322.121514 15.189209 
L 322.626676 15.101307 
L 322.626676 15.072007 
L 324.540766 14.984105 
L 324.540766 14.954804 
L 325.326135 14.866902 
L 325.326135 14.837602 
L 326.624559 14.7497 
L 326.636398 14.691099 
L 328.976719 14.603197 
L 328.976719 14.573896 
L 330.247517 14.485994 
L 330.350128 14.427393 
L 331.490689 14.339491 
L 331.490689 14.31019 
L 332.433921 14.222288 
L 332.433921 14.192988 
L 333.136411 14.105086 
L 333.136411 14.075785 
L 334.213827 13.987883 
L 334.249346 13.929282 
L 335.472785 13.84138 
L 335.472785 13.81208 
L 336.58572 13.724178 
L 336.696224 13.665576 
L 337.521059 13.577674 
L 337.521059 13.548374 
L 338.416932 13.460472 
L 338.416932 13.431171 
L 339.632478 13.343269 
L 339.742982 13.255368 
L 341.041406 13.167466 
L 341.041406 13.138165 
L 341.732057 13.050263 
L 341.732057 13.020962 
L 343.36594 12.933061 
L 343.401459 12.874459 
L 344.885372 12.786557 
L 344.885372 12.757257 
L 346.065399 12.669355 
L 346.065399 12.640054 
L 347.734801 12.552152 
L 347.734801 12.522852 
L 349.068744 12.43495 
L 349.127943 12.376349 
L 350.698681 12.288447 
L 350.730253 12.229845 
L 352.802206 12.141943 
L 352.802206 12.112643 
L 354.404517 12.024741 
L 354.42425 11.96614 
L 355.817392 11.878238 
L 355.817392 11.848937 
L 359.12857 11.761035 
L 359.12857 11.731735 
L 361.208416 11.643833 
L 361.208416 11.614532 
L 362.759421 11.52663 
L 362.759421 11.49733 
L 363.880249 11.438728 
L 363.880249 11.380127 
L 366.49683 11.292225 
L 366.49683 11.262924 
L 371.595808 11.175023 
L 371.595808 11.145722 
L 374.737284 11.05782 
L 374.737284 11.028519 
L 378.58125 10.999219 
L 378.58125 10.999219 
" style="fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;"></path>
   </g>
   <g id="line2d_18">
    <path clip-path="url(#p14b7ad9762)" d="M 43.78125 228.439219 
L 378.58125 10.999219 
" style="fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;"></path>
   </g>
   <g id="patch_3">
    <path d="M 43.78125 228.439219 
L 43.78125 10.999219 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path>
   </g>
   <g id="patch_4">
    <path d="M 378.58125 228.439219 
L 378.58125 10.999219 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path>
   </g>
   <g id="patch_5">
    <path d="M 43.78125 228.439219 
L 378.58125 228.439219 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path>
   </g>
   <g id="patch_6">
    <path d="M 43.78125 10.999219 
L 378.58125 10.999219 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path>
   </g>
   <g id="legend_1">
    <g id="patch_7">
     <path d="M 50.78125 48.355469 
L 124.178125 48.355469 
Q 126.178125 48.355469 126.178125 46.355469 
L 126.178125 17.999219 
Q 126.178125 15.999219 124.178125 15.999219 
L 50.78125 15.999219 
Q 48.78125 15.999219 48.78125 17.999219 
L 48.78125 46.355469 
Q 48.78125 48.355469 50.78125 48.355469 
z
" style="fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;"></path>
    </g>
    <g id="line2d_19">
     <path d="M 52.78125 24.097656 
L 72.78125 24.097656 
" style="fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;"></path>
    </g>
    <g id="line2d_20"></g>
    <g id="text_19">
     <!-- ROC -->
     <g transform="translate(80.78125 27.597656)scale(0.1 -0.1)">
      <defs>
       <path d="M 39.40625 66.21875 
Q 28.65625 66.21875 22.328125 58.203125 
Q 16.015625 50.203125 16.015625 36.375 
Q 16.015625 22.609375 22.328125 14.59375 
Q 28.65625 6.59375 39.40625 6.59375 
Q 50.140625 6.59375 56.421875 14.59375 
Q 62.703125 22.609375 62.703125 36.375 
Q 62.703125 50.203125 56.421875 58.203125 
Q 50.140625 66.21875 39.40625 66.21875 
z
M 39.40625 74.21875 
Q 54.734375 74.21875 63.90625 63.9375 
Q 73.09375 53.65625 73.09375 36.375 
Q 73.09375 19.140625 63.90625 8.859375 
Q 54.734375 -1.421875 39.40625 -1.421875 
Q 24.03125 -1.421875 14.8125 8.828125 
Q 5.609375 19.09375 5.609375 36.375 
Q 5.609375 53.65625 14.8125 63.9375 
Q 24.03125 74.21875 39.40625 74.21875 
z
" id="DejaVuSans-79"></path>
       <path d="M 64.40625 67.28125 
L 64.40625 56.890625 
Q 59.421875 61.53125 53.78125 63.8125 
Q 48.140625 66.109375 41.796875 66.109375 
Q 29.296875 66.109375 22.65625 58.46875 
Q 16.015625 50.828125 16.015625 36.375 
Q 16.015625 21.96875 22.65625 14.328125 
Q 29.296875 6.6875 41.796875 6.6875 
Q 48.140625 6.6875 53.78125 8.984375 
Q 59.421875 11.28125 64.40625 15.921875 
L 64.40625 5.609375 
Q 59.234375 2.09375 53.4375 0.328125 
Q 47.65625 -1.421875 41.21875 -1.421875 
Q 24.65625 -1.421875 15.125 8.703125 
Q 5.609375 18.84375 5.609375 36.375 
Q 5.609375 53.953125 15.125 64.078125 
Q 24.65625 74.21875 41.21875 74.21875 
Q 47.75 74.21875 53.53125 72.484375 
Q 59.328125 70.75 64.40625 67.28125 
z
" id="DejaVuSans-67"></path>
      </defs>
      <use xlink:href="#DejaVuSans-82"></use>
      <use x="69.482422" xlink:href="#DejaVuSans-79"></use>
      <use x="148.193359" xlink:href="#DejaVuSans-67"></use>
     </g>
    </g>
    <g id="line2d_21">
     <path d="M 52.78125 38.775781 
L 72.78125 38.775781 
" style="fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;"></path>
    </g>
    <g id="line2d_22"></g>
    <g id="text_20">
     <!-- Random -->
     <g transform="translate(80.78125 42.275781)scale(0.1 -0.1)">
      <defs>
       <path d="M 45.40625 46.390625 
L 45.40625 75.984375 
L 54.390625 75.984375 
L 54.390625 0 
L 45.40625 0 
L 45.40625 8.203125 
Q 42.578125 3.328125 38.25 0.953125 
Q 33.9375 -1.421875 27.875 -1.421875 
Q 17.96875 -1.421875 11.734375 6.484375 
Q 5.515625 14.40625 5.515625 27.296875 
Q 5.515625 40.1875 11.734375 48.09375 
Q 17.96875 56 27.875 56 
Q 33.9375 56 38.25 53.625 
Q 42.578125 51.265625 45.40625 46.390625 
z
M 14.796875 27.296875 
Q 14.796875 17.390625 18.875 11.75 
Q 22.953125 6.109375 30.078125 6.109375 
Q 37.203125 6.109375 41.296875 11.75 
Q 45.40625 17.390625 45.40625 27.296875 
Q 45.40625 37.203125 41.296875 42.84375 
Q 37.203125 48.484375 30.078125 48.484375 
Q 22.953125 48.484375 18.875 42.84375 
Q 14.796875 37.203125 14.796875 27.296875 
z
" id="DejaVuSans-100"></path>
       <path d="M 30.609375 48.390625 
Q 23.390625 48.390625 19.1875 42.75 
Q 14.984375 37.109375 14.984375 27.296875 
Q 14.984375 17.484375 19.15625 11.84375 
Q 23.34375 6.203125 30.609375 6.203125 
Q 37.796875 6.203125 41.984375 11.859375 
Q 46.1875 17.53125 46.1875 27.296875 
Q 46.1875 37.015625 41.984375 42.703125 
Q 37.796875 48.390625 30.609375 48.390625 
z
M 30.609375 56 
Q 42.328125 56 49.015625 48.375 
Q 55.71875 40.765625 55.71875 27.296875 
Q 55.71875 13.875 49.015625 6.21875 
Q 42.328125 -1.421875 30.609375 -1.421875 
Q 18.84375 -1.421875 12.171875 6.21875 
Q 5.515625 13.875 5.515625 27.296875 
Q 5.515625 40.765625 12.171875 48.375 
Q 18.84375 56 30.609375 56 
z
" id="DejaVuSans-111"></path>
       <path d="M 52 44.1875 
Q 55.375 50.25 60.0625 53.125 
Q 64.75 56 71.09375 56 
Q 79.640625 56 84.28125 50.015625 
Q 88.921875 44.046875 88.921875 33.015625 
L 88.921875 0 
L 79.890625 0 
L 79.890625 32.71875 
Q 79.890625 40.578125 77.09375 44.375 
Q 74.3125 48.1875 68.609375 48.1875 
Q 61.625 48.1875 57.5625 43.546875 
Q 53.515625 38.921875 53.515625 30.90625 
L 53.515625 0 
L 44.484375 0 
L 44.484375 32.71875 
Q 44.484375 40.625 41.703125 44.40625 
Q 38.921875 48.1875 33.109375 48.1875 
Q 26.21875 48.1875 22.15625 43.53125 
Q 18.109375 38.875 18.109375 30.90625 
L 18.109375 0 
L 9.078125 0 
L 9.078125 54.6875 
L 18.109375 54.6875 
L 18.109375 46.1875 
Q 21.1875 51.21875 25.484375 53.609375 
Q 29.78125 56 35.6875 56 
Q 41.65625 56 45.828125 52.96875 
Q 50 49.953125 52 44.1875 
z
" id="DejaVuSans-109"></path>
      </defs>
      <use xlink:href="#DejaVuSans-82"></use>
      <use x="67.232422" xlink:href="#DejaVuSans-97"></use>
      <use x="128.511719" xlink:href="#DejaVuSans-110"></use>
      <use x="191.890625" xlink:href="#DejaVuSans-100"></use>
      <use x="255.367188" xlink:href="#DejaVuSans-111"></use>
      <use x="316.548828" xlink:href="#DejaVuSans-109"></use>
     </g>
    </g>
   </g>
  </g>
 </g>
 <defs>
  <clippath id="p14b7ad9762">
   <rect height="217.44" width="334.8" x="43.78125" y="10.999219"></rect>
  </clippath>
 </defs>
</svg>

</div>

</div>

</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances
<script src="https://utteranc.es/client.js"
        repo="Gwonchankim/channee"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script> -->

<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://channeeblogjupyter.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a class="u-url" href="/channee/jupyter/ml/machinelearning/2021/08/31/_09_01_Project2_4.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/channee/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/channee/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/channee/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/https%3A%2F%2Fgithub.com%2FGwonchankim" title="https://github.com/Gwonchankim"><svg class="svg-icon grey"><use xlink:href="/channee/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/channee/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
